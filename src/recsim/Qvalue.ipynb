{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "from math import sqrt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusers = 100\n",
    "nitems = 100\n",
    "k = 10\n",
    "\n",
    "pu = np.random.rand(k,1)\n",
    "\n",
    "Q = np.random.rand(nitems,k)\n",
    "nQ = np.dot(Q,Q.T)\n",
    "\n",
    "ru = np.dot(Q,pu)\n",
    "D = (np.diag(nQ) + np.diag(nQ.T) - 2*nQ)\n",
    "#D = np.sqrt(np.diag(nQ) + np.diag(nQ.T) - 2*nQ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrans=100\n",
    "nitems=len(ru)\n",
    "action = np.ceil(np.random.rand(ntrans,1)*nitems).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReward(r, D, state,action):\n",
    "    nitems = len(r)\n",
    "    DD = D.flatten()\n",
    "\n",
    "    reward = r[action] + DD[(state-1)*nitems + action]\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(ntrans, ru, D):\n",
    "\n",
    "    nitems = len(ru)\n",
    "    state = np.ceil(np.random.rand(ntrans,1)*nitems).astype(int)\n",
    "    action = np.ceil(np.random.rand(ntrans,1)*nitems).astype(int)\n",
    "    reward=[]\n",
    "    for i in range(1,len(state)):\n",
    "        for j in range(1, len(action)):\n",
    "            reward.append(getReward(ru, D, state,action))\n",
    "    data = [state,action,reward]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initQval(ru,D,numStates,numActions):\n",
    "\n",
    "    Qval = np.zeros((numStates,numActions))\n",
    "    for i in range(1, numStates):\n",
    "        for j in range(1, numActions):\n",
    "            Qval[i,j] = getReward(ru,D,i,j)\n",
    "    return Qval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 3.5195425 , 5.82646412, ..., 7.83167761, 7.20273947,\n",
       "        7.81979325],\n",
       "       [0.        , 3.32215686, 6.53622528, ..., 8.45046504, 7.67623283,\n",
       "        6.82905726],\n",
       "       ...,\n",
       "       [0.        , 4.15921108, 7.28813712, ..., 8.89402504, 8.02029852,\n",
       "        6.98133953],\n",
       "       [0.        , 2.84425513, 3.83194305, ..., 4.67794093, 4.21810398,\n",
       "        3.68015681],\n",
       "       [0.        , 4.00259732, 4.87166446, ..., 6.15067839, 3.88465792,\n",
       "        4.15780948]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initQval(ru,D,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qvalati(ru, i, D, gamma, nitems,maxdepth,depth):\n",
    "    qval =  np.zeros((nitems,1))\n",
    "    if (depth==maxdepth):\n",
    "        return\n",
    "    for j in range(1, nitems):\n",
    "        qval[j] = getReward(ru,D,i,j) + gamma*max(qvalati(ru,j,D,gamma,nitems,maxdepth,depth+1))\n",
    "    return qval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qlearn(ru, D, gamma, maxdepth):\n",
    "    nitems = len(ru)\n",
    "    Qval = np.zeros((nitems,nitems))\n",
    "    for i in range(1, nitems):\n",
    "        Qval[i,:] = qvalati(ru, i, D, gamma, nitems,maxdepth,1)\n",
    "    return Qval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(i,Qval,N):\n",
    "\n",
    "    rec = i\n",
    "\n",
    "    for k in range(2,N):\n",
    "        [dummy,j]=max(Qval[rec[-1],:])\n",
    "        rec = [rec,j]\n",
    "    return rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TDlearn(Qval, gamma, alpha, data,nEpochs,Qvalopt):\n",
    "    if (len(TDlearn.parameters)<5):\n",
    "        nEpochs = 1000\n",
    "    if (len(TDlearn.parameters)<6):\n",
    "        Qvalopt = []\n",
    "    ntrans = len(data)\n",
    "    for t in range(1,nEpochs):\n",
    "        ii = np.random.permutation(ntrans)\n",
    "\n",
    "        for s in range(1, ntrans): \n",
    "            state = data[ii[s],1]\n",
    "            action = data[ii[s], 2]\n",
    "            reward = data[ii[s], 3]\n",
    "            nextstate = action\n",
    "        \n",
    "            qmax = max(Qval[nextstate,:])\n",
    "            Qval[state,action] = (1-alpha)*Qval[state,action] + alpha*(reward + gamma*qmax)\n",
    "    \n",
    "        if (Qvalopt.length==0):\n",
    "            print(\"%d: %f\\n\", t, np.norm(Qvalopt-Qval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mqvalati\u001b[49m\u001b[43m(\u001b[49m\u001b[43mru\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [98], line 6\u001b[0m, in \u001b[0;36mqvalati\u001b[0;34m(ru, i, D, gamma, nitems, maxdepth, depth)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, nitems):\n\u001b[0;32m----> 6\u001b[0m     qval[j] \u001b[38;5;241m=\u001b[39m getReward(ru,D,i,j) \u001b[38;5;241m+\u001b[39m gamma\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[43mqvalati\u001b[49m\u001b[43m(\u001b[49m\u001b[43mru\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmaxdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qval\n",
      "Cell \u001b[0;32mIn [98], line 6\u001b[0m, in \u001b[0;36mqvalati\u001b[0;34m(ru, i, D, gamma, nitems, maxdepth, depth)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, nitems):\n\u001b[0;32m----> 6\u001b[0m     qval[j] \u001b[38;5;241m=\u001b[39m getReward(ru,D,i,j) \u001b[38;5;241m+\u001b[39m gamma\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqvalati\u001b[49m\u001b[43m(\u001b[49m\u001b[43mru\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmaxdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qval\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "qvalati(ru, 10, D, 0.2, 100, 3, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1aae1df98fcdaba42211eb04fb1c10f1061d9efaa8b282c66397dab6b26e66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

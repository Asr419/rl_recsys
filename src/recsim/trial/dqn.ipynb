{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusers = 100\n",
    "nitems = 100\n",
    "k = 10\n",
    "\n",
    "pu = np.random.rand(k,1)\n",
    "\n",
    "Q = np.random.rand(nitems,k)\n",
    "nQ = np.dot(Q,Q.T)\n",
    "\n",
    "ru = np.dot(Q,pu)\n",
    "D = (np.diag(nQ) + np.diag(nQ.T) - 2*nQ)\n",
    "nitems = len(ru)\n",
    "ntrans=200\n",
    "state = np.ceil(np.random.rand(ntrans,2)*nitems).astype(int)\n",
    "action = np.ceil(np.random.rand(ntrans,1)*nitems).astype(int)\n",
    "next_state = np.ceil(np.random.rand(ntrans,2)*nitems).astype(int)\n",
    "for i in range(0, len(state)):\n",
    "    next_state[i][0]=state[i][1]\n",
    "    next_state[i][1]=action[i][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = np.ceil(np.random.rand(ntrans,1)*nitems).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({'state': list(state), 'action': list(action), 'next_state': list(next_state)}, columns=['state', 'action', 'next_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>next_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[58, 43]</td>\n",
       "      <td>[19]</td>\n",
       "      <td>[43, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[60, 34]</td>\n",
       "      <td>[64]</td>\n",
       "      <td>[34, 64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[21, 78]</td>\n",
       "      <td>[37]</td>\n",
       "      <td>[78, 37]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[33, 51]</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[51, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[47, 70]</td>\n",
       "      <td>[38]</td>\n",
       "      <td>[70, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>[29, 61]</td>\n",
       "      <td>[73]</td>\n",
       "      <td>[61, 73]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[48, 80]</td>\n",
       "      <td>[98]</td>\n",
       "      <td>[80, 98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[79, 23]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[23, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[100, 12]</td>\n",
       "      <td>[86]</td>\n",
       "      <td>[12, 86]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[49, 39]</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[39, 17]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state action next_state\n",
       "0     [58, 43]   [19]   [43, 19]\n",
       "1     [60, 34]   [64]   [34, 64]\n",
       "2     [21, 78]   [37]   [78, 37]\n",
       "3     [33, 51]   [17]   [51, 17]\n",
       "4     [47, 70]   [38]   [70, 38]\n",
       "..         ...    ...        ...\n",
       "195   [29, 61]   [73]   [61, 73]\n",
       "196   [48, 80]   [98]   [80, 98]\n",
       "197   [79, 23]    [1]    [23, 1]\n",
       "198  [100, 12]   [86]   [12, 86]\n",
       "199   [49, 39]   [17]   [39, 17]\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReward1(ru, D, state, action):\n",
    "    nitems = len(ru)\n",
    "    dist = D.flatten()\n",
    "    try:\n",
    "        reward = ru[action][0]\n",
    "        for i in range(0, len(state)):\n",
    "            if state[i]==-1:\n",
    "                break\n",
    "            else:\n",
    "                reward += (1/((len(state)-i)+1)) * dist[(state[i])*nitems + action] \n",
    "    except IndexError:\n",
    "        reward = 0\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReward(ru, D, state, action):\n",
    "    nitems = len(ru)\n",
    "    dist = D.flatten()\n",
    "    try:\n",
    "        reward = ru[action][0] +  dist[(state[0])*nitems + action] + dist[(state[1])*nitems + action]\n",
    "    except IndexError:\n",
    "        reward = 1.5\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward=[]\n",
    "for i in range(0,len(state)):\n",
    "    reward.append(getReward(ru, D, state[i],action[i]))\n",
    "\n",
    "#reward = ru[action[0]][0] + DD[(state[1])*nitems + action[0]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"reward\"]=reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>next_state</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[58, 43]</td>\n",
       "      <td>[19]</td>\n",
       "      <td>[43, 19]</td>\n",
       "      <td>[2.544252429670139]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[60, 34]</td>\n",
       "      <td>[64]</td>\n",
       "      <td>[34, 64]</td>\n",
       "      <td>[11.370304021007444]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[21, 78]</td>\n",
       "      <td>[37]</td>\n",
       "      <td>[78, 37]</td>\n",
       "      <td>[5.667940934231813]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[33, 51]</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[51, 17]</td>\n",
       "      <td>[2.956446823175522]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[47, 70]</td>\n",
       "      <td>[38]</td>\n",
       "      <td>[70, 38]</td>\n",
       "      <td>[7.160635379876405]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>[29, 61]</td>\n",
       "      <td>[73]</td>\n",
       "      <td>[61, 73]</td>\n",
       "      <td>[7.825356193400408]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[48, 80]</td>\n",
       "      <td>[98]</td>\n",
       "      <td>[80, 98]</td>\n",
       "      <td>[2.528101693363611]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[79, 23]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[23, 1]</td>\n",
       "      <td>[5.1560354212800785]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[100, 12]</td>\n",
       "      <td>[86]</td>\n",
       "      <td>[12, 86]</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[49, 39]</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[39, 17]</td>\n",
       "      <td>[6.326248533641114]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state action next_state                reward\n",
       "0     [58, 43]   [19]   [43, 19]   [2.544252429670139]\n",
       "1     [60, 34]   [64]   [34, 64]  [11.370304021007444]\n",
       "2     [21, 78]   [37]   [78, 37]   [5.667940934231813]\n",
       "3     [33, 51]   [17]   [51, 17]   [2.956446823175522]\n",
       "4     [47, 70]   [38]   [70, 38]   [7.160635379876405]\n",
       "..         ...    ...        ...                   ...\n",
       "195   [29, 61]   [73]   [61, 73]   [7.825356193400408]\n",
       "196   [48, 80]   [98]   [80, 98]   [2.528101693363611]\n",
       "197   [79, 23]    [1]    [23, 1]  [5.1560354212800785]\n",
       "198  [100, 12]   [86]   [12, 86]                   1.5\n",
       "199   [49, 39]   [17]   [39, 17]   [6.326248533641114]\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 13:51:39.511371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 13:51:39.656476: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:39.656513: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-02 13:51:40.544679: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:40.544790: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:40.544805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from numpy import int64\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from gym.spaces import Box, Discrete\n",
    "from tqdm import tqdm\n",
    "\n",
    "  \n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "a=Box(low=0, high=99, shape=(1,2), dtype=int64)\n",
    "k=a.sample()\n",
    "print(k.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31 43]\n",
      "Asr\n"
     ]
    }
   ],
   "source": [
    "observation_space = Box(low=0, high=99, shape=(1,2), dtype=int64)\n",
    "a=observation_space.sample().ravel()\n",
    "print(a)\n",
    "action=89\n",
    "a=np.append(a, action)\n",
    "a[2]\n",
    "if action in a:\n",
    "    print(\"Asr\")\n",
    "else:\n",
    "    print(\"as\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv1(Env):\n",
    "    def __init__(self):\n",
    "        self.i=0\n",
    "        self.action_space = Discrete(100)\n",
    "        self.observation_space = Box(low=0, high=99, shape=(1,10), dtype=int64)\n",
    "        self.state = self.observation_space.sample().ravel()\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.i+=1 \n",
    "        for j in range(self.i,10):\n",
    "            self.state[j]=-1\n",
    "        \n",
    "        self.next_state = self.observation_space.sample().ravel()\n",
    "        #self.next_state[0] = self.state[1]\n",
    "        #self.next_state[1]= action\n",
    "        \n",
    "        if action in self.state:\n",
    "            self.state[self.i]=action\n",
    "            reward = -10\n",
    "        else:  \n",
    "            self.state[self.i]=action \n",
    "            reward= getReward1(ru, D, self.state, action)\n",
    "        \n",
    "        if self.i==9:\n",
    "            print(self.state)\n",
    "            done=True\n",
    "        else:\n",
    "            done=False\n",
    "        info={}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.i=0\n",
    "        self.state = self.observation_space.sample().ravel()\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.i=0\n",
    "        self.action_space = Discrete(100)\n",
    "        self.observation_space = Box(low=0, high=99, shape=(1,2), dtype=int64)\n",
    "        self.state = self.observation_space.sample().ravel()\n",
    "        self.a=self.state\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.i+=1\n",
    "        self.next_state = self.observation_space.sample().ravel()\n",
    "        self.next_state[0] = self.state[1]\n",
    "        self.next_state[1]= action\n",
    "        #print(self.next_state)\n",
    "        if action in self.a:\n",
    "            reward=-100\n",
    "        else:\n",
    "            reward= getReward(ru, D, self.state, action)\n",
    "        self.state=self.next_state\n",
    "        self.a=np.append(self.a,action)\n",
    "        if self.i==12:\n",
    "            print(self.a)\n",
    "            done=True\n",
    "        else:\n",
    "            done=False\n",
    "        info={}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.a=[]\n",
    "        self.i=0\n",
    "        self.state = self.observation_space.sample().ravel()\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 72 72  1 32 34 16 75  4 34]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=env.observation_space.sample().ravel()\n",
    "\n",
    "print(a)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 630.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31 47 31 74 67 34 33 67 13 21]\n",
      "Episode:1 Score:7.243799608790377\n",
      "[26 80 93 31 49 66 39 85 78 38]\n",
      "Episode:2 Score:35.30539679313147\n",
      "[24 23 52 47 28 80 74 24 48 77]\n",
      "Episode:3 Score:23.902502902833092\n",
      "[51 16 47 75 61 82 51 77 81 93]\n",
      "Episode:4 Score:24.117777938949946\n",
      "[96 95 15 74 45 51 58 70 14 97]\n",
      "Episode:5 Score:34.165801945607484\n",
      "[88  0 17 45 41  0 86  3 14 44]\n",
      "Episode:6 Score:18.94605002413871\n",
      "[28 86 56 86 17 66 48 39 10 80]\n",
      "Episode:7 Score:27.119142934841843\n",
      "[26 93 56 50 21 56 19 75 72 15]\n",
      "Episode:8 Score:16.212557601900638\n",
      "[38 92 22 70 63 30 86 27 84 51]\n",
      "Episode:9 Score:33.683252979291936\n",
      "[42 13 68 14 68 73 36 30 62 55]\n",
      "Episode:10 Score:22.023832055579646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 #20 shower episodes\n",
    "for episode in tqdm(range(1, episodes+1)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.shape(env.observation_space)\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 24)             264       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 24)             600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 24)             600       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 24)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               2500      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,964\n",
      "Trainable params: 3,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2, gamma=0.9)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Adam._name = 'hey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2023-02-02 13:51:44.977032: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:44.977148: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:44.977242: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:44.977331: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:44.977419: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:44.977507: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:44.977593: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:44.977681: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-02 13:51:44.977712: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-02 13:51:44.978118: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 13:51:44.998869: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-02-02 13:51:45.047087: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_3_1/kernel/Assign' id:307 op device:{requested: '', assigned: ''} def:{{{node dense_3_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_3_1/kernel, dense_3_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10001 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-02-02 13:51:45.677186: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_3/BiasAdd' id:159 op device:{requested: '', assigned: ''} def:{{{node dense_3/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3/MatMul, dense_3/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-02 13:51:45.703796: W tensorflow/c/c_api.cc:291] Operation '{name:'count_3/Assign' id:505 op device:{requested: '', assigned: ''} def:{{{node count_3/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_3, count_3/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76 72 32 32 32 32 72 32 33 33]\n",
      "     9/10001: episode: 1, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: -44.667, mean reward: -4.963 [-10.000, 10.667], mean action: 41.111 [32.000, 72.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-02-02 13:51:45.780618: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_3_1/BiasAdd' id:317 op device:{requested: '', assigned: ''} def:{{{node dense_3_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_1/MatMul, dense_3_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-02 13:51:46.118469: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/AddN' id:601 op device:{requested: '', assigned: ''} def:{{{node loss_3/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul, loss_3/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-02-02 13:51:46.164754: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/dense_3/kernel/m/Assign' id:815 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_3/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_3/kernel/m, training/Adam/dense_3/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68 72 32 32 73 38 16 38 38 16]\n",
      "    18/10001: episode: 2, duration: 0.748s, episode steps:   9, steps per second:  12, episode reward: -22.881, mean reward: -2.542 [-10.000,  4.977], mean action: 39.444 [16.000, 73.000],  loss: 47.455552, mae: 2.785034, mean_q: 9.007038\n",
      "[ 9 16 73 17 77 94 80 33 73 44]\n",
      "    27/10001: episode: 3, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 24.335, mean reward:  2.704 [-10.000,  8.218], mean action: 56.333 [16.000, 94.000],  loss: 35.179802, mae: 2.407600, mean_q: 8.134888\n",
      "[63 33 73 73 73 77 73 16 75 77]\n",
      "    36/10001: episode: 4, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -22.060, mean reward: -2.451 [-10.000,  4.375], mean action: 63.333 [16.000, 77.000],  loss: 20.268673, mae: 3.519451, mean_q: 12.593787\n",
      "[77 77 73 77 17 77 77 75 53 80]\n",
      "    45/10001: episode: 5, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: -16.992, mean reward: -1.888 [-10.000,  7.457], mean action: 67.333 [17.000, 80.000],  loss: 17.214115, mae: 3.338127, mean_q: 12.517774\n",
      "[25 72 16 17 17 77 80 17 17 17]\n",
      "    54/10001: episode: 6, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: -21.470, mean reward: -2.386 [-10.000,  5.612], mean action: 36.667 [16.000, 80.000],  loss: 9.739112, mae: 3.643676, mean_q: 14.668082\n",
      "[97 17 17 17 17 33 73 16 72 53]\n",
      "    63/10001: episode: 7, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -5.845, mean reward: -0.649 [-10.000,  6.248], mean action: 35.000 [16.000, 73.000],  loss: 12.630458, mae: 3.261642, mean_q: 11.601046\n",
      "[42  2 80 43 37 10 78 33 33 17]\n",
      "    72/10001: episode: 8, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 22.742, mean reward:  2.527 [-10.000,  7.840], mean action: 37.000 [2.000, 80.000],  loss: 9.449733, mae: 3.409925, mean_q: 11.750402\n",
      "[42 73 73 17 63 75 80 75 80  2]\n",
      "    81/10001: episode: 9, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -2.708, mean reward: -0.301 [-10.000,  9.165], mean action: 59.778 [2.000, 80.000],  loss: 11.084257, mae: 3.713959, mean_q: 13.486300\n",
      "[40  2 80 80 35 39 78 33 80 10]\n",
      "    90/10001: episode: 10, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.610, mean reward:  1.068 [-10.000,  7.837], mean action: 48.556 [2.000, 80.000],  loss: 9.710861, mae: 3.139745, mean_q: 11.347891\n",
      "[64 10 80 10 10 10 10 10 78 78]\n",
      "    99/10001: episode: 11, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: -49.968, mean reward: -5.552 [-10.000,  3.638], mean action: 32.889 [10.000, 80.000],  loss: 11.061985, mae: 3.389651, mean_q: 12.939292\n",
      "[51 75 80 78 78 33 33 33 89 78]\n",
      "   108/10001: episode: 12, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: -19.239, mean reward: -2.138 [-10.000,  6.511], mean action: 64.111 [33.000, 89.000],  loss: 11.048175, mae: 2.948479, mean_q: 12.039519\n",
      "[66 78 75 78 11 47 53 73  2 78]\n",
      "   117/10001: episode: 13, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  9.166, mean reward:  1.018 [-10.000,  7.452], mean action: 55.000 [2.000, 78.000],  loss: 19.418421, mae: 2.764065, mean_q: 11.399824\n",
      "[81 19 33 73 73 33 80 33 33 73]\n",
      "   126/10001: episode: 14, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: -34.060, mean reward: -3.784 [-10.000,  4.864], mean action: 50.000 [19.000, 80.000],  loss: 18.462383, mae: 2.747816, mean_q: 11.386511\n",
      "[39 43 33 63 80 47 89 89 73  2]\n",
      "   135/10001: episode: 15, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 25.632, mean reward:  2.848 [-10.000,  8.436], mean action: 57.667 [2.000, 89.000],  loss: 16.826124, mae: 2.911328, mean_q: 11.536026\n",
      "[ 2 43  6 38 80 38 16 16 80 19]\n",
      "   144/10001: episode: 16, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -10.751, mean reward: -1.195 [-10.000,  4.314], mean action: 37.333 [6.000, 80.000],  loss: 13.579847, mae: 3.168954, mean_q: 11.660337\n",
      "[93 80 63 63 53 80 63 80 73 43]\n",
      "   153/10001: episode: 17, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -18.174, mean reward: -2.019 [-10.000,  7.110], mean action: 66.444 [43.000, 80.000],  loss: 13.473840, mae: 3.265908, mean_q: 13.805353\n",
      "[23 97  6 17  6 77  6  6  6 34]\n",
      "   162/10001: episode: 18, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: -26.540, mean reward: -2.949 [-10.000,  4.223], mean action: 28.333 [6.000, 97.000],  loss: 17.112511, mae: 2.625338, mean_q: 9.802450\n",
      "[90 43 73  0 75 75 78 19 89 17]\n",
      "   171/10001: episode: 19, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 19.515, mean reward:  2.168 [-10.000,  6.235], mean action: 52.111 [0.000, 89.000],  loss: 13.368898, mae: 2.638861, mean_q: 10.711208\n",
      "[48 40 37 96 75 75 89 16 16 92]\n",
      "   180/10001: episode: 20, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  5.256, mean reward:  0.584 [-10.000,  4.915], mean action: 59.556 [16.000, 96.000],  loss: 13.477998, mae: 2.606044, mean_q: 10.684184\n",
      "[82 17 33 16 92 19 19 19 19 19]\n",
      "   189/10001: episode: 21, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -24.953, mean reward: -2.773 [-10.000,  4.560], mean action: 28.111 [16.000, 92.000],  loss: 12.006731, mae: 2.613115, mean_q: 10.701103\n",
      "[59 33 97 35 84 89 17  2 80 19]\n",
      "   198/10001: episode: 22, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 33.402, mean reward:  3.711 [ 1.739,  6.384], mean action: 50.667 [2.000, 97.000],  loss: 11.840548, mae: 2.814199, mean_q: 11.249914\n",
      "[83  2 97 80 80 89 43 43 92 19]\n",
      "   207/10001: episode: 23, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  4.914, mean reward:  0.546 [-10.000,  4.571], mean action: 60.556 [2.000, 97.000],  loss: 12.487401, mae: 2.702241, mean_q: 10.249763\n",
      "[37 63 80 17 78 77 94 96 63 19]\n",
      "   216/10001: episode: 24, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 17.244, mean reward:  1.916 [-10.000,  4.755], mean action: 65.222 [17.000, 96.000],  loss: 12.044407, mae: 2.628883, mean_q: 10.580169\n",
      "[14 97 43 77 96 96 16 16 93  2]\n",
      "   225/10001: episode: 25, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 11.583, mean reward:  1.287 [-10.000,  9.654], mean action: 59.556 [2.000, 97.000],  loss: 12.466917, mae: 2.801426, mean_q: 11.605646\n",
      "[21 37 43 78 43 77  2  2  2  2]\n",
      "   234/10001: episode: 26, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: -20.164, mean reward: -2.240 [-10.000,  6.733], mean action: 31.778 [2.000, 78.000],  loss: 10.548396, mae: 2.813549, mean_q: 11.491472\n",
      "[78 97 89  2  2 35 35 43 53 92]\n",
      "   243/10001: episode: 27, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward:  7.738, mean reward:  0.860 [-10.000,  4.812], mean action: 49.778 [2.000, 97.000],  loss: 11.576930, mae: 2.938148, mean_q: 12.572677\n",
      "[74 35 89 35 47 47 55 35 13  8]\n",
      "   252/10001: episode: 28, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: -7.338, mean reward: -0.815 [-10.000,  7.016], mean action: 40.444 [8.000, 89.000],  loss: 8.637207, mae: 3.107041, mean_q: 13.607518\n",
      "[80 89 96 80 89 39 37 43 37 37]\n",
      "   261/10001: episode: 29, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -21.330, mean reward: -2.370 [-10.000,  5.739], mean action: 60.778 [37.000, 96.000],  loss: 10.575794, mae: 2.813255, mean_q: 12.075351\n",
      "[40 96 77 37 82 19 93 65 96 97]\n",
      "   270/10001: episode: 30, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.840, mean reward:  2.316 [-10.000,  6.612], mean action: 73.556 [19.000, 97.000],  loss: 13.306296, mae: 2.733674, mean_q: 10.770165\n",
      "[ 3 33  2  6 89 35 72 84 10 33]\n",
      "   279/10001: episode: 31, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.391, mean reward:  2.266 [-10.000,  5.021], mean action: 40.444 [2.000, 89.000],  loss: 12.826435, mae: 2.563699, mean_q: 9.721903\n",
      "[26 80 10  2  2 77 93 16 93 17]\n",
      "   288/10001: episode: 32, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  5.405, mean reward:  0.601 [-10.000,  4.593], mean action: 43.333 [2.000, 93.000],  loss: 10.054769, mae: 2.717107, mean_q: 10.860623\n",
      "[81 97 10 33 92 16 33 77 37 97]\n",
      "   297/10001: episode: 33, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward:  5.159, mean reward:  0.573 [-10.000,  5.082], mean action: 54.667 [10.000, 97.000],  loss: 12.186718, mae: 3.042981, mean_q: 12.722130\n",
      "[67 17 37 33 40 78 40 96  8  8]\n",
      "   306/10001: episode: 34, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  6.897, mean reward:  0.766 [-10.000,  5.005], mean action: 39.667 [8.000, 96.000],  loss: 10.487283, mae: 3.034191, mean_q: 11.421415\n",
      "[88 24 37 96 13 55 55 55 73  8]\n",
      "   315/10001: episode: 35, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  8.382, mean reward:  0.931 [-10.000,  6.815], mean action: 46.222 [8.000, 96.000],  loss: 11.226954, mae: 2.999947, mean_q: 11.296924\n",
      "[39 92 77 35 84 77 96 96 35 97]\n",
      "   324/10001: episode: 36, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -6.969, mean reward: -0.774 [-10.000,  6.605], mean action: 76.556 [35.000, 97.000],  loss: 10.555524, mae: 2.961468, mean_q: 11.224751\n",
      "[50 92 10 77 92 77 65 65 10 77]\n",
      "   333/10001: episode: 37, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -36.355, mean reward: -4.039 [-10.000,  3.590], mean action: 62.778 [10.000, 92.000],  loss: 11.283594, mae: 2.950987, mean_q: 11.387986\n",
      "[72  2 65  0 33 25 50 33 84 97]\n",
      "   342/10001: episode: 38, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 19.252, mean reward:  2.139 [-10.000,  6.103], mean action: 43.222 [0.000, 97.000],  loss: 10.808758, mae: 2.971545, mean_q: 11.231370\n",
      "[ 2 10 32 43 39 17 78 17 82 10]\n",
      "   351/10001: episode: 39, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  6.337, mean reward:  0.704 [-10.000,  7.194], mean action: 36.444 [10.000, 82.000],  loss: 7.825377, mae: 3.589198, mean_q: 13.125081\n",
      "[35  2 24 19 19 16 68 27 24 13]\n",
      "   360/10001: episode: 40, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  5.448, mean reward:  0.605 [-10.000,  7.653], mean action: 23.556 [2.000, 68.000],  loss: 9.330434, mae: 3.098913, mean_q: 11.865652\n",
      "[82  2 24 24 24 55 16 82  2  2]\n",
      "   369/10001: episode: 41, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: -37.608, mean reward: -4.179 [-10.000,  3.805], mean action: 25.667 [2.000, 82.000],  loss: 10.880520, mae: 3.571127, mean_q: 13.666099\n",
      "[30 97 89 10 82 19 37 78 82 84]\n",
      "   378/10001: episode: 42, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 15.991, mean reward:  1.777 [-10.000,  4.506], mean action: 64.222 [10.000, 97.000],  loss: 10.688343, mae: 3.255554, mean_q: 13.135881\n",
      "[15 97 38  6 38 89 89 89 77 84]\n",
      "   387/10001: episode: 43, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -6.762, mean reward: -0.751 [-10.000,  5.504], mean action: 67.444 [6.000, 97.000],  loss: 10.176030, mae: 3.592196, mean_q: 14.288816\n",
      "[90 97 19 19 19 19 19 73 10 55]\n",
      "   396/10001: episode: 44, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: -17.428, mean reward: -1.936 [-10.000,  6.634], mean action: 36.667 [10.000, 97.000],  loss: 13.617336, mae: 3.372888, mean_q: 13.463457\n",
      "[ 8 16 37 78 84 93 82 82 82  2]\n",
      "   405/10001: episode: 45, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 10.989, mean reward:  1.221 [-10.000,  8.955], mean action: 61.778 [2.000, 93.000],  loss: 12.116993, mae: 3.716177, mean_q: 14.825448\n",
      "[47 97 16 77 16 37 37 37 16 37]\n",
      "   414/10001: episode: 46, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -37.117, mean reward: -4.124 [-10.000,  3.771], mean action: 41.111 [16.000, 97.000],  loss: 15.792622, mae: 3.601151, mean_q: 14.244175\n",
      "[45 24 16 27 84 35 93  0 61 93]\n",
      "   423/10001: episode: 47, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 16.890, mean reward:  1.877 [-10.000,  4.721], mean action: 48.111 [0.000, 93.000],  loss: 11.015621, mae: 3.504261, mean_q: 13.328135\n",
      "[67  0 16 80 84 35 93 92 93 92]\n",
      "   432/10001: episode: 48, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  4.172, mean reward:  0.464 [-10.000,  4.785], mean action: 65.000 [0.000, 93.000],  loss: 12.115812, mae: 3.843503, mean_q: 15.171681\n",
      "[91 75 16 16 16 16 97 16 16 73]\n",
      "   441/10001: episode: 49, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -32.745, mean reward: -3.638 [-10.000,  6.614], mean action: 37.889 [16.000, 97.000],  loss: 12.959652, mae: 3.972732, mean_q: 15.376528\n",
      "[55 13 73 25 84 92 92 27 27 27]\n",
      "   450/10001: episode: 50, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -6.139, mean reward: -0.682 [-10.000,  6.686], mean action: 51.111 [13.000, 92.000],  loss: 13.165181, mae: 3.781581, mean_q: 15.525504\n",
      "[13 92 78 43 82 82 93 35 43 96]\n",
      "   459/10001: episode: 51, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  7.395, mean reward:  0.822 [-10.000,  4.721], mean action: 71.556 [35.000, 96.000],  loss: 16.494493, mae: 3.830187, mean_q: 15.150443\n",
      "[63 75 77 43 82 77 63 27  0  2]\n",
      "   468/10001: episode: 52, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 14.256, mean reward:  1.584 [-10.000,  9.905], mean action: 49.556 [0.000, 82.000],  loss: 11.370112, mae: 3.725397, mean_q: 14.351299\n",
      "[74 97 10 33 73 32 89 65 32 77]\n",
      "   477/10001: episode: 53, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 20.959, mean reward:  2.329 [-10.000,  6.256], mean action: 56.444 [10.000, 97.000],  loss: 15.654351, mae: 4.053945, mean_q: 15.867336\n",
      "[58 97 43 73 39 40  0  0  0  0]\n",
      "   486/10001: episode: 54, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -9.143, mean reward: -1.016 [-10.000,  4.708], mean action: 32.444 [0.000, 97.000],  loss: 11.126468, mae: 3.711091, mean_q: 14.682308\n",
      "[81 10 33 27 27 27  0  0  0  0]\n",
      "   495/10001: episode: 55, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -35.439, mean reward: -3.938 [-10.000,  4.656], mean action: 13.778 [0.000, 33.000],  loss: 17.325657, mae: 4.143606, mean_q: 16.269741\n",
      "[72 24 24 73 24 84 24 24 24 24]\n",
      "   504/10001: episode: 56, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -50.096, mean reward: -5.566 [-10.000,  4.447], mean action: 36.111 [24.000, 84.000],  loss: 15.140194, mae: 4.208376, mean_q: 16.739777\n",
      "[74 84 33 33 96 32 89 97 37 77]\n",
      "   513/10001: episode: 57, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.851, mean reward:  2.317 [-10.000,  6.220], mean action: 64.222 [32.000, 97.000],  loss: 14.882233, mae: 3.808599, mean_q: 15.768843\n",
      "[56  2 96 25 78 78 78 27  2  0]\n",
      "   522/10001: episode: 58, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -6.477, mean reward: -0.720 [-10.000,  7.011], mean action: 42.889 [0.000, 96.000],  loss: 15.214259, mae: 4.077627, mean_q: 16.659414\n",
      "[36 61 10 43 75 89 65 97 97 97]\n",
      "   531/10001: episode: 59, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  5.590, mean reward:  0.621 [-10.000,  5.168], mean action: 70.444 [10.000, 97.000],  loss: 15.179657, mae: 4.082034, mean_q: 15.949941\n",
      "[96 97 77 92 84 25 25 11 15 13]\n",
      "   540/10001: episode: 60, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 17.979, mean reward:  1.998 [-10.000,  5.369], mean action: 48.778 [11.000, 97.000],  loss: 15.653419, mae: 4.025137, mean_q: 15.707399\n",
      "[94 75 63 73 47 25 13 97 61 75]\n",
      "   549/10001: episode: 61, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 19.318, mean reward:  2.146 [-10.000,  5.235], mean action: 58.778 [13.000, 97.000],  loss: 13.583751, mae: 4.169779, mean_q: 15.961292\n",
      "[12 75 77 43 68 50 68 97 61 25]\n",
      "   558/10001: episode: 62, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 22.801, mean reward:  2.533 [-10.000,  8.101], mean action: 62.667 [25.000, 97.000],  loss: 16.502220, mae: 4.333500, mean_q: 16.908587\n",
      "[21 50 80 68 50 50 13 43 61 13]\n",
      "   567/10001: episode: 63, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -7.702, mean reward: -0.856 [-10.000,  6.293], mean action: 47.556 [13.000, 80.000],  loss: 15.844228, mae: 4.826991, mean_q: 18.704569\n",
      "[ 9 92 80 43 19 19 43 89 61 92]\n",
      "   576/10001: episode: 64, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -7.914, mean reward: -0.879 [-10.000,  5.696], mean action: 59.778 [19.000, 92.000],  loss: 16.429968, mae: 4.240394, mean_q: 16.204210\n",
      "[23 13 92 43 43 43 43  8 50 92]\n",
      "   585/10001: episode: 65, duration: 0.056s, episode steps:   9, steps per second: 159, episode reward: -22.675, mean reward: -2.519 [-10.000,  5.150], mean action: 47.444 [8.000, 92.000],  loss: 22.244463, mae: 4.680981, mean_q: 17.304539\n",
      "[39 89 65 78 11 11 11 61 61 19]\n",
      "   594/10001: episode: 66, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -10.831, mean reward: -1.203 [-10.000,  4.231], mean action: 45.111 [11.000, 89.000],  loss: 18.115801, mae: 4.245627, mean_q: 16.027321\n",
      "[93 92 65 97 61 61 97 61 61 61]\n",
      "   603/10001: episode: 67, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: -38.481, mean reward: -4.276 [-10.000,  3.611], mean action: 72.889 [61.000, 97.000],  loss: 16.788565, mae: 4.312427, mean_q: 16.715153\n",
      "[70 61 73 37 25 39 37 68 92 25]\n",
      "   612/10001: episode: 68, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  4.770, mean reward:  0.530 [-10.000,  5.006], mean action: 50.778 [25.000, 92.000],  loss: 17.950630, mae: 4.822813, mean_q: 18.611750\n",
      "[13 68 80 35 68 17 17 77 40 89]\n",
      "   621/10001: episode: 69, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 12.158, mean reward:  1.351 [-10.000,  7.014], mean action: 54.556 [17.000, 89.000],  loss: 16.909117, mae: 4.417166, mean_q: 16.866081\n",
      "[17 89 80 10 77 80  2  2 78 68]\n",
      "   630/10001: episode: 70, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  5.198, mean reward:  0.578 [-10.000,  6.410], mean action: 54.000 [2.000, 89.000],  loss: 15.292316, mae: 4.792924, mean_q: 18.707214\n",
      "[40  2 50 17 84 84 25 68 40 10]\n",
      "   639/10001: episode: 71, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  4.820, mean reward:  0.536 [-10.000,  6.273], mean action: 42.222 [2.000, 84.000],  loss: 16.960541, mae: 4.580450, mean_q: 17.609810\n",
      "[72 61 92 40 40 40 43 13 75 97]\n",
      "   648/10001: episode: 72, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  8.013, mean reward:  0.890 [-10.000,  6.145], mean action: 55.667 [13.000, 97.000],  loss: 17.574804, mae: 4.651736, mean_q: 17.672552\n",
      "[41 97 80  6  6  6 80 33 89  5]\n",
      "   657/10001: episode: 73, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -5.264, mean reward: -0.585 [-10.000,  6.639], mean action: 44.667 [5.000, 97.000],  loss: 16.774956, mae: 4.902237, mean_q: 18.507309\n",
      "[59 10 92 19 17 84 33 37 92 97]\n",
      "   666/10001: episode: 74, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: 19.675, mean reward:  2.186 [-10.000,  6.812], mean action: 53.444 [10.000, 97.000],  loss: 15.014574, mae: 4.917493, mean_q: 18.695696\n",
      "[19 97 19 19 19 19 89 63 63 97]\n",
      "   675/10001: episode: 75, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -48.133, mean reward: -5.348 [-10.000,  5.607], mean action: 53.889 [19.000, 97.000],  loss: 26.745558, mae: 4.541923, mean_q: 17.466862\n",
      "[23 61 80 78 84 17 89 11 11 84]\n",
      "   684/10001: episode: 76, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  5.619, mean reward:  0.624 [-10.000,  5.173], mean action: 57.222 [11.000, 89.000],  loss: 20.121056, mae: 5.090876, mean_q: 19.511501\n",
      "[80 10 80 80 17 39 84  8 75 50]\n",
      "   693/10001: episode: 77, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  5.305, mean reward:  0.589 [-10.000,  5.002], mean action: 49.222 [8.000, 84.000],  loss: 19.431314, mae: 5.313290, mean_q: 19.612799\n",
      "[77 92 37 73 97 75 73 97 97 50]\n",
      "   702/10001: episode: 78, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -9.346, mean reward: -1.038 [-10.000,  4.233], mean action: 76.778 [37.000, 97.000],  loss: 21.504461, mae: 4.787551, mean_q: 18.863503\n",
      "[57 43 80 16 84 16 37 50 43  2]\n",
      "   711/10001: episode: 79, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward:  8.667, mean reward:  0.963 [-10.000, 10.162], mean action: 41.222 [2.000, 84.000],  loss: 26.437744, mae: 5.076258, mean_q: 19.069574\n",
      "[90 97 92 43 75 78 65 43 43 97]\n",
      "   720/10001: episode: 80, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -9.943, mean reward: -1.105 [-10.000,  3.570], mean action: 70.333 [43.000, 97.000],  loss: 21.224791, mae: 4.912258, mean_q: 18.643730\n",
      "[21 84 89 92 84 39 78 40 89 43]\n",
      "   729/10001: episode: 81, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 10.432, mean reward:  1.159 [-10.000,  7.081], mean action: 70.889 [39.000, 92.000],  loss: 22.913292, mae: 5.291136, mean_q: 20.184641\n",
      "[37 97 80 78 75 75 92 84  0  0]\n",
      "   738/10001: episode: 82, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  1.705, mean reward:  0.189 [-10.000,  3.638], mean action: 64.556 [0.000, 97.000],  loss: 19.574253, mae: 5.010632, mean_q: 18.961008\n",
      "[82 92 80 77 84 37 92 89 25 25]\n",
      "   747/10001: episode: 83, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  5.740, mean reward:  0.638 [-10.000,  5.506], mean action: 66.778 [25.000, 92.000],  loss: 27.694824, mae: 5.257407, mean_q: 19.798315\n",
      "[19 97 77 94 84 39 94 63 65  0]\n",
      "   756/10001: episode: 84, duration: 0.056s, episode steps:   9, steps per second: 160, episode reward: 16.538, mean reward:  1.838 [-10.000,  4.273], mean action: 68.111 [0.000, 97.000],  loss: 22.683140, mae: 5.300206, mean_q: 19.034920\n",
      "[33 97 80 78 84 84 78 89 11 17]\n",
      "   765/10001: episode: 85, duration: 0.056s, episode steps:   9, steps per second: 161, episode reward:  6.788, mean reward:  0.754 [-10.000,  5.493], mean action: 68.667 [11.000, 97.000],  loss: 19.616081, mae: 5.254125, mean_q: 19.290926\n",
      "[98  2 33 68 84 84 68 50 17 50]\n",
      "   774/10001: episode: 86, duration: 0.056s, episode steps:   9, steps per second: 160, episode reward: -8.689, mean reward: -0.965 [-10.000,  4.800], mean action: 50.667 [2.000, 84.000],  loss: 23.811268, mae: 5.011215, mean_q: 19.010147\n",
      "[29 89 89 17 10 16 10 17 89 15]\n",
      "   783/10001: episode: 87, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -24.425, mean reward: -2.714 [-10.000,  3.850], mean action: 39.111 [10.000, 89.000],  loss: 28.367842, mae: 5.165049, mean_q: 18.911329\n",
      "[53 89 80 89 39 82 15 15 15 15]\n",
      "   792/10001: episode: 88, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: -22.186, mean reward: -2.465 [-10.000,  5.004], mean action: 48.778 [15.000, 89.000],  loss: 28.293728, mae: 5.269813, mean_q: 19.831123\n",
      "[14 97 89 17 16  6 55  6 37 27]\n",
      "   801/10001: episode: 89, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 21.914, mean reward:  2.435 [-10.000,  7.912], mean action: 38.889 [6.000, 97.000],  loss: 33.404121, mae: 5.328371, mean_q: 19.644491\n",
      "[47 73 37 92 84 92 50 27  2 40]\n",
      "   810/10001: episode: 90, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 26.758, mean reward:  2.973 [-10.000,  7.968], mean action: 55.222 [2.000, 92.000],  loss: 24.703836, mae: 5.063144, mean_q: 18.730080\n",
      "[92  2 97 39 84 10 10 84 17 17]\n",
      "   819/10001: episode: 91, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -9.602, mean reward: -1.067 [-10.000,  3.978], mean action: 40.000 [2.000, 97.000],  loss: 23.516457, mae: 5.257895, mean_q: 19.844753\n",
      "[94 97 50 92 84 84 37 97 17 17]\n",
      "   828/10001: episode: 92, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: -10.645, mean reward: -1.183 [-10.000,  4.054], mean action: 63.889 [17.000, 97.000],  loss: 22.058189, mae: 5.618841, mean_q: 20.746126\n",
      "[99 73 92 43 84 39 61 13 13 13]\n",
      "   837/10001: episode: 93, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  2.000, mean reward:  0.222 [-10.000,  3.697], mean action: 47.889 [13.000, 92.000],  loss: 27.380529, mae: 5.182123, mean_q: 18.981806\n",
      "[ 7 43 89 94 39 13 82 50 68  2]\n",
      "   846/10001: episode: 94, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 38.074, mean reward:  4.230 [ 2.636, 10.745], mean action: 53.333 [2.000, 94.000],  loss: 25.197479, mae: 5.128290, mean_q: 19.089437\n",
      "[24  2 92 94 61 61 15 15 15 15]\n",
      "   855/10001: episode: 95, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: -25.055, mean reward: -2.784 [-10.000,  3.867], mean action: 41.111 [2.000, 94.000],  loss: 25.270138, mae: 5.653176, mean_q: 21.033686\n",
      "[46 97 80 61 68 15 82 80 47 73]\n",
      "   864/10001: episode: 96, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.503, mean reward:  2.278 [-10.000,  6.428], mean action: 67.000 [15.000, 97.000],  loss: 24.703453, mae: 5.380158, mean_q: 19.127495\n",
      "[ 5  2 73 93 61 61 93 27 27 27]\n",
      "   873/10001: episode: 97, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -19.150, mean reward: -2.128 [-10.000,  7.058], mean action: 51.556 [2.000, 93.000],  loss: 24.424536, mae: 5.824114, mean_q: 20.665974\n",
      "[99 73 92 43 75 75 13 13 55 75]\n",
      "   882/10001: episode: 98, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -10.335, mean reward: -1.148 [-10.000,  3.697], mean action: 57.111 [13.000, 92.000],  loss: 24.561018, mae: 4.726577, mean_q: 16.943623\n",
      "[98 89 50 73 75 82 80  2 96 75]\n",
      "   891/10001: episode: 99, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 25.300, mean reward:  2.811 [-10.000,  7.047], mean action: 69.111 [2.000, 96.000],  loss: 23.617472, mae: 5.656108, mean_q: 20.462315\n",
      "[ 8 73  6  6  2  6 73 94 93 10]\n",
      "   900/10001: episode: 100, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: -6.146, mean reward: -0.683 [-10.000,  4.737], mean action: 40.333 [2.000, 94.000],  loss: 22.405624, mae: 5.467600, mean_q: 19.416512\n",
      "[95 89 92 96 47 84 80 96 39 10]\n",
      "   909/10001: episode: 101, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 20.148, mean reward:  2.239 [-10.000,  5.650], mean action: 70.333 [10.000, 96.000],  loss: 20.212029, mae: 5.488780, mean_q: 19.789721\n",
      "[ 9 92 89 50 55 55 50  8 97 89]\n",
      "   918/10001: episode: 102, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -5.803, mean reward: -0.645 [-10.000,  6.482], mean action: 65.000 [8.000, 97.000],  loss: 23.774956, mae: 5.356255, mean_q: 19.277348\n",
      "[66 89 77 92 89 89 84 75 10 72]\n",
      "   927/10001: episode: 103, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  3.961, mean reward:  0.440 [-10.000,  5.185], mean action: 75.222 [10.000, 92.000],  loss: 27.305199, mae: 5.451769, mean_q: 19.525753\n",
      "[17 89  2  2 94 89 84 92 84  2]\n",
      "   936/10001: episode: 104, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -23.898, mean reward: -2.655 [-10.000,  4.087], mean action: 59.778 [2.000, 94.000],  loss: 23.909691, mae: 5.921490, mean_q: 20.558250\n",
      "[12 89 94 13 13 55 97  8  8  8]\n",
      "   945/10001: episode: 105, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -7.531, mean reward: -0.837 [-10.000,  5.813], mean action: 42.778 [8.000, 97.000],  loss: 24.010973, mae: 5.622327, mean_q: 20.408812\n",
      "[76 73 92 40 39 39  8 39 39 73]\n",
      "   954/10001: episode: 106, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -22.027, mean reward: -2.447 [-10.000,  4.252], mean action: 49.111 [8.000, 92.000],  loss: 27.810007, mae: 5.760821, mean_q: 20.768841\n",
      "[52 89 80 53 84 53 77 50 61  2]\n",
      "   963/10001: episode: 107, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.594, mean reward:  2.510 [-10.000,  9.912], mean action: 61.000 [2.000, 89.000],  loss: 25.148781, mae: 5.348711, mean_q: 19.397282\n",
      "[25 89  2 94 75 75 43 75 75 11]\n",
      "   972/10001: episode: 108, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: -6.019, mean reward: -0.669 [-10.000,  6.220], mean action: 59.889 [2.000, 94.000],  loss: 28.307127, mae: 5.965036, mean_q: 21.034203\n",
      "[34 97 94 77 43 84 37 27 89  2]\n",
      "   981/10001: episode: 109, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 42.257, mean reward:  4.695 [ 2.620,  9.099], mean action: 61.111 [2.000, 97.000],  loss: 21.189028, mae: 5.412431, mean_q: 19.106934\n",
      "[95 11 73 40 82 77 33 33 25 25]\n",
      "   990/10001: episode: 110, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 10.094, mean reward:  1.122 [-10.000,  6.568], mean action: 44.333 [11.000, 82.000],  loss: 24.534315, mae: 6.191492, mean_q: 21.663622\n",
      "[61 89 77 40 53 39 61 97  2 84]\n",
      "   999/10001: episode: 111, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 23.403, mean reward:  2.600 [-10.000,  8.163], mean action: 60.222 [2.000, 97.000],  loss: 25.561420, mae: 5.575666, mean_q: 19.559391\n",
      "[74 39 80 82 39 82 80  8 53 89]\n",
      "  1008/10001: episode: 112, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -6.338, mean reward: -0.704 [-10.000,  5.812], mean action: 61.333 [8.000, 89.000],  loss: 21.845245, mae: 5.969409, mean_q: 20.961979\n",
      "[79 10 73 55 25 53  8  8 25 17]\n",
      "  1017/10001: episode: 113, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  6.061, mean reward:  0.673 [-10.000,  4.659], mean action: 30.444 [8.000, 73.000],  loss: 22.306953, mae: 5.797940, mean_q: 20.793959\n",
      "[22 73 94 77 55 77 84 50 53  2]\n",
      "  1026/10001: episode: 114, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 21.902, mean reward:  2.434 [-10.000, 10.240], mean action: 62.778 [2.000, 94.000],  loss: 24.991381, mae: 5.617279, mean_q: 19.719217\n",
      "[70  2 97 13 16 77 37  8  8  8]\n",
      "  1035/10001: episode: 115, duration: 0.056s, episode steps:   9, steps per second: 161, episode reward:  6.336, mean reward:  0.704 [-10.000,  5.464], mean action: 29.556 [2.000, 97.000],  loss: 22.275631, mae: 5.931839, mean_q: 20.725653\n",
      "[44 10 89 77 13 53  8  8  8  8]\n",
      "  1044/10001: episode: 116, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -8.965, mean reward: -0.996 [-10.000,  4.777], mean action: 30.444 [8.000, 89.000],  loss: 28.852242, mae: 6.010550, mean_q: 21.509995\n",
      "[ 9 10 89 77 77 77 55 55 84 13]\n",
      "  1053/10001: episode: 117, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -9.209, mean reward: -1.023 [-10.000,  4.137], mean action: 59.667 [10.000, 89.000],  loss: 29.473751, mae: 5.740906, mean_q: 19.875526\n",
      "[46 10 92 77 77 77 33 33 15 17]\n",
      "  1062/10001: episode: 118, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: -6.240, mean reward: -0.693 [-10.000,  7.170], mean action: 47.889 [10.000, 92.000],  loss: 25.778578, mae: 5.729793, mean_q: 19.582031\n",
      "[67 61 92 16 16 55 77 16 84 96]\n",
      "  1071/10001: episode: 119, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  4.747, mean reward:  0.527 [-10.000,  5.639], mean action: 57.000 [16.000, 96.000],  loss: 26.253403, mae: 5.116995, mean_q: 17.784260\n",
      "[81 73 33 73 47 39 47 73 39 39]\n",
      "  1080/10001: episode: 120, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -35.713, mean reward: -3.968 [-10.000,  4.533], mean action: 51.444 [33.000, 73.000],  loss: 20.442493, mae: 6.056809, mean_q: 21.089952\n",
      "[90 47 97 55 33 55 82 97 96 73]\n",
      "  1089/10001: episode: 121, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 11.351, mean reward:  1.261 [-10.000,  6.140], mean action: 70.556 [33.000, 97.000],  loss: 22.490557, mae: 5.698281, mean_q: 19.436550\n",
      "[95 89 89 27 97 53 78 61 11  2]\n",
      "  1098/10001: episode: 122, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 24.485, mean reward:  2.721 [-10.000,  8.580], mean action: 56.333 [2.000, 97.000],  loss: 19.616526, mae: 5.920096, mean_q: 20.026115\n",
      "[55  2 73 17 55 55 33 55 55 43]\n",
      "  1107/10001: episode: 123, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -14.523, mean reward: -1.614 [-10.000,  7.695], mean action: 43.111 [2.000, 73.000],  loss: 22.732300, mae: 5.802733, mean_q: 19.597183\n",
      "[58 92 10 33 89 94 27 27 43  2]\n",
      "  1116/10001: episode: 124, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 26.883, mean reward:  2.987 [-10.000,  8.221], mean action: 46.333 [2.000, 94.000],  loss: 19.213778, mae: 6.168009, mean_q: 21.072153\n",
      "[51 15 92 16 33 53 25 82 37  2]\n",
      "  1125/10001: episode: 125, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 41.254, mean reward:  4.584 [ 2.615,  9.307], mean action: 39.444 [2.000, 92.000],  loss: 23.419201, mae: 5.669799, mean_q: 19.098446\n",
      "[74 15 97 16 33 13 16 50 39 11]\n",
      "  1134/10001: episode: 126, duration: 0.064s, episode steps:   9, steps per second: 142, episode reward: 22.118, mean reward:  2.458 [-10.000,  6.852], mean action: 32.222 [11.000, 97.000],  loss: 18.377773, mae: 5.935385, mean_q: 19.501169\n",
      "[27 68 80  2  2 17  2 25 65 40]\n",
      "  1143/10001: episode: 127, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  8.426, mean reward:  0.936 [-10.000,  7.692], mean action: 33.444 [2.000, 80.000],  loss: 23.010292, mae: 5.919818, mean_q: 19.718151\n",
      "[98 68 33 73 82  2 47 61  2  2]\n",
      "  1152/10001: episode: 128, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  7.920, mean reward:  0.880 [-10.000,  5.419], mean action: 41.111 [2.000, 82.000],  loss: 21.322710, mae: 5.956578, mean_q: 20.227797\n",
      "[63 68 33 96 17 33 33 73  2  2]\n",
      "  1161/10001: episode: 129, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -4.494, mean reward: -0.499 [-10.000,  6.626], mean action: 39.667 [2.000, 96.000],  loss: 25.426676, mae: 5.564118, mean_q: 18.937935\n",
      "[24 89 94  2 33  2  2  2 96 50]\n",
      "  1170/10001: episode: 130, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -7.606, mean reward: -0.845 [-10.000,  5.463], mean action: 41.111 [2.000, 96.000],  loss: 21.676382, mae: 6.016269, mean_q: 20.170397\n",
      "[33 47 13 92 82 92 35 35 96  2]\n",
      "  1179/10001: episode: 131, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 10.669, mean reward:  1.185 [-10.000,  9.580], mean action: 54.889 [2.000, 96.000],  loss: 20.511221, mae: 5.208050, mean_q: 17.530281\n",
      "[78 92 63 27 75 89 25 39 65 77]\n",
      "  1188/10001: episode: 132, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.504, mean reward:  4.056 [ 2.193,  6.979], mean action: 61.333 [25.000, 92.000],  loss: 23.050117, mae: 5.938159, mean_q: 19.512976\n",
      "[85  2 97 37 17 37 47 37 37 11]\n",
      "  1197/10001: episode: 133, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -6.292, mean reward: -0.699 [-10.000,  7.116], mean action: 35.778 [2.000, 97.000],  loss: 18.018246, mae: 5.410527, mean_q: 17.754854\n",
      "[71 11 11 11 97  2 92 61 43 11]\n",
      "  1206/10001: episode: 134, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -4.643, mean reward: -0.516 [-10.000,  5.627], mean action: 37.667 [2.000, 97.000],  loss: 17.428137, mae: 5.774799, mean_q: 19.319059\n",
      "[34 11 73 53 17 15 78 13 89 61]\n",
      "  1215/10001: episode: 135, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: 34.144, mean reward:  3.794 [ 2.862,  6.216], mean action: 45.556 [11.000, 89.000],  loss: 22.193211, mae: 5.897439, mean_q: 20.170198\n",
      "[82 53 53 43 82 93 33 96 15  2]\n",
      "  1224/10001: episode: 136, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 11.512, mean reward:  1.279 [-10.000,  8.541], mean action: 52.222 [2.000, 96.000],  loss: 20.418821, mae: 6.204873, mean_q: 21.212811\n",
      "[19 92 80 65 82 33 50 43 47 73]\n",
      "  1233/10001: episode: 137, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 38.172, mean reward:  4.241 [ 2.107,  6.443], mean action: 62.778 [33.000, 92.000],  loss: 20.722792, mae: 5.890227, mean_q: 19.691498\n",
      "[83  2 27 27 82 89 17 65 96  2]\n",
      "  1242/10001: episode: 138, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  4.711, mean reward:  0.523 [-10.000,  4.278], mean action: 45.222 [2.000, 96.000],  loss: 16.627537, mae: 6.152922, mean_q: 20.541498\n",
      "[60 89 13 43 75 94  2  2 15 15]\n",
      "  1251/10001: episode: 139, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  6.284, mean reward:  0.698 [-10.000,  6.679], mean action: 38.667 [2.000, 94.000],  loss: 21.371706, mae: 5.970057, mean_q: 20.124809\n",
      "[ 6 75 94  6 50  6  6  6  6 55]\n",
      "  1260/10001: episode: 140, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: -38.855, mean reward: -4.317 [-10.000,  3.583], mean action: 33.778 [6.000, 94.000],  loss: 20.188477, mae: 6.067838, mean_q: 20.253773\n",
      "[99  2 13 15 96 93 82 61 96 11]\n",
      "  1269/10001: episode: 141, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 21.036, mean reward:  2.337 [-10.000,  6.114], mean action: 52.111 [2.000, 96.000],  loss: 18.465019, mae: 6.447859, mean_q: 22.019236\n",
      "[91 47 13 15 13 15 73 92 50 77]\n",
      "  1278/10001: episode: 142, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  8.428, mean reward:  0.936 [-10.000,  6.651], mean action: 43.889 [13.000, 92.000],  loss: 25.416267, mae: 5.402327, mean_q: 18.088057\n",
      "[99 10 13 15 33 80 96 96 96 10]\n",
      "  1287/10001: episode: 143, duration: 0.056s, episode steps:   9, steps per second: 161, episode reward: -7.259, mean reward: -0.807 [-10.000,  5.950], mean action: 49.889 [10.000, 96.000],  loss: 21.462339, mae: 6.016350, mean_q: 20.155151\n",
      "[98 10 33 15 43 93  0  0  0 96]\n",
      "  1296/10001: episode: 144, duration: 0.056s, episode steps:   9, steps per second: 161, episode reward:  8.393, mean reward:  0.933 [-10.000,  6.807], mean action: 32.222 [0.000, 96.000],  loss: 16.788246, mae: 6.436359, mean_q: 21.410118\n",
      "[36 10 93 33 78 93 33 13 33 39]\n",
      "  1305/10001: episode: 145, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -9.282, mean reward: -1.031 [-10.000,  5.103], mean action: 47.222 [10.000, 93.000],  loss: 23.445942, mae: 6.097767, mean_q: 20.059206\n",
      "[54 15 73 33 33 82 33 65 65 77]\n",
      "  1314/10001: episode: 146, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: -5.909, mean reward: -0.657 [-10.000,  5.809], mean action: 52.889 [15.000, 82.000],  loss: 20.199261, mae: 5.967849, mean_q: 20.083555\n",
      "[93 97 53 75 33 33 33 75 73 10]\n",
      "  1323/10001: episode: 147, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: -5.787, mean reward: -0.643 [-10.000,  5.576], mean action: 53.556 [10.000, 97.000],  loss: 27.085899, mae: 5.896206, mean_q: 19.915581\n",
      "[84 10 37 37 17 82 16 15 15 37]\n",
      "  1332/10001: episode: 148, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: -9.253, mean reward: -1.028 [-10.000,  5.348], mean action: 29.556 [10.000, 82.000],  loss: 22.232704, mae: 5.615839, mean_q: 18.966509\n",
      "[79 63 75 37 82 78 13 82 15 13]\n",
      "  1341/10001: episode: 149, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward:  2.463, mean reward:  0.274 [-10.000,  4.988], mean action: 50.889 [13.000, 82.000],  loss: 22.713070, mae: 6.094366, mean_q: 19.838955\n",
      "[36 75 10 80 84 73 82 97 82 47]\n",
      "  1350/10001: episode: 150, duration: 0.056s, episode steps:   9, steps per second: 160, episode reward: 20.564, mean reward:  2.285 [-10.000,  5.193], mean action: 70.000 [10.000, 97.000],  loss: 22.613455, mae: 5.946251, mean_q: 18.974892\n",
      "[84  2 97 16 17 13 13 17 13 84]\n",
      "  1359/10001: episode: 151, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -24.424, mean reward: -2.714 [-10.000,  3.858], mean action: 30.222 [2.000, 97.000],  loss: 25.604893, mae: 6.462432, mean_q: 21.025505\n",
      "[32 15  2 16 82 93 82 93 75 84]\n",
      "  1368/10001: episode: 152, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  5.526, mean reward:  0.614 [-10.000,  4.955], mean action: 60.222 [2.000, 93.000],  loss: 24.922153, mae: 5.912454, mean_q: 19.562290\n",
      "[29 15 92 93 82 93 13 84 39 82]\n",
      "  1377/10001: episode: 153, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  3.057, mean reward:  0.340 [-10.000,  4.662], mean action: 65.889 [13.000, 93.000],  loss: 20.420425, mae: 6.361349, mean_q: 21.010485\n",
      "[ 0 96 94 43 47 82 82 82 89 73]\n",
      "  1386/10001: episode: 154, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  9.387, mean reward:  1.043 [-10.000,  5.718], mean action: 76.444 [43.000, 96.000],  loss: 24.157799, mae: 5.837087, mean_q: 19.260368\n",
      "[56 73 33 10 50 89 89 50 84 40]\n",
      "  1395/10001: episode: 155, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  8.538, mean reward:  0.949 [-10.000,  7.338], mean action: 57.556 [10.000, 89.000],  loss: 26.078960, mae: 5.859001, mean_q: 19.429394\n",
      "[99 11 75 75 82 82 84 68 73 73]\n",
      "  1404/10001: episode: 156, duration: 0.056s, episode steps:   9, steps per second: 160, episode reward: -9.150, mean reward: -1.017 [-10.000,  5.768], mean action: 69.222 [11.000, 84.000],  loss: 20.026846, mae: 6.176593, mean_q: 20.236153\n",
      "[67 39 33 40 84 15 92  0  0  0]\n",
      "  1413/10001: episode: 157, duration: 0.056s, episode steps:   9, steps per second: 161, episode reward:  2.123, mean reward:  0.236 [-10.000,  4.558], mean action: 33.667 [0.000, 92.000],  loss: 24.143686, mae: 5.210035, mean_q: 16.969357\n",
      "[42  2 24 15 25 82 82 82 68 73]\n",
      "  1422/10001: episode: 158, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  6.377, mean reward:  0.709 [-10.000,  6.434], mean action: 50.333 [2.000, 82.000],  loss: 25.224678, mae: 5.670116, mean_q: 18.451416\n",
      "[70  2 93 15 53 35 25 82 89 73]\n",
      "  1431/10001: episode: 159, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 40.082, mean reward:  4.454 [ 2.538,  6.247], mean action: 51.889 [2.000, 93.000],  loss: 21.193293, mae: 5.867550, mean_q: 19.125483\n",
      "[62 10 15 15 96 15 92 11 61 82]\n",
      "  1440/10001: episode: 160, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 12.107, mean reward:  1.345 [-10.000,  8.455], mean action: 44.111 [10.000, 96.000],  loss: 21.882158, mae: 6.366021, mean_q: 20.349459\n",
      "[24  2 92 50 82 96 25 96 53 53]\n",
      "  1449/10001: episode: 161, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  5.601, mean reward:  0.622 [-10.000,  4.859], mean action: 61.000 [2.000, 96.000],  loss: 20.019375, mae: 6.100541, mean_q: 19.139458\n",
      "[27  2 89 35 82 92 25 97 37 16]\n",
      "  1458/10001: episode: 162, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 32.158, mean reward:  3.573 [ 2.838,  4.401], mean action: 52.778 [2.000, 97.000],  loss: 20.620405, mae: 5.726265, mean_q: 17.958338\n",
      "[23 73 50 84 75 82 53 75 92 89]\n",
      "  1467/10001: episode: 163, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.843, mean reward:  2.316 [-10.000,  5.951], mean action: 74.778 [50.000, 92.000],  loss: 19.722435, mae: 5.660227, mean_q: 17.604721\n",
      "[86 40 43 40 13 75 68 89 68 73]\n",
      "  1476/10001: episode: 164, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  7.916, mean reward:  0.880 [-10.000,  7.067], mean action: 56.556 [13.000, 89.000],  loss: 21.172411, mae: 6.011450, mean_q: 18.880571\n",
      "[18 43 35 35 47 43 84 65 89 65]\n",
      "  1485/10001: episode: 165, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -8.176, mean reward: -0.908 [-10.000,  6.134], mean action: 56.222 [35.000, 89.000],  loss: 20.621895, mae: 6.012778, mean_q: 18.949408\n",
      "[32 75 92 53 84 84 33 49 89 65]\n",
      "  1494/10001: episode: 166, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 23.055, mean reward:  2.562 [-10.000,  7.179], mean action: 69.333 [33.000, 92.000],  loss: 20.430136, mae: 5.474263, mean_q: 17.642069\n",
      "[36 92 92 50 38 25 84 33 43 53]\n",
      "  1503/10001: episode: 167, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 23.391, mean reward:  2.599 [-10.000,  7.981], mean action: 56.667 [25.000, 92.000],  loss: 19.727016, mae: 5.544383, mean_q: 17.931499\n",
      "[76 10 75 53 25 33 84 39 39 11]\n",
      "  1512/10001: episode: 168, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: 21.898, mean reward:  2.433 [-10.000,  6.388], mean action: 41.000 [10.000, 84.000],  loss: 20.497389, mae: 5.975420, mean_q: 18.928026\n",
      "[16 92 35 80 24 13 97 33 68 61]\n",
      "  1521/10001: episode: 169, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 34.766, mean reward:  3.863 [ 1.898,  7.926], mean action: 55.889 [13.000, 97.000],  loss: 21.553087, mae: 5.577308, mean_q: 17.938202\n",
      "[13 43 35  6  6  2 96 43 53 92]\n",
      "  1530/10001: episode: 170, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  7.272, mean reward:  0.808 [-10.000,  5.544], mean action: 41.778 [2.000, 96.000],  loss: 19.270956, mae: 5.875727, mean_q: 18.618279\n",
      "[72 11 75 15 15 82 39 89 89 73]\n",
      "  1539/10001: episode: 171, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  9.941, mean reward:  1.105 [-10.000,  6.316], mean action: 54.222 [11.000, 89.000],  loss: 19.753395, mae: 5.511227, mean_q: 17.496223\n",
      "[23 75 33 39 84 80 25 89 73 73]\n",
      "  1548/10001: episode: 172, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.935, mean reward:  2.548 [-10.000,  5.454], mean action: 63.444 [25.000, 89.000],  loss: 17.649912, mae: 6.118280, mean_q: 19.425179\n",
      "[23  2 10 15 82 92 17 65 96 73]\n",
      "  1557/10001: episode: 173, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.075, mean reward:  4.008 [ 2.610,  6.678], mean action: 50.222 [2.000, 96.000],  loss: 19.498055, mae: 5.501028, mean_q: 18.449244\n",
      "[26 82 50 17 37 82 25 80 73 73]\n",
      "  1566/10001: episode: 174, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  7.555, mean reward:  0.839 [-10.000,  5.895], mean action: 57.667 [17.000, 82.000],  loss: 22.771904, mae: 5.717094, mean_q: 18.248545\n",
      "[62 92 33 37 73 89 53 37 89 40]\n",
      "  1575/10001: episode: 175, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  9.020, mean reward:  1.002 [-10.000,  7.119], mean action: 60.333 [33.000, 92.000],  loss: 19.943277, mae: 5.935565, mean_q: 18.786186\n",
      "[37  2 43 15 47 43 15 82 89  2]\n",
      "  1584/10001: episode: 176, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -4.884, mean reward: -0.543 [-10.000,  6.345], mean action: 37.556 [2.000, 89.000],  loss: 20.314644, mae: 5.914700, mean_q: 18.544050\n",
      "[86  2 97 15 25 33 16 89 89 40]\n",
      "  1593/10001: episode: 177, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.749, mean reward:  2.750 [-10.000,  6.813], mean action: 45.111 [2.000, 97.000],  loss: 17.022890, mae: 5.714800, mean_q: 18.250040\n",
      "[58 93 33 38 97 84 50 27 89 43]\n",
      "  1602/10001: episode: 178, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.076, mean reward:  4.231 [ 2.461,  6.801], mean action: 61.556 [27.000, 97.000],  loss: 18.040382, mae: 5.606497, mean_q: 17.617420\n",
      "[73 11 97 33 78 49 37 11 89 61]\n",
      "  1611/10001: episode: 179, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 20.430, mean reward:  2.270 [-10.000,  5.329], mean action: 51.778 [11.000, 97.000],  loss: 19.296621, mae: 5.884115, mean_q: 18.678633\n",
      "[84 80 93 17 25 82 49 16 16 37]\n",
      "  1620/10001: episode: 180, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 20.426, mean reward:  2.270 [-10.000,  5.054], mean action: 46.111 [16.000, 93.000],  loss: 22.119268, mae: 5.345709, mean_q: 16.621748\n",
      "[52 97 33 97 25 17 16 68 17  8]\n",
      "  1629/10001: episode: 181, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  6.324, mean reward:  0.703 [-10.000,  6.931], mean action: 42.000 [8.000, 97.000],  loss: 19.165092, mae: 5.818015, mean_q: 17.628654\n",
      "[ 3  8 89 77 25 37 78 92 47 80]\n",
      "  1638/10001: episode: 182, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 38.516, mean reward:  4.280 [ 3.466,  7.566], mean action: 59.222 [8.000, 92.000],  loss: 19.742329, mae: 6.247428, mean_q: 18.863159\n",
      "[81 97 35 73 97 89 84 80 89 73]\n",
      "  1647/10001: episode: 183, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: -7.297, mean reward: -0.811 [-10.000,  5.471], mean action: 79.667 [35.000, 97.000],  loss: 18.191414, mae: 5.872864, mean_q: 17.682283\n",
      "[66  8  8 97 25 25 82 47 47 73]\n",
      "  1656/10001: episode: 184, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -5.254, mean reward: -0.584 [-10.000,  5.637], mean action: 45.778 [8.000, 97.000],  loss: 16.286568, mae: 5.926416, mean_q: 18.508238\n",
      "[48 97 35 27 73 89 78 89 73 40]\n",
      "  1665/10001: episode: 185, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  7.879, mean reward:  0.875 [-10.000,  6.120], mean action: 66.778 [27.000, 97.000],  loss: 16.574114, mae: 5.713343, mean_q: 17.719431\n",
      "[91 73 93 15 25 25 97 92 33 77]\n",
      "  1674/10001: episode: 186, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 25.633, mean reward:  2.848 [-10.000,  8.360], mean action: 58.889 [15.000, 97.000],  loss: 18.395407, mae: 5.589446, mean_q: 17.332649\n",
      "[13 96 35 93 47 47 73 44 61 82]\n",
      "  1683/10001: episode: 187, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 24.082, mean reward:  2.676 [-10.000,  8.294], mean action: 64.222 [35.000, 96.000],  loss: 17.758705, mae: 5.440464, mean_q: 16.832069\n",
      "[90 49 13 27 27 80 89 73 73 53]\n",
      "  1692/10001: episode: 188, duration: 0.056s, episode steps:   9, steps per second: 161, episode reward:  7.431, mean reward:  0.826 [-10.000,  5.114], mean action: 53.778 [13.000, 89.000],  loss: 19.142159, mae: 6.081204, mean_q: 18.564545\n",
      "[74 17 93 40 78 78 65  0 94 20]\n",
      "  1701/10001: episode: 189, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 17.928, mean reward:  1.992 [-10.000,  5.283], mean action: 53.889 [0.000, 94.000],  loss: 19.028584, mae: 5.448189, mean_q: 17.217848\n",
      "[12 93 49 49 82 93 49 35 61 89]\n",
      "  1710/10001: episode: 190, duration: 0.056s, episode steps:   9, steps per second: 160, episode reward: -4.270, mean reward: -0.474 [-10.000,  6.694], mean action: 66.667 [35.000, 93.000],  loss: 18.484022, mae: 5.870649, mean_q: 18.784399\n",
      "[18  2 49 49 49 94 49 49 84 47]\n",
      "  1719/10001: episode: 191, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -21.791, mean reward: -2.421 [-10.000,  4.225], mean action: 52.444 [2.000, 94.000],  loss: 19.714699, mae: 5.702172, mean_q: 19.220354\n",
      "[47 49 49 96 25 37 17 53  2 61]\n",
      "  1728/10001: episode: 192, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 24.053, mean reward:  2.673 [-10.000,  8.185], mean action: 43.222 [2.000, 96.000],  loss: 21.309156, mae: 5.756849, mean_q: 18.368044\n",
      "[92 11 75 25 17 25 39 47 73 47]\n",
      "  1737/10001: episode: 193, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward:  6.383, mean reward:  0.709 [-10.000,  5.770], mean action: 39.889 [11.000, 75.000],  loss: 21.716246, mae: 5.397645, mean_q: 16.379181\n",
      "[ 4 92 43 43 84 93 37 43 89 84]\n",
      "  1746/10001: episode: 194, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -8.325, mean reward: -0.925 [-10.000,  5.393], mean action: 67.556 [37.000, 93.000],  loss: 18.838751, mae: 5.231050, mean_q: 16.120083\n",
      "[45 92 13 53 84 89 89 11 33 61]\n",
      "  1755/10001: episode: 195, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.412, mean reward:  2.490 [-10.000,  8.723], mean action: 58.333 [11.000, 92.000],  loss: 18.112164, mae: 5.459547, mean_q: 16.544376\n",
      "[54 82 10 10 84 89 93 61 97 47]\n",
      "  1764/10001: episode: 196, duration: 0.056s, episode steps:   9, steps per second: 160, episode reward: 19.801, mean reward:  2.200 [-10.000,  5.356], mean action: 63.667 [10.000, 97.000],  loss: 17.841806, mae: 5.344735, mean_q: 16.650911\n",
      "[12 10  2 43 82 10 84 35  2  2]\n",
      "  1773/10001: episode: 197, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -8.539, mean reward: -0.949 [-10.000,  4.517], mean action: 30.000 [2.000, 84.000],  loss: 21.570665, mae: 5.437571, mean_q: 16.972216\n",
      "[91 82 10 82 10 33 84 73 33 91]\n",
      "  1782/10001: episode: 198, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: -20.668, mean reward: -2.296 [-10.000,  5.932], mean action: 55.333 [10.000, 91.000],  loss: 16.791069, mae: 5.728165, mean_q: 17.414248\n",
      "[65 80 32 84 84 84 10 89 93 43]\n",
      "  1791/10001: episode: 199, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 10.857, mean reward:  1.206 [-10.000,  7.040], mean action: 66.556 [10.000, 93.000],  loss: 20.244963, mae: 5.576032, mean_q: 17.305552\n",
      "[41 82 33 89 82 84 77 80 84 43]\n",
      "  1800/10001: episode: 200, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.844, mean reward:  0.983 [-10.000,  6.040], mean action: 72.667 [33.000, 89.000],  loss: 18.378201, mae: 5.765658, mean_q: 17.805000\n",
      "[87  2 82 16 17 37 47 89 89 43]\n",
      "  1809/10001: episode: 201, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 21.428, mean reward:  2.381 [-10.000,  6.783], mean action: 46.889 [2.000, 89.000],  loss: 15.486917, mae: 5.413572, mean_q: 17.046316\n",
      "[13 47 35 77 17 82 77 89 89 53]\n",
      "  1818/10001: episode: 202, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  7.282, mean reward:  0.809 [-10.000,  5.272], mean action: 62.889 [17.000, 89.000],  loss: 18.590679, mae: 5.689211, mean_q: 17.323677\n",
      "[93 73 27 27 97 27 84 77 77 96]\n",
      "  1827/10001: episode: 203, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -8.368, mean reward: -0.930 [-10.000,  4.198], mean action: 65.000 [27.000, 97.000],  loss: 17.421638, mae: 5.141893, mean_q: 15.974607\n",
      "[41 97 10 93 73  2 61 47 68 68]\n",
      "  1836/10001: episode: 204, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 17.442, mean reward:  1.938 [-10.000,  5.214], mean action: 57.667 [2.000, 97.000],  loss: 20.732033, mae: 5.231986, mean_q: 16.512432\n",
      "[72 27 93 33 82 37 35 33 84 43]\n",
      "  1845/10001: episode: 205, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.731, mean reward:  2.303 [-10.000,  6.011], mean action: 51.889 [27.000, 93.000],  loss: 17.834288, mae: 5.758746, mean_q: 17.555515\n",
      "[65 27 80 33 84 37 78 11 11 11]\n",
      "  1854/10001: episode: 206, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  6.234, mean reward:  0.693 [-10.000,  5.226], mean action: 41.333 [11.000, 84.000],  loss: 16.223667, mae: 6.067977, mean_q: 17.925760\n",
      "[59 11 27 80 84 27 27 27  2  2]\n",
      "  1863/10001: episode: 207, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: -20.331, mean reward: -2.259 [-10.000,  6.469], mean action: 31.889 [2.000, 84.000],  loss: 14.735297, mae: 5.640625, mean_q: 16.979609\n",
      "[10 80 96 37 37 17 25 65 44 11]\n",
      "  1872/10001: episode: 208, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.745, mean reward:  2.527 [-10.000,  7.167], mean action: 45.778 [11.000, 96.000],  loss: 18.454414, mae: 5.473096, mean_q: 16.835356\n",
      "[10  8 89 39 33 50 80 82 44 43]\n",
      "  1881/10001: episode: 209, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 38.360, mean reward:  4.262 [ 2.201,  6.403], mean action: 52.000 [8.000, 89.000],  loss: 18.817917, mae: 5.799363, mean_q: 17.651327\n",
      "[74 11 80  0 47 96 96 19 82 80]\n",
      "  1890/10001: episode: 210, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  5.778, mean reward:  0.642 [-10.000,  7.240], mean action: 56.778 [0.000, 96.000],  loss: 19.379692, mae: 5.551127, mean_q: 16.804325\n",
      "[19 73 96 94 82 82 37 32 42  2]\n",
      "  1899/10001: episode: 211, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 23.180, mean reward:  2.576 [-10.000, 10.005], mean action: 60.000 [2.000, 96.000],  loss: 15.474053, mae: 5.838239, mean_q: 17.779266\n",
      "[10 10 89 93 33 53 17 13 82  8]\n",
      "  1908/10001: episode: 212, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 23.835, mean reward:  2.648 [-10.000,  6.970], mean action: 44.222 [8.000, 93.000],  loss: 19.250727, mae: 5.629803, mean_q: 16.860128\n",
      "[ 0 96 96 94 82 82 96 10 63 92]\n",
      "  1917/10001: episode: 213, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -7.293, mean reward: -0.810 [-10.000,  4.832], mean action: 79.000 [10.000, 96.000],  loss: 20.217800, mae: 5.632363, mean_q: 16.830091\n",
      "[98  8 38 40 82 94 25 78 43 44]\n",
      "  1926/10001: episode: 214, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 37.829, mean reward:  4.203 [ 2.511,  6.270], mean action: 50.222 [8.000, 94.000],  loss: 18.413128, mae: 5.888592, mean_q: 17.497681\n",
      "[54 44 65 82 84 82 96 44 44 44]\n",
      "  1935/10001: episode: 215, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -24.566, mean reward: -2.730 [-10.000,  4.399], mean action: 65.000 [44.000, 96.000],  loss: 14.223733, mae: 5.738345, mean_q: 16.991367\n",
      "[17 92 35 10 40 89 94 43 97  2]\n",
      "  1944/10001: episode: 216, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.641, mean reward:  4.516 [ 2.654,  9.017], mean action: 55.778 [2.000, 97.000],  loss: 18.455435, mae: 5.795935, mean_q: 17.001167\n",
      "[93 27 96  0  0  0  0  0  0 47]\n",
      "  1953/10001: episode: 217, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -34.609, mean reward: -3.845 [-10.000,  6.125], mean action: 18.889 [0.000, 96.000],  loss: 20.320709, mae: 5.434123, mean_q: 16.028687\n",
      "[18 92 80 17 37 37 25  8 65  2]\n",
      "  1962/10001: episode: 218, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 26.094, mean reward:  2.899 [-10.000,  9.618], mean action: 40.333 [2.000, 92.000],  loss: 19.902328, mae: 5.431356, mean_q: 16.225029\n",
      "[ 4 73 35 37 82 96 17 96 43 73]\n",
      "  1971/10001: episode: 219, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  6.310, mean reward:  0.701 [-10.000,  5.562], mean action: 61.333 [17.000, 96.000],  loss: 17.775915, mae: 5.296805, mean_q: 15.971444\n",
      "[25 92 33 92 47 39 96 44 93  2]\n",
      "  1980/10001: episode: 220, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 26.229, mean reward:  2.914 [-10.000,  9.174], mean action: 59.778 [2.000, 96.000],  loss: 15.797869, mae: 5.724241, mean_q: 16.787270\n",
      "[89 15  2 75 25 47 82 49 44  2]\n",
      "  1989/10001: episode: 221, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.734, mean reward:  2.304 [-10.000,  5.573], mean action: 37.889 [2.000, 82.000],  loss: 18.647148, mae: 5.228044, mean_q: 15.662457\n",
      "[78 92 10 93 84 80 94 82 93 44]\n",
      "  1998/10001: episode: 222, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 20.632, mean reward:  2.292 [-10.000,  6.030], mean action: 74.667 [10.000, 94.000],  loss: 20.095297, mae: 5.698609, mean_q: 16.938725\n",
      "[67 82 25 93 47 97 63 89 77 97]\n",
      "  2007/10001: episode: 223, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 19.678, mean reward:  2.186 [-10.000,  5.250], mean action: 74.444 [25.000, 97.000],  loss: 19.217596, mae: 5.461120, mean_q: 16.157217\n",
      "[52 92 33 38 97 77 96 53 37 97]\n",
      "  2016/10001: episode: 224, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 19.495, mean reward:  2.166 [-10.000,  4.542], mean action: 68.889 [33.000, 97.000],  loss: 15.176064, mae: 5.490672, mean_q: 16.076345\n",
      "[30 96 80 89 38 33 82 33 92  2]\n",
      "  2025/10001: episode: 225, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 25.275, mean reward:  2.808 [-10.000,  7.762], mean action: 60.556 [2.000, 96.000],  loss: 19.264671, mae: 5.713540, mean_q: 16.660902\n",
      "[73 96 53 10 89  2 15 89 89 96]\n",
      "  2034/10001: episode: 226, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -8.684, mean reward: -0.965 [-10.000,  5.371], mean action: 59.889 [2.000, 96.000],  loss: 19.302706, mae: 5.383851, mean_q: 15.934723\n",
      "[15  2  2 89 47 96 40 96 73 96]\n",
      "  2043/10001: episode: 227, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -5.786, mean reward: -0.643 [-10.000,  5.127], mean action: 60.111 [2.000, 96.000],  loss: 20.774126, mae: 5.430941, mean_q: 16.316790\n",
      "[64  2 73 53 47 53 17 92  2 11]\n",
      "  2052/10001: episode: 228, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward:  5.353, mean reward:  0.595 [-10.000,  4.653], mean action: 38.889 [2.000, 92.000],  loss: 18.899487, mae: 5.677697, mean_q: 17.120493\n",
      "[62 96 77 93 47 38 15 89  2  2]\n",
      "  2061/10001: episode: 229, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.667, mean reward:  2.630 [-10.000,  8.125], mean action: 51.000 [2.000, 96.000],  loss: 16.421312, mae: 5.406999, mean_q: 16.361359\n",
      "[20 10 10  2 80 92 25 82 92 15]\n",
      "  2070/10001: episode: 230, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  8.377, mean reward:  0.931 [-10.000,  5.680], mean action: 45.333 [2.000, 92.000],  loss: 21.490561, mae: 5.440405, mean_q: 16.726849\n",
      "[90 97 33 38 73 73 33 75 15 40]\n",
      "  2079/10001: episode: 231, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  8.420, mean reward:  0.936 [-10.000,  7.212], mean action: 53.000 [15.000, 97.000],  loss: 19.650801, mae: 5.650942, mean_q: 17.347775\n",
      "[72 38 73 25 47  0 10 16 72 42]\n",
      "  2088/10001: episode: 232, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward: 18.155, mean reward:  2.017 [-10.000,  4.597], mean action: 35.889 [0.000, 73.000],  loss: 19.661873, mae: 5.481237, mean_q: 16.343853\n",
      "[ 5 92 80 65 82 17 78 33 82 42]\n",
      "  2097/10001: episode: 233, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.101, mean reward:  2.456 [-10.000,  7.884], mean action: 63.444 [17.000, 92.000],  loss: 18.720196, mae: 5.441496, mean_q: 16.301247\n",
      "[28 92 65 96 97 73 53 63 92 89]\n",
      "  2106/10001: episode: 234, duration: 0.058s, episode steps:   9, steps per second: 157, episode reward: 19.792, mean reward:  2.199 [-10.000,  6.180], mean action: 80.000 [53.000, 97.000],  loss: 19.270288, mae: 5.834287, mean_q: 17.047523\n",
      "[94 92 38 96 97 84 39 32 43  2]\n",
      "  2115/10001: episode: 235, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 38.397, mean reward:  4.266 [ 1.968,  9.802], mean action: 58.111 [2.000, 97.000],  loss: 20.866207, mae: 5.810318, mean_q: 16.945274\n",
      "[34 80 77 96 17 84 82 92 40 42]\n",
      "  2124/10001: episode: 236, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.349, mean reward:  4.039 [ 2.839,  6.408], mean action: 67.778 [17.000, 96.000],  loss: 17.178915, mae: 5.582553, mean_q: 16.643002\n",
      "[54 84 93 16 82 78 25 35 42 53]\n",
      "  2133/10001: episode: 237, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 33.386, mean reward:  3.710 [ 2.555,  4.810], mean action: 56.444 [16.000, 93.000],  loss: 16.622046, mae: 5.691758, mean_q: 16.699579\n",
      "[24 96 32 10  2 92 25 47 43 89]\n",
      "  2142/10001: episode: 238, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.661, mean reward:  4.296 [ 2.176,  6.012], mean action: 48.444 [2.000, 96.000],  loss: 17.072882, mae: 5.787694, mean_q: 17.218283\n",
      "[ 2 92 32  6 35 89 25 65 89 89]\n",
      "  2151/10001: episode: 239, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  4.589, mean reward:  0.510 [-10.000,  4.701], mean action: 58.000 [6.000, 92.000],  loss: 16.505545, mae: 5.950869, mean_q: 17.534821\n",
      "[81 15 84 47 84 94 65 37 37 43]\n",
      "  2160/10001: episode: 240, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  7.053, mean reward:  0.784 [-10.000,  8.321], mean action: 56.222 [15.000, 94.000],  loss: 18.269798, mae: 5.451994, mean_q: 16.611004\n",
      "[35 96 84 17 78 94 25  8 97 42]\n",
      "  2169/10001: episode: 241, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 35.131, mean reward:  3.903 [ 2.672,  5.987], mean action: 60.111 [8.000, 97.000],  loss: 15.746453, mae: 5.986292, mean_q: 17.372089\n",
      "[84 37 75 38 84 42 53 11 10 11]\n",
      "  2178/10001: episode: 242, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  6.959, mean reward:  0.773 [-10.000,  5.547], mean action: 40.111 [10.000, 84.000],  loss: 23.120955, mae: 5.336534, mean_q: 15.579444\n",
      "[25 73 93 40 25 65 25 97 49 49]\n",
      "  2187/10001: episode: 243, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -6.354, mean reward: -0.706 [-10.000,  5.279], mean action: 57.333 [25.000, 97.000],  loss: 19.674730, mae: 6.027457, mean_q: 17.094286\n",
      "[98 39 75 96 82 40 50  8 49  2]\n",
      "  2196/10001: episode: 244, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 41.504, mean reward:  4.612 [ 2.326,  9.976], mean action: 49.000 [2.000, 96.000],  loss: 18.700903, mae: 5.604192, mean_q: 16.459669\n",
      "[22 11 43 78 96 35 35  4 15 68]\n",
      "  2205/10001: episode: 245, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 17.751, mean reward:  1.972 [-10.000,  5.954], mean action: 42.778 [4.000, 96.000],  loss: 21.203175, mae: 5.638472, mean_q: 17.186836\n",
      "[17 65 10 78 38 33 97 33 44 13]\n",
      "  2214/10001: episode: 246, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 19.733, mean reward:  2.193 [-10.000,  6.528], mean action: 45.667 [10.000, 97.000],  loss: 18.785748, mae: 5.282649, mean_q: 15.583047\n",
      "[19 96 39 17 17 42  8 92 96 96]\n",
      "  2223/10001: episode: 247, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -7.818, mean reward: -0.869 [-10.000,  5.052], mean action: 55.889 [8.000, 96.000],  loss: 21.194405, mae: 5.528803, mean_q: 16.325611\n",
      "[36 93 32 39 33 93 33 92 40 35]\n",
      "  2232/10001: episode: 248, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward:  7.722, mean reward:  0.858 [-10.000,  5.945], mean action: 54.444 [32.000, 93.000],  loss: 20.945873, mae: 5.747001, mean_q: 16.915518\n",
      "[67 96 13 39 92  2 80 33 33 33]\n",
      "  2241/10001: episode: 249, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 10.863, mean reward:  1.207 [-10.000,  7.300], mean action: 46.778 [2.000, 96.000],  loss: 19.281212, mae: 5.830441, mean_q: 17.251427\n",
      "[ 3  2 10 11 96  2  2  2  2  2]\n",
      "  2250/10001: episode: 250, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: -34.948, mean reward: -3.883 [-10.000,  4.098], mean action: 14.333 [2.000, 96.000],  loss: 19.103340, mae: 5.879753, mean_q: 17.493668\n",
      "[52 40 97 96 17 82 84 49 49 43]\n",
      "  2259/10001: episode: 251, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 21.426, mean reward:  2.381 [-10.000,  6.686], mean action: 61.889 [17.000, 97.000],  loss: 15.548328, mae: 5.876047, mean_q: 17.106968\n",
      "[ 7 96 32 96 39 97 80 49 13 40]\n",
      "  2268/10001: episode: 252, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 24.547, mean reward:  2.727 [-10.000,  7.886], mean action: 60.222 [13.000, 97.000],  loss: 19.208488, mae: 5.787310, mean_q: 17.046736\n",
      "[13 39 82 96 47 82 78 43 49 49]\n",
      "  2277/10001: episode: 253, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward:  6.511, mean reward:  0.723 [-10.000,  5.192], mean action: 62.778 [39.000, 96.000],  loss: 16.506296, mae: 5.397216, mean_q: 16.104317\n",
      "[27 49 33 93 47 96 40 82 73 97]\n",
      "  2286/10001: episode: 254, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 37.271, mean reward:  4.141 [ 2.571,  5.631], mean action: 67.778 [33.000, 97.000],  loss: 16.822981, mae: 6.142528, mean_q: 18.523445\n",
      "[26 73 84 39 93 15 96 97 44 13]\n",
      "  2295/10001: episode: 255, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 32.145, mean reward:  3.572 [ 2.522,  5.157], mean action: 61.556 [13.000, 97.000],  loss: 25.399714, mae: 5.286654, mean_q: 16.022133\n",
      "[67 49 27 43 96 96 96  0  0  0]\n",
      "  2304/10001: episode: 256, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: -23.304, mean reward: -2.589 [-10.000,  4.384], mean action: 45.222 [0.000, 96.000],  loss: 21.001350, mae: 5.506915, mean_q: 16.178202\n",
      "[30 96 77 96 39 96 80 92 44 49]\n",
      "  2313/10001: episode: 257, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  6.747, mean reward:  0.750 [-10.000,  5.300], mean action: 74.333 [39.000, 96.000],  loss: 19.391075, mae: 6.121149, mean_q: 18.735941\n",
      "[98 96 96 33 49 96 47 72 37  2]\n",
      "  2322/10001: episode: 258, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 12.958, mean reward:  1.440 [-10.000,  9.929], mean action: 58.667 [2.000, 96.000],  loss: 19.499249, mae: 5.589494, mean_q: 17.445061\n",
      "[77 96 96 93 93 33 93 27 89 49]\n",
      "  2331/10001: episode: 259, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -2.913, mean reward: -0.324 [-10.000,  6.132], mean action: 74.333 [27.000, 96.000],  loss: 21.949791, mae: 5.680038, mean_q: 16.942284\n",
      "[91 78 39 73 75  2  2 93 42 40]\n",
      "  2340/10001: episode: 260, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 23.176, mean reward:  2.575 [-10.000,  7.179], mean action: 49.333 [2.000, 93.000],  loss: 25.986412, mae: 5.315477, mean_q: 16.175167\n",
      "[88 42 42 42 40 42 49 49 49 40]\n",
      "  2349/10001: episode: 261, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -49.078, mean reward: -5.453 [-10.000,  4.653], mean action: 43.889 [40.000, 49.000],  loss: 21.031454, mae: 6.127977, mean_q: 18.992142\n",
      "[65 42 42 42 82 42 42 80 77 40]\n",
      "  2358/10001: episode: 262, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: -12.866, mean reward: -1.430 [-10.000,  7.424], mean action: 54.333 [40.000, 82.000],  loss: 22.399994, mae: 5.914980, mean_q: 18.382223\n",
      "[84  2 97 82 33 65 97 89 93 40]\n",
      "  2367/10001: episode: 263, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 24.075, mean reward:  2.675 [-10.000,  6.602], mean action: 66.444 [2.000, 97.000],  loss: 21.285458, mae: 5.993639, mean_q: 17.134291\n",
      "[42 78 82 40 33 73 55 13 13 13]\n",
      "  2376/10001: episode: 264, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  6.141, mean reward:  0.682 [-10.000,  5.529], mean action: 44.444 [13.000, 82.000],  loss: 15.520966, mae: 5.577600, mean_q: 15.955195\n",
      "[97 15 75 82 82 94 39  2 32  2]\n",
      "  2385/10001: episode: 265, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  4.714, mean reward:  0.524 [-10.000,  7.377], mean action: 47.000 [2.000, 94.000],  loss: 15.611080, mae: 5.912756, mean_q: 17.156746\n",
      "[19 65 10 10 10 89  8  8 55 40]\n",
      "  2394/10001: episode: 266, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -3.639, mean reward: -0.404 [-10.000,  7.742], mean action: 32.778 [8.000, 89.000],  loss: 19.050850, mae: 5.486101, mean_q: 15.901130\n",
      "[92 65 10 39 49 92 84 92 32 77]\n",
      "  2403/10001: episode: 267, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  3.647, mean reward:  0.405 [-10.000,  6.332], mean action: 60.000 [10.000, 92.000],  loss: 23.868887, mae: 5.782665, mean_q: 16.562218\n",
      "[26 68 93 39 82 96 25 97 11 11]\n",
      "  2412/10001: episode: 268, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.793, mean reward:  2.533 [-10.000,  5.555], mean action: 58.000 [11.000, 97.000],  loss: 18.255751, mae: 5.296422, mean_q: 15.567206\n",
      "[82 15 80 82 82 80 39  0  0  0]\n",
      "  2421/10001: episode: 269, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -38.420, mean reward: -4.269 [-10.000,  3.624], mean action: 42.000 [0.000, 82.000],  loss: 17.974051, mae: 5.895182, mean_q: 17.012871\n",
      "[63  2 80 82 78  8 94  8  2 43]\n",
      "  2430/10001: episode: 270, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  8.040, mean reward:  0.893 [-10.000,  6.290], mean action: 44.111 [2.000, 94.000],  loss: 18.151611, mae: 5.852408, mean_q: 16.892542\n",
      "[15 93 93 78 82 82 80 80 53 93]\n",
      "  2439/10001: episode: 271, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -20.526, mean reward: -2.281 [-10.000,  4.920], mean action: 81.556 [53.000, 93.000],  loss: 18.252943, mae: 5.936462, mean_q: 17.027046\n",
      "[57 73 39 39 44 96 37 97 73 26]\n",
      "  2448/10001: episode: 272, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  8.601, mean reward:  0.956 [-10.000,  5.869], mean action: 58.222 [26.000, 97.000],  loss: 18.241854, mae: 6.072721, mean_q: 17.297531\n",
      "[22 73 16 92 96 73 84 49  4 49]\n",
      "  2457/10001: episode: 273, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  6.623, mean reward:  0.736 [-10.000,  6.342], mean action: 59.556 [4.000, 96.000],  loss: 16.942390, mae: 5.899941, mean_q: 16.387627\n",
      "[74 97 89 39 13 97 47 49 43 40]\n",
      "  2466/10001: episode: 274, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 23.054, mean reward:  2.562 [-10.000,  7.160], mean action: 57.111 [13.000, 97.000],  loss: 17.855171, mae: 6.068312, mean_q: 16.752068\n",
      "[38 65 33 72 96 89 16 93 61 68]\n",
      "  2475/10001: episode: 275, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 30.918, mean reward:  3.435 [ 2.211,  4.882], mean action: 65.889 [16.000, 96.000],  loss: 18.298422, mae: 5.665669, mean_q: 16.164129\n",
      "[93 96 33 33 73 80  6 11 89 40]\n",
      "  2484/10001: episode: 276, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 24.368, mean reward:  2.708 [-10.000,  5.577], mean action: 51.222 [6.000, 96.000],  loss: 16.524487, mae: 5.633161, mean_q: 16.080338\n",
      "[75  2 33 93 82  8  4  8 35 80]\n",
      "  2493/10001: episode: 277, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.565, mean reward:  2.729 [-10.000,  6.293], mean action: 38.333 [2.000, 93.000],  loss: 16.905952, mae: 5.492454, mean_q: 15.583674\n",
      "[78 80 33 33 97 77 93 11 25 25]\n",
      "  2502/10001: episode: 278, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  9.464, mean reward:  1.052 [-10.000,  5.970], mean action: 52.667 [11.000, 97.000],  loss: 20.434471, mae: 5.949021, mean_q: 17.072733\n",
      "[98 73 44 43 49 78 37 49 75 93]\n",
      "  2511/10001: episode: 279, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.530, mean reward:  2.614 [-10.000,  6.794], mean action: 60.111 [37.000, 93.000],  loss: 18.237339, mae: 5.581140, mean_q: 16.108088\n",
      "[42 13 93 39 17 16 37 94 11 40]\n",
      "  2520/10001: episode: 280, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 36.929, mean reward:  4.103 [ 2.478,  8.351], mean action: 40.000 [11.000, 94.000],  loss: 20.463203, mae: 5.718031, mean_q: 16.206841\n",
      "[70 68 38 97 68 96 40 93 40 65]\n",
      "  2529/10001: episode: 281, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  9.656, mean reward:  1.073 [-10.000,  5.853], mean action: 67.222 [38.000, 97.000],  loss: 20.008650, mae: 5.839361, mean_q: 16.405205\n",
      "[57 44 33 25 49 65 25 53 43  0]\n",
      "  2538/10001: episode: 282, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 19.620, mean reward:  2.180 [-10.000,  5.937], mean action: 37.444 [0.000, 65.000],  loss: 12.830532, mae: 5.968648, mean_q: 16.780823\n",
      "[33 35 93  4  4  4 15 43 43 84]\n",
      "  2547/10001: episode: 283, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -10.112, mean reward: -1.124 [-10.000,  4.954], mean action: 36.111 [4.000, 93.000],  loss: 19.018084, mae: 6.071415, mean_q: 17.035378\n",
      "[62  2 73 82 82  4 11  4  4  4]\n",
      "  2556/10001: episode: 284, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -19.479, mean reward: -2.164 [-10.000,  4.559], mean action: 29.556 [2.000, 82.000],  loss: 19.098036, mae: 5.912990, mean_q: 16.538597\n",
      "[84 10 27  2 44 93 39 19 89 93]\n",
      "  2565/10001: episode: 285, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 19.872, mean reward:  2.208 [-10.000,  5.252], mean action: 46.222 [2.000, 93.000],  loss: 18.373262, mae: 5.988981, mean_q: 16.727623\n",
      "[43 96 93 40 25 15 40 27 37 43]\n",
      "  2574/10001: episode: 286, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  7.431, mean reward:  0.826 [-10.000,  6.278], mean action: 46.222 [15.000, 96.000],  loss: 16.411139, mae: 6.021708, mean_q: 16.990089\n",
      "[41 96 93 39 27  4 61 73  4  4]\n",
      "  2583/10001: episode: 287, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward:  7.426, mean reward:  0.825 [-10.000,  5.084], mean action: 44.556 [4.000, 96.000],  loss: 18.634762, mae: 5.575028, mean_q: 15.859447\n",
      "[73 37  2 73 97  4  4  4  4  4]\n",
      "  2592/10001: episode: 288, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: -34.740, mean reward: -3.860 [-10.000,  4.652], mean action: 25.444 [2.000, 97.000],  loss: 20.184547, mae: 5.777026, mean_q: 16.933846\n",
      "[97  4  4  4  4  4  4 38 73 11]\n",
      "  2601/10001: episode: 289, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: -35.063, mean reward: -3.896 [-10.000,  4.163], mean action: 16.222 [4.000, 73.000],  loss: 19.766441, mae: 5.185661, mean_q: 15.573680\n",
      "[74 73 89 84 82 16 40 89  4  4]\n",
      "  2610/10001: episode: 290, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  7.274, mean reward:  0.808 [-10.000,  5.783], mean action: 53.444 [4.000, 89.000],  loss: 20.138845, mae: 5.934747, mean_q: 17.141020\n",
      "[20 10  8 73 33 40 11 89  4  4]\n",
      "  2619/10001: episode: 291, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 24.506, mean reward:  2.723 [-10.000,  5.619], mean action: 30.222 [4.000, 89.000],  loss: 18.581526, mae: 5.566152, mean_q: 16.046450\n",
      "[54 15 38 93 17 32 82 44 15 42]\n",
      "  2628/10001: episode: 292, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 17.978, mean reward:  1.998 [-10.000,  5.811], mean action: 42.000 [15.000, 93.000],  loss: 17.437660, mae: 5.769410, mean_q: 16.218977\n",
      "[32 84 77 11 11 40  8 94 43 12]\n",
      "  2637/10001: episode: 293, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 21.411, mean reward:  2.379 [-10.000,  5.771], mean action: 42.222 [8.000, 94.000],  loss: 19.725727, mae: 5.584474, mean_q: 16.193865\n",
      "[99 33 44 43 82 39 78 25 25 20]\n",
      "  2646/10001: episode: 294, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 20.227, mean reward:  2.247 [-10.000,  5.268], mean action: 43.222 [20.000, 82.000],  loss: 17.984816, mae: 5.708502, mean_q: 16.176256\n",
      "[50 44 97 11 11 40 16 19 89 26]\n",
      "  2655/10001: episode: 295, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 22.070, mean reward:  2.452 [-10.000,  6.840], mean action: 39.222 [11.000, 97.000],  loss: 22.224834, mae: 5.533562, mean_q: 15.628682\n",
      "[23 94 77 35 13 73 80 73 93 40]\n",
      "  2664/10001: episode: 296, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.427, mean reward:  2.714 [-10.000,  6.774], mean action: 64.222 [13.000, 94.000],  loss: 19.132593, mae: 5.822112, mean_q: 16.216167\n",
      "[23 35 35 35 25 63 25 35 43 53]\n",
      "  2673/10001: episode: 297, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -19.974, mean reward: -2.219 [-10.000,  6.045], mean action: 38.778 [25.000, 63.000],  loss: 17.365620, mae: 5.501097, mean_q: 16.005482\n",
      "[69 11  8 92 17 17 84 49 73 77]\n",
      "  2682/10001: episode: 298, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 23.070, mean reward:  2.563 [-10.000,  6.253], mean action: 47.556 [8.000, 92.000],  loss: 16.247869, mae: 5.772443, mean_q: 16.605812\n",
      "[27 73 13 13 38 82 37 10 25 25]\n",
      "  2691/10001: episode: 299, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  7.616, mean reward:  0.846 [-10.000,  5.409], mean action: 35.111 [10.000, 82.000],  loss: 17.509880, mae: 5.737636, mean_q: 16.180099\n",
      "[22 82 80 17 17 16  8 94 94 42]\n",
      "  2700/10001: episode: 300, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  4.821, mean reward:  0.536 [-10.000,  4.738], mean action: 50.000 [8.000, 94.000],  loss: 18.627678, mae: 5.617824, mean_q: 15.870183\n",
      "[85  8 75  8 16 82 77 43 44  2]\n",
      "  2709/10001: episode: 301, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.916, mean reward:  2.880 [-10.000,  9.725], mean action: 39.444 [2.000, 82.000],  loss: 20.845833, mae: 5.918008, mean_q: 16.382711\n",
      "[ 9  2 89 82 78 94 16 65 17 40]\n",
      "  2718/10001: episode: 302, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.787, mean reward:  4.087 [ 2.618,  8.416], mean action: 53.667 [2.000, 94.000],  loss: 21.097900, mae: 5.582768, mean_q: 16.304434\n",
      "[30 10  8 92 33 94 11 65  8 40]\n",
      "  2727/10001: episode: 303, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 22.663, mean reward:  2.518 [-10.000,  7.330], mean action: 40.111 [8.000, 94.000],  loss: 20.128500, mae: 5.621920, mean_q: 16.114288\n",
      "[19 93 10 10  8 93  8  8  8 94]\n",
      "  2736/10001: episode: 304, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -35.806, mean reward: -3.978 [-10.000,  4.233], mean action: 36.889 [8.000, 94.000],  loss: 18.740211, mae: 5.848242, mean_q: 16.413273\n",
      "[21 35 89 94 68 78 93 92 73 77]\n",
      "  2745/10001: episode: 305, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 37.023, mean reward:  4.114 [ 2.356,  6.127], mean action: 77.667 [35.000, 94.000],  loss: 15.251461, mae: 5.364296, mean_q: 15.255249\n",
      "[27 35 82 40 78 11 78 35 53 40]\n",
      "  2754/10001: episode: 306, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -9.431, mean reward: -1.048 [-10.000,  3.911], mean action: 50.222 [11.000, 82.000],  loss: 16.913548, mae: 5.954688, mean_q: 16.449499\n",
      "[66 35 43 38 96 35 96 82 40 61]\n",
      "  2763/10001: episode: 307, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  7.501, mean reward:  0.833 [-10.000,  6.134], mean action: 58.444 [35.000, 96.000],  loss: 15.753386, mae: 5.259077, mean_q: 14.628252\n",
      "[15 84 82 96 73 44 11 97 26 26]\n",
      "  2772/10001: episode: 308, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.699, mean reward:  2.522 [-10.000,  5.488], mean action: 59.889 [11.000, 97.000],  loss: 16.136513, mae: 5.563707, mean_q: 15.712858\n",
      "[26 44 27 89 97 35 89 61 75 53]\n",
      "  2781/10001: episode: 309, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 18.742, mean reward:  2.082 [-10.000,  4.427], mean action: 63.333 [27.000, 97.000],  loss: 22.139837, mae: 5.694567, mean_q: 16.292280\n",
      "[76 38 73 44 84 35 40  2 45 10]\n",
      "  2790/10001: episode: 310, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 38.206, mean reward:  4.245 [ 2.615,  7.053], mean action: 41.222 [2.000, 84.000],  loss: 16.105171, mae: 5.931360, mean_q: 16.518867\n",
      "[50 53 27 27 13 97 44 44 49 10]\n",
      "  2799/10001: episode: 311, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  7.845, mean reward:  0.872 [-10.000,  6.198], mean action: 40.444 [10.000, 97.000],  loss: 18.251114, mae: 5.773613, mean_q: 16.133652\n",
      "[87 44 33 44 97 68 32 82 94 42]\n",
      "  2808/10001: episode: 312, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 18.880, mean reward:  2.098 [-10.000,  6.551], mean action: 59.556 [32.000, 97.000],  loss: 20.069517, mae: 5.734177, mean_q: 15.948009\n",
      "[70  8 13 16 38 37 84 27 82 44]\n",
      "  2817/10001: episode: 313, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.788, mean reward:  4.199 [ 2.675,  7.137], mean action: 38.778 [8.000, 84.000],  loss: 19.760124, mae: 5.317190, mean_q: 14.675878\n",
      "[76 82 80 15 38 63 16  8 19 40]\n",
      "  2826/10001: episode: 314, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 33.634, mean reward:  3.737 [ 2.320,  8.064], mean action: 40.111 [8.000, 82.000],  loss: 18.516682, mae: 5.457537, mean_q: 15.700063\n",
      "[68 10 38 33 37 94 53 19 82 77]\n",
      "  2835/10001: episode: 315, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.808, mean reward:  4.090 [ 2.190,  7.437], mean action: 49.222 [10.000, 94.000],  loss: 17.708820, mae: 5.605569, mean_q: 15.919698\n",
      "[32 73 33  2 78 65  8  8 39  2]\n",
      "  2844/10001: episode: 316, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  7.183, mean reward:  0.798 [-10.000,  4.751], mean action: 34.222 [2.000, 78.000],  loss: 16.732536, mae: 5.533700, mean_q: 15.850184\n",
      "[41 33 33 63 33 61 78 89 82 55]\n",
      "  2853/10001: episode: 317, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  6.281, mean reward:  0.698 [-10.000,  6.579], mean action: 58.556 [33.000, 89.000],  loss: 15.694855, mae: 5.787536, mean_q: 16.496569\n",
      "[82 33 44 93 39 44 40 73 32  2]\n",
      "  2862/10001: episode: 318, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.475, mean reward:  2.608 [-10.000,  9.503], mean action: 44.444 [2.000, 93.000],  loss: 19.178450, mae: 5.124160, mean_q: 14.941929\n",
      "[14 33  2 72 25 44 80 92 78 77]\n",
      "  2871/10001: episode: 319, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 35.172, mean reward:  3.908 [ 2.457,  5.664], mean action: 55.889 [2.000, 92.000],  loss: 15.131102, mae: 6.080201, mean_q: 17.421177\n",
      "[29 10 92 33 78  8  8  8  8 94]\n",
      "  2880/10001: episode: 320, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -9.283, mean reward: -1.031 [-10.000,  4.909], mean action: 37.667 [8.000, 94.000],  loss: 22.103083, mae: 5.470397, mean_q: 15.641621\n",
      "[94 15 25 33 84 92 32  2  2  2]\n",
      "  2889/10001: episode: 321, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  6.932, mean reward:  0.770 [-10.000,  7.272], mean action: 31.889 [2.000, 92.000],  loss: 18.029213, mae: 5.478071, mean_q: 15.761586\n",
      "[81 93 33  2 44 89 65 11 92 15]\n",
      "  2898/10001: episode: 322, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 33.225, mean reward:  3.692 [ 2.599,  4.631], mean action: 49.333 [2.000, 93.000],  loss: 22.768064, mae: 5.320356, mean_q: 15.540035\n",
      "[29 73 33 10 75 96 94 11 11 11]\n",
      "  2907/10001: episode: 323, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  4.925, mean reward:  0.547 [-10.000,  4.766], mean action: 46.000 [10.000, 96.000],  loss: 21.452278, mae: 5.586795, mean_q: 15.840507\n",
      "[93 92 93 38 92 11 15 93 89 40]\n",
      "  2916/10001: episode: 324, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -4.496, mean reward: -0.500 [-10.000,  7.174], mean action: 62.556 [11.000, 93.000],  loss: 19.062447, mae: 5.538699, mean_q: 15.727539\n",
      "[92 80 93 93 15 15 17 89 44 77]\n",
      "  2925/10001: episode: 325, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  8.266, mean reward:  0.918 [-10.000,  6.599], mean action: 58.111 [15.000, 93.000],  loss: 15.587976, mae: 6.385188, mean_q: 18.203993\n",
      "[19 65  8 10 93 94 37 35 89 20]\n",
      "  2934/10001: episode: 326, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.094, mean reward:  4.010 [ 2.374,  6.026], mean action: 50.111 [8.000, 94.000],  loss: 17.719353, mae: 5.546518, mean_q: 15.785909\n",
      "[51 93 82  8 10 94  8  8  8  8]\n",
      "  2943/10001: episode: 327, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -23.016, mean reward: -2.557 [-10.000,  4.049], mean action: 35.444 [8.000, 94.000],  loss: 22.868553, mae: 5.758644, mean_q: 16.502640\n",
      "[28 93 53 43 38 97 16 93 40 73]\n",
      "  2952/10001: episode: 328, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 23.095, mean reward:  2.566 [-10.000,  6.597], mean action: 60.667 [16.000, 97.000],  loss: 20.025909, mae: 5.777540, mean_q: 16.689474\n",
      "[51  2 97 96 17 17 65 84 44 77]\n",
      "  2961/10001: episode: 329, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.041, mean reward:  2.227 [-10.000,  6.505], mean action: 55.444 [2.000, 97.000],  loss: 18.737358, mae: 5.818970, mean_q: 16.113213\n",
      "[96  8  2 80 17 78 78 39 26 47]\n",
      "  2970/10001: episode: 330, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 21.827, mean reward:  2.425 [-10.000,  5.603], mean action: 41.667 [2.000, 80.000],  loss: 17.116508, mae: 6.149181, mean_q: 16.826815\n",
      "[63 73 10  2 39 96 73 27 82 40]\n",
      "  2979/10001: episode: 331, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 26.937, mean reward:  2.993 [-10.000,  6.050], mean action: 49.111 [2.000, 96.000],  loss: 19.402826, mae: 5.674044, mean_q: 16.019552\n",
      "[22 11 73 40 78 94 94 19 94 89]\n",
      "  2988/10001: episode: 332, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  6.346, mean reward:  0.705 [-10.000,  6.830], mean action: 65.778 [11.000, 94.000],  loss: 19.493490, mae: 6.003703, mean_q: 17.052612\n",
      "[26 73 84 78 39 15  2  4 65 72]\n",
      "  2997/10001: episode: 333, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 34.870, mean reward:  3.874 [ 2.522,  6.530], mean action: 48.000 [2.000, 84.000],  loss: 19.697317, mae: 6.104942, mean_q: 17.266985\n",
      "[40 10 84 40 78 11 94 72 77 40]\n",
      "  3006/10001: episode: 334, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  4.394, mean reward:  0.488 [-10.000,  5.196], mean action: 56.222 [10.000, 94.000],  loss: 15.765325, mae: 6.325725, mean_q: 18.096222\n",
      "[11 89 72 33 33 16 78 11 33 27]\n",
      "  3015/10001: episode: 335, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -6.415, mean reward: -0.713 [-10.000,  6.944], mean action: 43.556 [11.000, 89.000],  loss: 18.802046, mae: 5.402610, mean_q: 15.593981\n",
      "[43 92 33  2 25 84 94 82 82 15]\n",
      "  3024/10001: episode: 336, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 18.340, mean reward:  2.038 [-10.000,  5.762], mean action: 56.556 [2.000, 94.000],  loss: 16.799950, mae: 5.875336, mean_q: 16.368269\n",
      "[88 26 26 26 26 65 43 43 49 10]\n",
      "  3033/10001: episode: 337, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -19.371, mean reward: -2.152 [-10.000,  5.570], mean action: 34.889 [10.000, 65.000],  loss: 19.195303, mae: 6.420463, mean_q: 17.895443\n",
      "[81 15 26 97  2 17 25 26 26 44]\n",
      "  3042/10001: episode: 338, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  7.488, mean reward:  0.832 [-10.000,  5.230], mean action: 30.889 [2.000, 97.000],  loss: 24.784210, mae: 5.540791, mean_q: 15.876614\n",
      "[51 94 77 94 97 38 94 11 33 11]\n",
      "  3051/10001: episode: 339, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -1.286, mean reward: -0.143 [-10.000,  9.490], mean action: 61.000 [11.000, 97.000],  loss: 20.284346, mae: 6.127316, mean_q: 17.017130\n",
      "[30 44 43 72 97 89 82 77 10 77]\n",
      "  3060/10001: episode: 340, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 20.658, mean reward:  2.295 [-10.000,  5.507], mean action: 65.667 [10.000, 97.000],  loss: 22.288317, mae: 5.593391, mean_q: 15.771484\n",
      "[21 40 89  8  8 96 16 16 16 92]\n",
      "  3069/10001: episode: 341, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -7.982, mean reward: -0.887 [-10.000,  5.422], mean action: 42.333 [8.000, 96.000],  loss: 22.311590, mae: 5.526977, mean_q: 15.755169\n",
      "[21 92 35  6 94 78 94 89 94 26]\n",
      "  3078/10001: episode: 342, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  8.013, mean reward:  0.890 [-10.000,  6.044], mean action: 67.556 [6.000, 94.000],  loss: 19.740101, mae: 6.038511, mean_q: 16.949036\n",
      "[63 92 43 43 97 92 92 82 39 32]\n",
      "  3087/10001: episode: 343, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -8.279, mean reward: -0.920 [-10.000,  5.762], mean action: 68.000 [32.000, 97.000],  loss: 21.523342, mae: 5.136443, mean_q: 14.718365\n",
      "[20 75 92  0 35 94 37 16 16 42]\n",
      "  3096/10001: episode: 344, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 16.867, mean reward:  1.874 [-10.000,  4.871], mean action: 45.222 [0.000, 94.000],  loss: 18.089277, mae: 5.578255, mean_q: 15.600080\n",
      "[24 93 93 93 17 97 13 97 26 40]\n",
      "  3105/10001: episode: 345, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -5.099, mean reward: -0.567 [-10.000,  7.702], mean action: 63.222 [13.000, 97.000],  loss: 15.755766, mae: 5.670072, mean_q: 15.890760\n",
      "[ 2 73 80 93 33 65 96 43 92 92]\n",
      "  3114/10001: episode: 346, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.565, mean reward:  2.285 [-10.000,  5.219], mean action: 74.111 [33.000, 96.000],  loss: 18.749992, mae: 5.995208, mean_q: 16.599974\n",
      "[60  2 80 65 78 37 45 10 10 26]\n",
      "  3123/10001: episode: 347, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 23.567, mean reward:  2.619 [-10.000,  6.147], mean action: 39.222 [2.000, 80.000],  loss: 21.236647, mae: 5.801342, mean_q: 16.192751\n",
      "[90 96 43 43 73 89 65 82 77 89]\n",
      "  3132/10001: episode: 348, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  8.786, mean reward:  0.976 [-10.000,  5.937], mean action: 73.000 [43.000, 96.000],  loss: 18.628567, mae: 5.821949, mean_q: 15.826891\n",
      "[ 6 93  8 80 33 65 84 27 82 80]\n",
      "  3141/10001: episode: 349, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.310, mean reward:  2.479 [-10.000,  6.143], mean action: 61.333 [8.000, 93.000],  loss: 18.681244, mae: 5.477261, mean_q: 15.368572\n",
      "[34  2 96 78 65 45 40 45 80 15]\n",
      "  3150/10001: episode: 350, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.916, mean reward:  2.546 [-10.000,  6.159], mean action: 51.778 [2.000, 96.000],  loss: 18.521568, mae: 6.224383, mean_q: 17.178431\n",
      "[50  2 97 33 78 45 45 25 45 53]\n",
      "  3159/10001: episode: 351, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward:  8.614, mean reward:  0.957 [-10.000,  5.083], mean action: 47.000 [2.000, 97.000],  loss: 16.843691, mae: 5.922062, mean_q: 16.460644\n",
      "[ 8 92  6  6 84  2  2 16  2 84]\n",
      "  3168/10001: episode: 352, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward: -22.656, mean reward: -2.517 [-10.000,  5.359], mean action: 32.667 [2.000, 92.000],  loss: 15.402598, mae: 5.873121, mean_q: 16.450808\n",
      "[48 45 33 73 65 11 40 40 61 10]\n",
      "  3177/10001: episode: 353, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 21.680, mean reward:  2.409 [-10.000,  5.938], mean action: 42.000 [10.000, 73.000],  loss: 16.484087, mae: 6.250372, mean_q: 17.217545\n",
      "[21 92 50 33 33 97 93 45 45 10]\n",
      "  3186/10001: episode: 354, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  8.796, mean reward:  0.977 [-10.000,  5.339], mean action: 55.333 [10.000, 97.000],  loss: 17.211433, mae: 5.716266, mean_q: 15.937690\n",
      "[18 27 93 17 78 45 37 94 45 43]\n",
      "  3195/10001: episode: 355, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 21.551, mean reward:  2.395 [-10.000,  7.590], mean action: 53.222 [17.000, 94.000],  loss: 19.294636, mae: 6.494718, mean_q: 18.072645\n",
      "[58 10  8 27 47 35 11 45 45 44]\n",
      "  3204/10001: episode: 356, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 19.963, mean reward:  2.218 [-10.000,  4.975], mean action: 30.222 [8.000, 47.000],  loss: 19.257322, mae: 6.247641, mean_q: 17.679686\n",
      "[77  2 75 17 53 82 94 19 82 77]\n",
      "  3213/10001: episode: 357, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  2.481, mean reward:  0.276 [-10.000,  5.040], mean action: 55.667 [2.000, 94.000],  loss: 16.335487, mae: 6.519988, mean_q: 17.887791\n",
      "[12  2 93 82 38 82 32 92 40 11]\n",
      "  3222/10001: episode: 358, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.451, mean reward:  2.495 [-10.000,  6.303], mean action: 52.444 [2.000, 93.000],  loss: 20.097445, mae: 6.424541, mean_q: 17.538120\n",
      "[97 37  2 38  2 75 43 38 38 43]\n",
      "  3231/10001: episode: 359, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -21.931, mean reward: -2.437 [-10.000,  4.468], mean action: 35.111 [2.000, 75.000],  loss: 19.081093, mae: 5.746771, mean_q: 16.121996\n",
      "[99 37 97 55 39 17  8 55 45 11]\n",
      "  3240/10001: episode: 360, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 22.997, mean reward:  2.555 [-10.000,  6.406], mean action: 40.444 [8.000, 97.000],  loss: 20.563551, mae: 5.621965, mean_q: 15.804180\n",
      "[51 89 38  2 25 89 37 37 50 11]\n",
      "  3249/10001: episode: 361, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  8.097, mean reward:  0.900 [-10.000,  6.959], mean action: 42.000 [2.000, 89.000],  loss: 19.063028, mae: 5.936992, mean_q: 16.549997\n",
      "[26  2 73 16 45 92 94 19 82 82]\n",
      "  3258/10001: episode: 362, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 18.985, mean reward:  2.109 [-10.000,  7.104], mean action: 56.111 [2.000, 94.000],  loss: 16.853052, mae: 5.726718, mean_q: 15.875693\n",
      "[12 96 37 94 97 25 65 27  8 38]\n",
      "  3267/10001: episode: 363, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 37.070, mean reward:  4.119 [ 2.502,  7.106], mean action: 54.111 [8.000, 97.000],  loss: 18.345549, mae: 5.852724, mean_q: 15.959015\n",
      "[88 97 39 39 73 94 77 45 44 11]\n",
      "  3276/10001: episode: 364, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 22.071, mean reward:  2.452 [-10.000,  6.131], mean action: 57.667 [11.000, 97.000],  loss: 19.414625, mae: 5.874455, mean_q: 15.951470\n",
      "[99 94 33 10  2  2 44 49 49 11]\n",
      "  3285/10001: episode: 365, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  7.186, mean reward:  0.798 [-10.000,  4.894], mean action: 32.667 [2.000, 94.000],  loss: 18.846672, mae: 5.762586, mean_q: 15.704860\n",
      "[64 97 33 39 33 38 97 27 27 26]\n",
      "  3294/10001: episode: 366, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -6.252, mean reward: -0.695 [-10.000,  5.631], mean action: 46.333 [26.000, 97.000],  loss: 17.720766, mae: 6.238816, mean_q: 17.111238\n",
      "[10 93 82 33 15 82 94 25 27 27]\n",
      "  3303/10001: episode: 367, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.362, mean reward:  0.929 [-10.000,  7.003], mean action: 53.111 [15.000, 94.000],  loss: 18.784840, mae: 6.057993, mean_q: 16.775301\n",
      "[84 96  2  2  2  2  2  2  2 10]\n",
      "  3312/10001: episode: 368, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -47.711, mean reward: -5.301 [-10.000,  4.769], mean action: 13.333 [2.000, 96.000],  loss: 18.133507, mae: 5.956719, mean_q: 16.614651\n",
      "[20  2 96 78  2  2 65 47 10 40]\n",
      "  3321/10001: episode: 369, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.620, mean reward:  0.958 [-10.000,  7.456], mean action: 38.000 [2.000, 96.000],  loss: 24.208405, mae: 5.524116, mean_q: 15.787286\n",
      "[57 10 93 16 96 92 80 97 40 11]\n",
      "  3330/10001: episode: 370, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 38.182, mean reward:  4.242 [ 2.666,  6.094], mean action: 59.444 [10.000, 97.000],  loss: 16.975090, mae: 5.992270, mean_q: 16.875450\n",
      "[ 9 37 32 94 13  2  2 97 73 97]\n",
      "  3339/10001: episode: 371, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  8.412, mean reward:  0.935 [-10.000,  6.507], mean action: 49.667 [2.000, 97.000],  loss: 16.685112, mae: 6.291600, mean_q: 16.899942\n",
      "[79 82 92 61 25 25 44 80 61 41]\n",
      "  3348/10001: episode: 372, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  9.372, mean reward:  1.041 [-10.000,  6.613], mean action: 56.778 [25.000, 92.000],  loss: 22.278473, mae: 5.635582, mean_q: 15.505792\n",
      "[67 82 82 39 93 53 33 97 94 11]\n",
      "  3357/10001: episode: 373, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 22.582, mean reward:  2.509 [-10.000,  6.532], mean action: 64.889 [11.000, 97.000],  loss: 21.964121, mae: 6.475676, mean_q: 17.448605\n",
      "[56 82 65 61 73  4 61 26  8  8]\n",
      "  3366/10001: episode: 374, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  8.652, mean reward:  0.961 [-10.000,  5.410], mean action: 43.111 [4.000, 82.000],  loss: 18.786453, mae: 5.733061, mean_q: 15.817081\n",
      "[61  4 84 33 65 45 37 44 44 43]\n",
      "  3375/10001: episode: 375, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.733, mean reward:  2.526 [-10.000,  7.288], mean action: 44.333 [4.000, 84.000],  loss: 18.920437, mae: 5.592232, mean_q: 15.765773\n",
      "[28 92  2 82 73 80 82 27 27  8]\n",
      "  3384/10001: episode: 376, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  9.906, mean reward:  1.101 [-10.000,  5.628], mean action: 52.556 [2.000, 92.000],  loss: 21.647984, mae: 5.657869, mean_q: 15.958406\n",
      "[16 93 16 82 82 25 96 33 25 40]\n",
      "  3393/10001: episode: 377, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  0.709, mean reward:  0.079 [-10.000,  7.652], mean action: 54.667 [16.000, 96.000],  loss: 17.604225, mae: 5.958195, mean_q: 16.279011\n",
      "[35 96 82 78 97 65 93 82 40 10]\n",
      "  3402/10001: episode: 378, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.732, mean reward:  2.526 [-10.000,  6.332], mean action: 71.444 [10.000, 97.000],  loss: 18.961203, mae: 5.650935, mean_q: 15.219020\n",
      "[11 65 80 78  2 65 44 20 43 92]\n",
      "  3411/10001: episode: 379, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 22.808, mean reward:  2.534 [-10.000,  6.146], mean action: 54.333 [2.000, 92.000],  loss: 21.113503, mae: 5.782131, mean_q: 15.663393\n",
      "[60 73 94 38 17 84 93 82 42 20]\n",
      "  3420/10001: episode: 380, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 35.720, mean reward:  3.969 [ 2.607,  6.410], mean action: 60.333 [17.000, 94.000],  loss: 20.060091, mae: 5.978065, mean_q: 16.308298\n",
      "[38 73 80 78 44 84 94 20 20 85]\n",
      "  3429/10001: episode: 381, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 18.643, mean reward:  2.071 [-10.000,  5.877], mean action: 64.222 [20.000, 94.000],  loss: 19.129128, mae: 5.864218, mean_q: 16.007912\n",
      "[73 25  2 92 82 15 25 40 89 82]\n",
      "  3438/10001: episode: 382, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  7.991, mean reward:  0.888 [-10.000,  6.103], mean action: 50.222 [2.000, 92.000],  loss: 21.589119, mae: 5.725497, mean_q: 15.533138\n",
      "[70  2 25 82 82 45 25 45 43 84]\n",
      "  3447/10001: episode: 383, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -6.607, mean reward: -0.734 [-10.000,  5.558], mean action: 48.111 [2.000, 84.000],  loss: 18.460562, mae: 6.098639, mean_q: 16.489122\n",
      "[ 9 25 80 10 25 35 80 65 39 40]\n",
      "  3456/10001: episode: 384, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 10.260, mean reward:  1.140 [-10.000,  8.265], mean action: 44.333 [10.000, 80.000],  loss: 21.172981, mae: 5.871868, mean_q: 16.054277\n",
      "[88 44 44 43 43 92 72 20 27 20]\n",
      "  3465/10001: episode: 385, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -5.321, mean reward: -0.591 [-10.000,  7.872], mean action: 45.000 [20.000, 92.000],  loss: 17.545162, mae: 5.654938, mean_q: 15.735862\n",
      "[57 44 43 44 25 73 32 82 20 43]\n",
      "  3474/10001: episode: 386, duration: 0.056s, episode steps:   9, steps per second: 160, episode reward:  7.304, mean reward:  0.812 [-10.000,  6.236], mean action: 45.111 [20.000, 82.000],  loss: 14.497045, mae: 6.549216, mean_q: 18.415300\n",
      "[68 96 33 43 96 96 35 20 20 39]\n",
      "  3483/10001: episode: 387, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: -5.867, mean reward: -0.652 [-10.000,  5.273], mean action: 53.111 [20.000, 96.000],  loss: 19.273502, mae: 5.550595, mean_q: 15.656662\n",
      "[45 40 27 10 73 89 35 27 47 94]\n",
      "  3492/10001: episode: 388, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: 17.164, mean reward:  1.907 [-10.000,  4.154], mean action: 49.111 [10.000, 94.000],  loss: 16.265236, mae: 5.915163, mean_q: 16.287891\n",
      "[78  2 73 33 82 45 80 20 22 44]\n",
      "  3501/10001: episode: 389, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 34.657, mean reward:  3.851 [ 2.601,  4.922], mean action: 44.556 [2.000, 82.000],  loss: 17.029932, mae: 6.075535, mean_q: 16.615967\n",
      "[90 15 38 39 65 49 93 53 20 94]\n",
      "  3510/10001: episode: 390, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 34.049, mean reward:  3.783 [ 2.607,  5.085], mean action: 51.778 [15.000, 94.000],  loss: 20.910728, mae: 5.809130, mean_q: 16.035965\n",
      "[98 93  8 33 93 80 16 80 43 20]\n",
      "  3519/10001: episode: 391, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  9.846, mean reward:  1.094 [-10.000,  6.284], mean action: 51.778 [8.000, 93.000],  loss: 16.505997, mae: 5.997248, mean_q: 16.380373\n",
      "[ 3  8 65 15 16 16 77 20  5 20]\n",
      "  3528/10001: episode: 392, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  5.736, mean reward:  0.637 [-10.000,  4.710], mean action: 26.889 [5.000, 77.000],  loss: 18.263641, mae: 5.704957, mean_q: 15.900375\n",
      "[ 5  8 80 15 93 49 49 25 53 20]\n",
      "  3537/10001: episode: 393, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 23.411, mean reward:  2.601 [-10.000,  6.537], mean action: 43.556 [8.000, 93.000],  loss: 19.364996, mae: 5.999586, mean_q: 16.602516\n",
      "[ 8 10 25 15 13 65 84 89 12 39]\n",
      "  3546/10001: episode: 394, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 31.596, mean reward:  3.511 [ 2.585,  5.374], mean action: 39.111 [10.000, 89.000],  loss: 19.313316, mae: 5.677188, mean_q: 15.734776\n",
      "[94  2 82 33 16 82 77 11 11 61]\n",
      "  3555/10001: episode: 395, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  6.870, mean reward:  0.763 [-10.000,  4.902], mean action: 41.667 [2.000, 82.000],  loss: 17.105473, mae: 5.363736, mean_q: 14.595296\n",
      "[38  8 25 82 78 11 11 89 15 77]\n",
      "  3564/10001: episode: 396, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 21.952, mean reward:  2.439 [-10.000,  6.158], mean action: 44.000 [8.000, 89.000],  loss: 19.902016, mae: 5.810969, mean_q: 15.706080\n",
      "[57 68 10 50 78 96  8 89 82  2]\n",
      "  3573/10001: episode: 397, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.261, mean reward:  4.696 [ 2.090,  9.332], mean action: 53.667 [2.000, 96.000],  loss: 19.562450, mae: 5.359063, mean_q: 15.049896\n",
      "[46 82 50 10 97 32 16 49 89  2]\n",
      "  3582/10001: episode: 398, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 37.511, mean reward:  4.168 [ 1.681,  9.831], mean action: 47.444 [2.000, 97.000],  loss: 18.265734, mae: 5.545590, mean_q: 15.110148\n",
      "[83 96 33  2 10  8 50 20 78  2]\n",
      "  3591/10001: episode: 399, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 18.636, mean reward:  2.071 [-10.000,  4.596], mean action: 33.222 [2.000, 96.000],  loss: 18.984911, mae: 6.134928, mean_q: 16.518990\n",
      "[55  2  8  8 24 72 92 11 20 82]\n",
      "  3600/10001: episode: 400, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 24.795, mean reward:  2.755 [-10.000,  8.090], mean action: 35.444 [2.000, 92.000],  loss: 19.953514, mae: 5.409235, mean_q: 15.134831\n",
      "[68 65  8 39 78  2 77 89 11 11]\n",
      "  3609/10001: episode: 401, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 23.965, mean reward:  2.663 [-10.000,  6.193], mean action: 42.222 [2.000, 89.000],  loss: 18.594757, mae: 6.136793, mean_q: 16.772186\n",
      "[35 68 77 77 39 65 97 27 40 26]\n",
      "  3618/10001: episode: 402, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 27.157, mean reward:  3.017 [-10.000,  7.024], mean action: 57.333 [26.000, 97.000],  loss: 15.793132, mae: 5.441259, mean_q: 15.099331\n",
      "[65  2 25  2 97 92 68 82 40 41]\n",
      "  3627/10001: episode: 403, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 24.566, mean reward:  2.730 [-10.000,  6.442], mean action: 49.889 [2.000, 97.000],  loss: 17.863930, mae: 5.605764, mean_q: 15.645667\n",
      "[77  2 25 38 53 24 92 53 94 26]\n",
      "  3636/10001: episode: 404, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 19.334, mean reward:  2.148 [-10.000,  5.766], mean action: 45.222 [2.000, 94.000],  loss: 17.779716, mae: 5.905162, mean_q: 16.333416\n",
      "[59 73 33  2  2 78 43 77 43 32]\n",
      "  3645/10001: episode: 405, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  4.040, mean reward:  0.449 [-10.000,  4.266], mean action: 42.556 [2.000, 78.000],  loss: 18.646250, mae: 6.020336, mean_q: 16.485191\n",
      "[ 8 27 35 33 78 92 93 27 92  2]\n",
      "  3654/10001: episode: 406, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 10.751, mean reward:  1.195 [-10.000,  8.405], mean action: 53.222 [2.000, 93.000],  loss: 17.482309, mae: 5.817992, mean_q: 15.863163\n",
      "[32 73 82 96 17 15 17 47 24 73]\n",
      "  3663/10001: episode: 407, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  4.580, mean reward:  0.509 [-10.000,  4.033], mean action: 49.333 [15.000, 96.000],  loss: 14.814855, mae: 6.067627, mean_q: 16.805862\n",
      "[61  2 92 17 53 37 37 20 92 40]\n",
      "  3672/10001: episode: 408, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward:  8.482, mean reward:  0.942 [-10.000,  8.297], mean action: 43.333 [2.000, 92.000],  loss: 15.665295, mae: 6.204317, mean_q: 16.518454\n",
      "[54 93 25  2 82 20  2 20 61 94]\n",
      "  3681/10001: episode: 409, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  4.559, mean reward:  0.507 [-10.000,  4.500], mean action: 44.333 [2.000, 94.000],  loss: 18.294186, mae: 5.760040, mean_q: 15.744673\n",
      "[ 5 93 10 84 73 73 73 27 44  8]\n",
      "  3690/10001: episode: 410, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  8.740, mean reward:  0.971 [-10.000,  6.002], mean action: 53.889 [8.000, 93.000],  loss: 19.761604, mae: 6.132936, mean_q: 16.447212\n",
      "[66 65 10 27 96 72 61  2 25 92]\n",
      "  3699/10001: episode: 411, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.155, mean reward:  4.017 [ 2.118,  7.018], mean action: 50.000 [2.000, 96.000],  loss: 17.591703, mae: 5.815911, mean_q: 15.670485\n",
      "[31 43 27 10 25 92 25 72 89  2]\n",
      "  3708/10001: episode: 412, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 25.931, mean reward:  2.881 [-10.000,  9.067], mean action: 42.778 [2.000, 92.000],  loss: 17.118200, mae: 5.874527, mean_q: 15.919911\n",
      "[18 10 97 35 53 53 55 49 25 92]\n",
      "  3717/10001: episode: 413, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 22.885, mean reward:  2.543 [-10.000,  7.290], mean action: 52.111 [10.000, 97.000],  loss: 17.744473, mae: 5.938442, mean_q: 16.461338\n",
      "[85  2 43 65 78 11 35 25 92 53]\n",
      "  3726/10001: episode: 414, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 34.222, mean reward:  3.802 [ 2.263,  5.771], mean action: 44.889 [2.000, 92.000],  loss: 17.339529, mae: 5.905072, mean_q: 16.061718\n",
      "[25 97 10  8 53  8 77 25 92 40]\n",
      "  3735/10001: episode: 415, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  9.090, mean reward:  1.010 [-10.000,  7.544], mean action: 45.556 [8.000, 97.000],  loss: 18.597881, mae: 5.690440, mean_q: 15.263548\n",
      "[ 0  8 72 16 93 42 80 84 92 94]\n",
      "  3744/10001: episode: 416, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 32.873, mean reward:  3.653 [ 2.646,  5.443], mean action: 64.556 [8.000, 94.000],  loss: 18.378494, mae: 5.569396, mean_q: 15.237717\n",
      "[80 96 16  2 27  2 65 44 40 94]\n",
      "  3753/10001: episode: 417, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.658, mean reward:  2.295 [-10.000,  5.851], mean action: 42.889 [2.000, 96.000],  loss: 14.837215, mae: 6.362959, mean_q: 16.950743\n",
      "[29 89  8  2 65 61 61 25 47 94]\n",
      "  3762/10001: episode: 418, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 18.918, mean reward:  2.102 [-10.000,  5.211], mean action: 50.222 [2.000, 94.000],  loss: 19.025198, mae: 6.278633, mean_q: 17.002586\n",
      "[41  8  2 97 78 92  2 43  2 75]\n",
      "  3771/10001: episode: 419, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  6.591, mean reward:  0.732 [-10.000,  4.831], mean action: 44.333 [2.000, 97.000],  loss: 16.941860, mae: 6.064151, mean_q: 16.247654\n",
      "[56 15  2 82 78 65 33  2 11 11]\n",
      "  3780/10001: episode: 420, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  8.767, mean reward:  0.974 [-10.000,  7.054], mean action: 33.222 [2.000, 82.000],  loss: 15.887183, mae: 6.016586, mean_q: 16.295870\n",
      "[75 15  2  2 97 15 11 10 15 92]\n",
      "  3789/10001: episode: 421, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: -5.313, mean reward: -0.590 [-10.000,  5.073], mean action: 28.778 [2.000, 97.000],  loss: 18.660400, mae: 6.110290, mean_q: 16.581919\n",
      "[73 82 10 92 15 73 75 26 26 20]\n",
      "  3798/10001: episode: 422, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward:  4.035, mean reward:  0.448 [-10.000,  4.448], mean action: 46.556 [10.000, 92.000],  loss: 18.679365, mae: 6.065140, mean_q: 16.420906\n",
      "[70 93 93  8 65 94 32 32 53 26]\n",
      "  3807/10001: episode: 423, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward:  6.840, mean reward:  0.760 [-10.000,  7.052], mean action: 55.111 [8.000, 94.000],  loss: 19.044260, mae: 6.142065, mean_q: 16.781059\n",
      "[87 24 73 33 13 82 84 97 77 12]\n",
      "  3816/10001: episode: 424, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward: 33.759, mean reward:  3.751 [ 2.250,  4.965], mean action: 55.000 [12.000, 97.000],  loss: 19.289989, mae: 6.582916, mean_q: 17.626652\n",
      "[14 92  2 39 97 61 77 26 94 22]\n",
      "  3825/10001: episode: 425, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 32.854, mean reward:  3.650 [ 2.691,  5.079], mean action: 56.667 [2.000, 97.000],  loss: 17.982536, mae: 5.589235, mean_q: 15.070577\n",
      "[31 92  8 32 49  8 84 26 40 41]\n",
      "  3834/10001: episode: 426, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 23.983, mean reward:  2.665 [-10.000,  6.659], mean action: 42.222 [8.000, 92.000],  loss: 16.419209, mae: 6.278989, mean_q: 16.542536\n",
      "[16 92 77 77 82 39 84 26 20 10]\n",
      "  3843/10001: episode: 427, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.940, mean reward:  2.438 [-10.000,  5.547], mean action: 56.333 [10.000, 92.000],  loss: 20.342737, mae: 6.088497, mean_q: 16.002846\n",
      "[37 96 38  8 49 96 25 20 93 12]\n",
      "  3852/10001: episode: 428, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.411, mean reward:  2.490 [-10.000,  6.037], mean action: 48.556 [8.000, 96.000],  loss: 19.833815, mae: 6.519426, mean_q: 17.086159\n",
      "[ 7 92 80 72 38 40 25 92 94 94]\n",
      "  3861/10001: episode: 429, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  5.948, mean reward:  0.661 [-10.000,  4.992], mean action: 69.667 [25.000, 94.000],  loss: 19.166258, mae: 6.450079, mean_q: 17.085672\n",
      "[16 39 72 40 78 35 40 89 47 20]\n",
      "  3870/10001: episode: 430, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 19.926, mean reward:  2.214 [-10.000,  5.028], mean action: 51.111 [20.000, 89.000],  loss: 18.449385, mae: 6.082714, mean_q: 16.033962\n",
      "[39  8  2 97 82 92 80 72 75 77]\n",
      "  3879/10001: episode: 431, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 35.996, mean reward:  4.000 [ 2.394,  5.560], mean action: 65.000 [2.000, 97.000],  loss: 18.265591, mae: 6.519501, mean_q: 16.811943\n",
      "[63 15 10 73 84 10 93 35 11 47]\n",
      "  3888/10001: episode: 432, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 20.951, mean reward:  2.328 [-10.000,  5.372], mean action: 42.000 [10.000, 93.000],  loss: 16.571938, mae: 5.689724, mean_q: 15.066023\n",
      "[82 73 13 75  4 33 78 50 22 12]\n",
      "  3897/10001: episode: 433, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 31.676, mean reward:  3.520 [ 2.416,  5.986], mean action: 40.000 [4.000, 78.000],  loss: 17.750519, mae: 6.554945, mean_q: 17.123922\n",
      "[42 78 78 40 89 53 93 47 35 94]\n",
      "  3906/10001: episode: 434, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 21.340, mean reward:  2.371 [-10.000,  4.774], mean action: 67.444 [35.000, 94.000],  loss: 18.696707, mae: 5.689625, mean_q: 15.058086\n",
      "[66 40 33 38 13 89 68 82 15 41]\n",
      "  3915/10001: episode: 435, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 34.217, mean reward:  3.802 [ 2.032,  6.214], mean action: 46.556 [13.000, 89.000],  loss: 15.985576, mae: 6.483862, mean_q: 16.700893\n",
      "[94  2 33 33 93 92 80 82 55 43]\n",
      "  3924/10001: episode: 436, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 23.688, mean reward:  2.632 [-10.000,  6.182], mean action: 57.000 [2.000, 93.000],  loss: 17.341238, mae: 6.698079, mean_q: 17.240461\n",
      "[83 65  8 93  4  4  4  4  4  4]\n",
      "  3933/10001: episode: 437, duration: 0.056s, episode steps:   9, steps per second: 159, episode reward: -35.501, mean reward: -3.945 [-10.000,  4.792], mean action: 21.111 [4.000, 93.000],  loss: 16.351768, mae: 6.539831, mean_q: 16.777569\n",
      "[53 24 80 47 80 92 68 82 92  2]\n",
      "  3942/10001: episode: 438, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: 11.102, mean reward:  1.234 [-10.000,  9.425], mean action: 63.000 [2.000, 92.000],  loss: 18.559296, mae: 6.450316, mean_q: 17.225992\n",
      "[88 24 73 38 78 93 55 82 82 94]\n",
      "  3951/10001: episode: 439, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 19.291, mean reward:  2.143 [-10.000,  6.423], mean action: 68.778 [24.000, 94.000],  loss: 20.923912, mae: 6.344943, mean_q: 16.479275\n",
      "[98 65 25 61 53  0 84 26  8 40]\n",
      "  3960/10001: episode: 440, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 37.217, mean reward:  4.135 [ 2.354,  8.483], mean action: 40.222 [0.000, 84.000],  loss: 18.935020, mae: 7.107997, mean_q: 17.881207\n",
      "[67 24 27 84 53 92 93 82 26  2]\n",
      "  3969/10001: episode: 441, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 38.812, mean reward:  4.312 [ 2.454,  8.106], mean action: 53.667 [2.000, 93.000],  loss: 16.983852, mae: 6.118586, mean_q: 15.738198\n",
      "[90 96 97 39 25 78 84 10 47 11]\n",
      "  3978/10001: episode: 442, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.443, mean reward:  3.938 [ 2.897,  6.276], mean action: 54.111 [10.000, 97.000],  loss: 17.173882, mae: 6.440726, mean_q: 16.178413\n",
      "[ 0 24 72 84 97 11 92 10 61 40]\n",
      "  3987/10001: episode: 443, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 37.751, mean reward:  4.195 [ 2.649,  8.158], mean action: 54.556 [10.000, 97.000],  loss: 15.626410, mae: 6.103204, mean_q: 15.354488\n",
      "[48 96 92 84 38 65 93 92 77 53]\n",
      "  3996/10001: episode: 444, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 19.258, mean reward:  2.140 [-10.000,  5.428], mean action: 76.667 [38.000, 96.000],  loss: 19.933800, mae: 6.177999, mean_q: 15.825352\n",
      "[88 78 92 96 13 53 35 92 43 42]\n",
      "  4005/10001: episode: 445, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 19.943, mean reward:  2.216 [-10.000,  6.079], mean action: 60.444 [13.000, 96.000],  loss: 16.385141, mae: 5.984550, mean_q: 15.587413\n",
      "[22 80 72 78 97 39 44 10 40 44]\n",
      "  4014/10001: episode: 446, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 21.427, mean reward:  2.381 [-10.000,  6.565], mean action: 56.000 [10.000, 97.000],  loss: 19.102583, mae: 6.191074, mean_q: 16.206528\n",
      "[44 65  8 92 17 65 78 47 80 10]\n",
      "  4023/10001: episode: 447, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 23.778, mean reward:  2.642 [-10.000,  6.831], mean action: 51.333 [8.000, 92.000],  loss: 16.778284, mae: 6.225797, mean_q: 16.260456\n",
      "[29 93 39 93  4  4 38 73 92 22]\n",
      "  4032/10001: episode: 448, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  4.902, mean reward:  0.545 [-10.000,  4.612], mean action: 50.889 [4.000, 93.000],  loss: 14.390194, mae: 6.647646, mean_q: 16.979004\n",
      "[35 78 50 65 97 40 40 22 40 40]\n",
      "  4041/10001: episode: 449, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: -10.274, mean reward: -1.142 [-10.000,  5.193], mean action: 52.444 [22.000, 97.000],  loss: 17.247444, mae: 6.454621, mean_q: 16.640127\n",
      "[88  2 80 45 68 11 40 10 11 32]\n",
      "  4050/10001: episode: 450, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward: 18.075, mean reward:  2.008 [-10.000,  4.852], mean action: 33.222 [2.000, 80.000],  loss: 16.687275, mae: 6.298129, mean_q: 16.303114\n",
      "[67 78 10 80 33  4 44 93 40 50]\n",
      "  4059/10001: episode: 451, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 34.990, mean reward:  3.888 [ 2.250,  5.525], mean action: 48.000 [4.000, 93.000],  loss: 16.279045, mae: 6.295070, mean_q: 16.269796\n",
      "[89 84  8 43 89 39 55 10 77 40]\n",
      "  4068/10001: episode: 452, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 22.872, mean reward:  2.541 [-10.000,  7.245], mean action: 49.444 [8.000, 89.000],  loss: 14.183438, mae: 6.385511, mean_q: 16.670677\n",
      "[ 2 73 72 77  4  4 17 73  4 75]\n",
      "  4077/10001: episode: 453, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -8.889, mean reward: -0.988 [-10.000,  4.525], mean action: 44.333 [4.000, 77.000],  loss: 14.923874, mae: 6.581535, mean_q: 16.905807\n",
      "[32 63  8  8 80 65 25 77  5 22]\n",
      "  4086/10001: episode: 454, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 21.147, mean reward:  2.350 [-10.000,  4.890], mean action: 39.222 [5.000, 80.000],  loss: 18.185232, mae: 6.308213, mean_q: 16.180567\n",
      "[90 24 73 65 96 92 25 89  2 22]\n",
      "  4095/10001: episode: 455, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 37.518, mean reward:  4.169 [ 2.432,  7.661], mean action: 54.222 [2.000, 96.000],  loss: 17.824163, mae: 7.116315, mean_q: 18.137400\n",
      "[15 11 73 82 96 24 24 89 40 40]\n",
      "  4104/10001: episode: 456, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  9.263, mean reward:  1.029 [-10.000,  6.361], mean action: 53.222 [11.000, 96.000],  loss: 18.712473, mae: 6.598639, mean_q: 16.840498\n",
      "[31 40 92 37 80 40 93 26 77 85]\n",
      "  4113/10001: episode: 457, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 23.707, mean reward:  2.634 [-10.000,  5.206], mean action: 63.333 [26.000, 93.000],  loss: 17.166031, mae: 6.475085, mean_q: 16.544376\n",
      "[56 92 80 15 93 94 80 47 92 77]\n",
      "  4122/10001: episode: 458, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward:  5.788, mean reward:  0.643 [-10.000,  5.818], mean action: 74.444 [15.000, 94.000],  loss: 18.884386, mae: 6.637896, mean_q: 17.466873\n",
      "[ 9 92 84 84  4 38 25  5  5 89]\n",
      "  4131/10001: episode: 459, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.100, mean reward:  1.011 [-10.000,  6.534], mean action: 47.333 [4.000, 92.000],  loss: 17.390985, mae: 6.096189, mean_q: 15.904548\n",
      "[17 92 37 84 25 89 44  5 93 26]\n",
      "  4140/10001: episode: 460, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 35.067, mean reward:  3.896 [ 2.700,  6.039], mean action: 55.000 [5.000, 93.000],  loss: 17.402376, mae: 6.648163, mean_q: 17.563885\n",
      "[36 92 89 96 44 80 93  5  5  5]\n",
      "  4149/10001: episode: 461, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  5.125, mean reward:  0.569 [-10.000,  4.681], mean action: 56.556 [5.000, 96.000],  loss: 16.848848, mae: 6.505732, mean_q: 16.705969\n",
      "[43 93 82 84 38 37 89 26 73 41]\n",
      "  4158/10001: episode: 462, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 35.984, mean reward:  3.998 [ 2.432,  5.587], mean action: 62.556 [26.000, 93.000],  loss: 19.418032, mae: 6.458704, mean_q: 16.225880\n",
      "[11 84 84 11 96 93 93 26 19 26]\n",
      "  4167/10001: episode: 463, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -22.621, mean reward: -2.513 [-10.000,  4.815], mean action: 59.111 [11.000, 96.000],  loss: 15.693749, mae: 6.891755, mean_q: 17.182775\n",
      "[18 93 84 33 13 73 93 27 97 40]\n",
      "  4176/10001: episode: 464, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.495, mean reward:  2.833 [-10.000,  6.495], mean action: 61.444 [13.000, 97.000],  loss: 14.885973, mae: 6.409623, mean_q: 16.257149\n",
      "[73 93 27 38 93 45 97 22 12 84]\n",
      "  4185/10001: episode: 465, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 17.060, mean reward:  1.896 [-10.000,  4.207], mean action: 56.778 [12.000, 97.000],  loss: 17.005774, mae: 6.925245, mean_q: 17.181061\n",
      "[57  2 93 13 61 10 93 72 40 22]\n",
      "  4194/10001: episode: 466, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 19.312, mean reward:  2.146 [-10.000,  6.949], mean action: 45.111 [2.000, 93.000],  loss: 18.052057, mae: 6.367795, mean_q: 16.351393\n",
      "[54 39 63 33 78 11 93 53 40 41]\n",
      "  4203/10001: episode: 467, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 37.775, mean reward:  4.197 [ 2.252,  6.232], mean action: 50.111 [11.000, 93.000],  loss: 16.109680, mae: 6.682498, mean_q: 16.733315\n",
      "[47 10 25 33 78 45 89 82 44 43]\n",
      "  4212/10001: episode: 468, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.140, mean reward:  4.349 [ 2.906,  6.456], mean action: 49.889 [10.000, 89.000],  loss: 19.770582, mae: 6.109120, mean_q: 15.364607\n",
      "[37 94 65 94 38 17 61 10 77 41]\n",
      "  4221/10001: episode: 469, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 23.050, mean reward:  2.561 [-10.000,  6.638], mean action: 55.222 [10.000, 94.000],  loss: 17.326771, mae: 6.638956, mean_q: 16.806675\n",
      "[81 94 75 68 78 77 35 82 53 26]\n",
      "  4230/10001: episode: 470, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.251, mean reward:  4.028 [ 2.394,  6.546], mean action: 65.333 [26.000, 94.000],  loss: 14.645504, mae: 6.058805, mean_q: 15.634277\n",
      "[26 73 80 33 33 96 77 19 22 40]\n",
      "  4239/10001: episode: 471, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.461, mean reward:  2.273 [-10.000,  6.847], mean action: 52.556 [19.000, 96.000],  loss: 18.360052, mae: 6.354726, mean_q: 16.759596\n",
      "[11 10 80 45 17 42 78 89 10  2]\n",
      "  4248/10001: episode: 472, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 24.587, mean reward:  2.732 [-10.000,  9.180], mean action: 41.444 [2.000, 89.000],  loss: 13.675529, mae: 6.763592, mean_q: 16.989464\n",
      "[24 39 50 33 13 80 73 22 22 43]\n",
      "  4257/10001: episode: 473, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 23.014, mean reward:  2.557 [-10.000,  6.792], mean action: 41.667 [13.000, 80.000],  loss: 18.476536, mae: 6.611475, mean_q: 16.712486\n",
      "[14 15 82 11 65 93 39 10 22 44]\n",
      "  4266/10001: episode: 474, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 33.719, mean reward:  3.747 [ 2.610,  4.811], mean action: 42.333 [10.000, 93.000],  loss: 15.955521, mae: 6.928463, mean_q: 17.279667\n",
      "[70 94 43  2 97 15 10 53 44 77]\n",
      "  4275/10001: episode: 475, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 36.074, mean reward:  4.008 [ 2.517,  5.819], mean action: 48.333 [2.000, 97.000],  loss: 14.472753, mae: 6.826303, mean_q: 17.202837\n",
      "[80 68 27 63 61 65 92 77 61 41]\n",
      "  4284/10001: episode: 476, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.996, mean reward:  2.333 [-10.000,  7.038], mean action: 61.667 [27.000, 92.000],  loss: 15.159943, mae: 6.578260, mean_q: 16.688053\n",
      "[ 6 89 72 45 73 65 77 11 10 10]\n",
      "  4293/10001: episode: 477, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 20.284, mean reward:  2.254 [-10.000,  5.069], mean action: 50.222 [10.000, 89.000],  loss: 14.776433, mae: 6.899439, mean_q: 17.363817\n",
      "[70 39 92 96 82 89 93  8 92 12]\n",
      "  4302/10001: episode: 478, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.960, mean reward:  2.329 [-10.000,  4.721], mean action: 67.000 [8.000, 96.000],  loss: 14.795873, mae: 6.372849, mean_q: 16.341763\n",
      "[72 73  8 92 78 15 72 10  8 92]\n",
      "  4311/10001: episode: 479, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: -8.566, mean reward: -0.952 [-10.000,  4.937], mean action: 49.778 [8.000, 92.000],  loss: 17.316525, mae: 6.849441, mean_q: 17.435814\n",
      "[35 92 72 94 25 25 75 92 78 77]\n",
      "  4320/10001: episode: 480, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward:  6.314, mean reward:  0.702 [-10.000,  6.662], mean action: 70.000 [25.000, 94.000],  loss: 15.149543, mae: 6.918112, mean_q: 17.443865\n",
      "[15 39 45 19 13 15 96 10 10 26]\n",
      "  4329/10001: episode: 481, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward:  8.122, mean reward:  0.902 [-10.000,  6.570], mean action: 30.333 [10.000, 96.000],  loss: 14.704609, mae: 6.687842, mean_q: 17.000137\n",
      "[75  8 75 33 82 94 93 10 27 55]\n",
      "  4338/10001: episode: 482, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 23.313, mean reward:  2.590 [-10.000,  6.758], mean action: 53.000 [8.000, 94.000],  loss: 15.842156, mae: 6.617304, mean_q: 16.944359\n",
      "[61 39  8 25 25  8 93 44 80 40]\n",
      "  4347/10001: episode: 483, duration: 0.072s, episode steps:   9, steps per second: 126, episode reward: 13.100, mean reward:  1.456 [-10.000,  7.166], mean action: 40.222 [8.000, 93.000],  loss: 16.804041, mae: 6.624939, mean_q: 17.009943\n",
      "[46 10 78 96 44 45 89 89 61 40]\n",
      "  4356/10001: episode: 484, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 21.867, mean reward:  2.430 [-10.000,  7.126], mean action: 61.333 [10.000, 96.000],  loss: 17.260410, mae: 6.774169, mean_q: 17.286158\n",
      "[26 10 78 84 84 65 65 27 10 92]\n",
      "  4365/10001: episode: 485, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -6.758, mean reward: -0.751 [-10.000,  7.325], mean action: 57.222 [10.000, 92.000],  loss: 15.686788, mae: 6.370540, mean_q: 16.253843\n",
      "[93  8 82 96 84 45 82 49 82 40]\n",
      "  4374/10001: episode: 486, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  6.977, mean reward:  0.775 [-10.000,  6.310], mean action: 63.111 [8.000, 96.000],  loss: 16.511885, mae: 6.968165, mean_q: 17.192173\n",
      "[89 65 65 65 40 82 47 68 94 40]\n",
      "  4383/10001: episode: 487, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -8.296, mean reward: -0.922 [-10.000,  5.295], mean action: 62.889 [40.000, 94.000],  loss: 16.833183, mae: 6.962709, mean_q: 17.397270\n",
      "[35 89 15 63 65 35 50 92 89 12]\n",
      "  4392/10001: episode: 488, duration: 0.063s, episode steps:   9, steps per second: 144, episode reward:  2.418, mean reward:  0.269 [-10.000,  4.829], mean action: 56.667 [12.000, 92.000],  loss: 16.471336, mae: 6.531551, mean_q: 16.407045\n",
      "[84 73 82 40 78 93 94 11 82 41]\n",
      "  4401/10001: episode: 489, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 21.939, mean reward:  2.438 [-10.000,  5.557], mean action: 66.000 [11.000, 94.000],  loss: 20.748861, mae: 6.891801, mean_q: 17.253838\n",
      "[16 89 39 80 84 75 97 40 12 41]\n",
      "  4410/10001: episode: 490, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.652, mean reward:  4.072 [ 2.773,  6.373], mean action: 61.889 [12.000, 97.000],  loss: 18.193430, mae: 6.434548, mean_q: 16.426344\n",
      "[ 4 92 77 19 78 94 93 11 11 10]\n",
      "  4419/10001: episode: 491, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 20.341, mean reward:  2.260 [-10.000,  5.766], mean action: 53.889 [10.000, 94.000],  loss: 15.358196, mae: 6.838158, mean_q: 17.236824\n",
      "[29 40 40 80 84 89 16  8  8 89]\n",
      "  4428/10001: episode: 492, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -9.710, mean reward: -1.079 [-10.000,  4.696], mean action: 50.444 [8.000, 89.000],  loss: 17.787863, mae: 6.471241, mean_q: 16.324638\n",
      "[86 37 75 11 11 89 19 77 10 22]\n",
      "  4437/10001: episode: 493, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.960, mean reward:  2.329 [-10.000,  5.729], mean action: 39.000 [10.000, 89.000],  loss: 15.801105, mae: 7.105352, mean_q: 17.699053\n",
      "[93 50 75 43 89 94 77 11 10 84]\n",
      "  4446/10001: episode: 494, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 33.743, mean reward:  3.749 [ 1.905,  5.331], mean action: 59.222 [10.000, 94.000],  loss: 17.346077, mae: 6.859426, mean_q: 17.151108\n",
      "[52 89  8  8 84 43  2 65 35 40]\n",
      "  4455/10001: episode: 495, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.611, mean reward:  2.735 [-10.000,  7.498], mean action: 41.556 [2.000, 89.000],  loss: 16.241188, mae: 7.235366, mean_q: 17.893991\n",
      "[60  2 75 33 61 45 10 43 80 85]\n",
      "  4464/10001: episode: 496, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 41.068, mean reward:  4.563 [ 2.329,  6.669], mean action: 48.222 [2.000, 85.000],  loss: 17.528646, mae: 6.999188, mean_q: 17.317568\n",
      "[96  4 25 78 53  5  5 93 42  2]\n",
      "  4473/10001: episode: 497, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 24.438, mean reward:  2.715 [-10.000,  9.503], mean action: 34.111 [2.000, 93.000],  loss: 19.143011, mae: 6.924607, mean_q: 17.411905\n",
      "[70 24 78 84 84 93 93 26 43 55]\n",
      "  4482/10001: episode: 498, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  8.276, mean reward:  0.920 [-10.000,  6.271], mean action: 64.444 [24.000, 93.000],  loss: 17.628336, mae: 6.552721, mean_q: 16.688683\n",
      "[29 89 35 89 25 13 33 12 89 85]\n",
      "  4491/10001: episode: 499, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  7.435, mean reward:  0.826 [-10.000,  6.967], mean action: 52.222 [12.000, 89.000],  loss: 14.958523, mae: 6.769447, mean_q: 17.781799\n",
      "[45 24 92 96 25 17 39 12 12 39]\n",
      "  4500/10001: episode: 500, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  2.848, mean reward:  0.316 [-10.000,  3.815], mean action: 39.556 [12.000, 96.000],  loss: 17.667437, mae: 6.778029, mean_q: 18.022545\n",
      "[11 92 97 93 25 82 78 12 12 12]\n",
      "  4509/10001: episode: 501, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  5.131, mean reward:  0.570 [-10.000,  5.028], mean action: 55.889 [12.000, 97.000],  loss: 17.458340, mae: 6.711571, mean_q: 17.816372\n",
      "[86 24 63 49 47 93 77 82 41 85]\n",
      "  4518/10001: episode: 502, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 39.487, mean reward:  4.387 [ 2.303,  6.591], mean action: 62.333 [24.000, 93.000],  loss: 21.965393, mae: 7.233585, mean_q: 18.924313\n",
      "[38 49 89 38 82 93 93 12 10 39]\n",
      "  4527/10001: episode: 503, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  7.593, mean reward:  0.844 [-10.000,  5.000], mean action: 56.111 [10.000, 93.000],  loss: 22.584145, mae: 6.697200, mean_q: 17.774626\n",
      "[32 63 63 33 93 33 93 12 10 12]\n",
      "  4536/10001: episode: 504, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -20.234, mean reward: -2.248 [-10.000,  5.487], mean action: 45.778 [10.000, 93.000],  loss: 19.644176, mae: 6.834219, mean_q: 17.538454\n",
      "[59 24 33 92 96 96 94 11 10 10]\n",
      "  4545/10001: episode: 505, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  5.443, mean reward:  0.605 [-10.000,  4.945], mean action: 51.778 [10.000, 96.000],  loss: 18.435555, mae: 6.993197, mean_q: 17.596138\n",
      "[80 24 27 97 96  8 10 92 15 40]\n",
      "  4554/10001: episode: 506, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 35.904, mean reward:  3.989 [ 2.624,  7.646], mean action: 45.444 [8.000, 97.000],  loss: 24.146233, mae: 7.325072, mean_q: 18.033365\n",
      "[99 96 72 97 17  2 38 89 65 11]\n",
      "  4563/10001: episode: 507, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 35.433, mean reward:  3.937 [ 2.578,  6.019], mean action: 54.111 [2.000, 97.000],  loss: 20.427534, mae: 7.069135, mean_q: 17.643383\n",
      "[17 72 72 93 96 40 97 22 77 94]\n",
      "  4572/10001: episode: 508, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 20.960, mean reward:  2.329 [-10.000,  5.034], mean action: 73.667 [22.000, 97.000],  loss: 19.214027, mae: 7.144546, mean_q: 17.565712\n",
      "[50  8 68 82 17 65 65 77 85 11]\n",
      "  4581/10001: episode: 509, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.831, mean reward:  2.759 [-10.000,  6.625], mean action: 53.111 [8.000, 85.000],  loss: 15.619121, mae: 6.797748, mean_q: 16.793350\n",
      "[23 45 33 45  4 97 82 22 82 85]\n",
      "  4590/10001: episode: 510, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  8.354, mean reward:  0.928 [-10.000,  5.030], mean action: 55.000 [4.000, 97.000],  loss: 19.234819, mae: 6.589338, mean_q: 16.272259\n",
      "[30 10 50 50 61 11 45 89 73  2]\n",
      "  4599/10001: episode: 511, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 27.610, mean reward:  3.068 [-10.000,  8.981], mean action: 43.444 [2.000, 89.000],  loss: 20.295195, mae: 7.245676, mean_q: 17.986645\n",
      "[20 82 89 17 15 93 77 77 89 15]\n",
      "  4608/10001: episode: 512, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -8.425, mean reward: -0.936 [-10.000,  4.607], mean action: 61.556 [15.000, 93.000],  loss: 18.178991, mae: 6.903374, mean_q: 17.691553\n",
      "[19 89 27  8 50 97 94 10 50 40]\n",
      "  4617/10001: episode: 513, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward: 24.063, mean reward:  2.674 [-10.000,  7.989], mean action: 51.667 [8.000, 97.000],  loss: 15.484687, mae: 6.533952, mean_q: 17.123695\n",
      "[22  8 73 33 78 45 35 80 44 85]\n",
      "  4626/10001: episode: 514, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 36.889, mean reward:  4.099 [ 2.915,  5.626], mean action: 53.444 [8.000, 85.000],  loss: 18.346088, mae: 7.544029, mean_q: 18.741381\n",
      "[76 49 10 27 89 94 44 53 44 15]\n",
      "  4635/10001: episode: 515, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 18.059, mean reward:  2.007 [-10.000,  4.757], mean action: 47.222 [10.000, 94.000],  loss: 17.137278, mae: 6.590814, mean_q: 16.809004\n",
      "[56 44 80 25 93 45 82 65 41 41]\n",
      "  4644/10001: episode: 516, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.501, mean reward:  2.500 [-10.000,  5.703], mean action: 57.333 [25.000, 93.000],  loss: 18.923103, mae: 7.174437, mean_q: 18.160593\n",
      "[29 97 10 80 75 97 72 77 44 41]\n",
      "  4653/10001: episode: 517, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.358, mean reward:  2.262 [-10.000,  6.906], mean action: 65.889 [10.000, 97.000],  loss: 18.323763, mae: 6.995059, mean_q: 18.492500\n",
      "[28 92 50 96 75  2 97 25 27 82]\n",
      "  4662/10001: episode: 518, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 42.405, mean reward:  4.712 [ 2.005,  7.235], mean action: 60.667 [2.000, 97.000],  loss: 18.477444, mae: 6.986555, mean_q: 17.839622\n",
      "[32 72 27 82 96 72 82 97 44 53]\n",
      "  4671/10001: episode: 519, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  7.226, mean reward:  0.803 [-10.000,  4.723], mean action: 69.444 [27.000, 97.000],  loss: 16.265644, mae: 7.334768, mean_q: 18.893131\n",
      "[68 25 97 33 53 82 77 11 26 53]\n",
      "  4680/10001: episode: 520, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.994, mean reward:  2.666 [-10.000,  5.318], mean action: 50.778 [11.000, 97.000],  loss: 16.257515, mae: 6.547847, mean_q: 16.805490\n",
      "[85 93 93 97 96 82 77 77 40 40]\n",
      "  4689/10001: episode: 521, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -4.939, mean reward: -0.549 [-10.000,  5.783], mean action: 77.222 [40.000, 97.000],  loss: 17.988731, mae: 6.553889, mean_q: 17.068649\n",
      "[90 89 39 50 53 65 89 77 77 15]\n",
      "  4698/10001: episode: 522, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  4.277, mean reward:  0.475 [-10.000,  5.235], mean action: 61.556 [15.000, 89.000],  loss: 18.171104, mae: 6.982565, mean_q: 17.780903\n",
      "[59 11 27 97 96 35 92 93 13 73]\n",
      "  4707/10001: episode: 523, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 33.468, mean reward:  3.719 [ 2.269,  6.230], mean action: 59.667 [11.000, 97.000],  loss: 17.290520, mae: 6.881836, mean_q: 17.701355\n",
      "[28 72 27 75 78 75 25 22 85 15]\n",
      "  4716/10001: episode: 524, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 20.016, mean reward:  2.224 [-10.000,  5.606], mean action: 52.667 [15.000, 85.000],  loss: 32.381157, mae: 7.041285, mean_q: 18.196316\n",
      "[26 89 89 96 96 25 96 77 27 19]\n",
      "  4725/10001: episode: 525, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -5.747, mean reward: -0.639 [-10.000,  6.933], mean action: 68.222 [19.000, 96.000],  loss: 22.401024, mae: 6.873734, mean_q: 18.305986\n",
      "[35 93 65 15 93 37 32 97 85  5]\n",
      "  4734/10001: episode: 526, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 18.266, mean reward:  2.030 [-10.000,  5.574], mean action: 58.000 [5.000, 97.000],  loss: 18.132463, mae: 7.410192, mean_q: 18.379126\n",
      "[98 89 22  2 97 15 35 41 85 85]\n",
      "  4743/10001: episode: 527, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 22.464, mean reward:  2.496 [-10.000,  5.546], mean action: 52.333 [2.000, 97.000],  loss: 17.493582, mae: 7.498788, mean_q: 18.440895\n",
      "[31 22 22 22 22 89 94 22 82 43]\n",
      "  4752/10001: episode: 528, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -16.442, mean reward: -1.827 [-10.000,  7.219], mean action: 46.444 [22.000, 94.000],  loss: 30.274693, mae: 7.128225, mean_q: 18.122040\n",
      "[ 4 22 50 96 44 89 94  8 78 32]\n",
      "  4761/10001: episode: 529, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 29.773, mean reward:  3.308 [ 1.843,  5.196], mean action: 57.000 [8.000, 96.000],  loss: 18.975365, mae: 7.336526, mean_q: 18.286795\n",
      "[42 89 89 94 75 75 77 40 89 42]\n",
      "  4770/10001: episode: 530, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -20.266, mean reward: -2.252 [-10.000,  5.867], mean action: 74.444 [40.000, 94.000],  loss: 25.796978, mae: 6.650466, mean_q: 17.077827\n",
      "[62 89 89 96 75 84 93 40 94 85]\n",
      "  4779/10001: episode: 531, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 21.774, mean reward:  2.419 [-10.000,  5.969], mean action: 82.778 [40.000, 96.000],  loss: 33.015728, mae: 6.940588, mean_q: 17.136337\n",
      "[98 75 89 89 40 96 53 85 39 77]\n",
      "  4788/10001: episode: 532, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 23.287, mean reward:  2.587 [-10.000,  6.077], mean action: 71.444 [39.000, 96.000],  loss: 34.268364, mae: 6.618570, mean_q: 17.153908\n",
      "[ 5 89 89 78 44 97 89 85 85 94]\n",
      "  4797/10001: episode: 533, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -7.853, mean reward: -0.873 [-10.000,  4.977], mean action: 83.333 [44.000, 97.000],  loss: 22.380974, mae: 7.000158, mean_q: 18.058622\n",
      "[ 1 80 89 35 40 44 93 11 63 30]\n",
      "  4806/10001: episode: 534, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.074, mean reward:  4.008 [ 1.983,  7.574], mean action: 53.889 [11.000, 93.000],  loss: 15.915946, mae: 6.724067, mean_q: 16.412664\n",
      "[42  8 97  8 96 10 80 93 12 26]\n",
      "  4815/10001: episode: 535, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 23.692, mean reward:  2.632 [-10.000,  5.986], mean action: 47.778 [8.000, 97.000],  loss: 17.459494, mae: 6.736598, mean_q: 16.115564\n",
      "[79 33 97 96 17 75 65 65 85  1]\n",
      "  4824/10001: episode: 536, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 19.249, mean reward:  2.139 [-10.000,  5.502], mean action: 59.333 [1.000, 97.000],  loss: 17.880232, mae: 6.806871, mean_q: 16.362482\n",
      "[74 39 92 82 96 10 89 94 82 85]\n",
      "  4833/10001: episode: 537, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.020, mean reward:  2.224 [-10.000,  5.538], mean action: 74.333 [10.000, 96.000],  loss: 16.129721, mae: 6.486628, mean_q: 16.053961\n",
      "[70 93 82 33 93 96 80 41 94 85]\n",
      "  4842/10001: episode: 538, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 23.977, mean reward:  2.664 [-10.000,  5.538], mean action: 77.444 [33.000, 96.000],  loss: 16.459747, mae: 6.656355, mean_q: 16.602318\n",
      "[91 65 96 94 44  2 37 78  2 41]\n",
      "  4851/10001: episode: 539, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 19.969, mean reward:  2.219 [-10.000,  5.970], mean action: 51.000 [2.000, 96.000],  loss: 16.761404, mae: 7.266279, mean_q: 17.996988\n",
      "[85 33 80 96  4 17 82 73 53 11]\n",
      "  4860/10001: episode: 540, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 35.770, mean reward:  3.974 [ 2.820,  5.052], mean action: 49.889 [4.000, 96.000],  loss: 16.100416, mae: 6.914196, mean_q: 17.166695\n",
      "[66 93 82 89 47 38 82 41 41 85]\n",
      "  4869/10001: episode: 541, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  6.869, mean reward:  0.763 [-10.000,  5.647], mean action: 66.444 [38.000, 93.000],  loss: 17.737110, mae: 7.103665, mean_q: 17.475637\n",
      "[27 80 35 96 39 39 73 44 40 84]\n",
      "  4878/10001: episode: 542, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 19.441, mean reward:  2.160 [-10.000,  5.923], mean action: 58.889 [35.000, 96.000],  loss: 18.894484, mae: 6.832418, mean_q: 16.678570\n",
      "[52 80 89 82 44  4 25 41 33 65]\n",
      "  4887/10001: episode: 543, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 39.694, mean reward:  4.410 [ 2.656,  8.605], mean action: 51.444 [4.000, 89.000],  loss: 14.894638, mae: 6.721196, mean_q: 16.354361\n",
      "[33 80 15 94 42 82 78 26  5 77]\n",
      "  4896/10001: episode: 544, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 33.904, mean reward:  3.767 [ 2.443,  6.495], mean action: 55.444 [5.000, 94.000],  loss: 15.819753, mae: 7.146841, mean_q: 16.897688\n",
      "[86  4 77 45 47 43 96 20 77 43]\n",
      "  4905/10001: episode: 545, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  5.677, mean reward:  0.631 [-10.000,  4.136], mean action: 50.222 [4.000, 96.000],  loss: 13.865124, mae: 7.244558, mean_q: 17.071724\n",
      "[22  4 75 82  4  4 25 92 43 94]\n",
      "  4914/10001: episode: 546, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  7.165, mean reward:  0.796 [-10.000,  5.309], mean action: 47.000 [4.000, 94.000],  loss: 16.033379, mae: 7.220707, mean_q: 17.029860\n",
      "[30  2 73 84 17 82 35  2 85 11]\n",
      "  4923/10001: episode: 547, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 20.641, mean reward:  2.293 [-10.000,  5.001], mean action: 43.444 [2.000, 85.000],  loss: 18.008192, mae: 7.015697, mean_q: 16.709602\n",
      "[60 17 37 73 78 92 32 47 94  5]\n",
      "  4932/10001: episode: 548, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 31.951, mean reward:  3.550 [ 2.426,  4.507], mean action: 52.778 [5.000, 94.000],  loss: 18.601267, mae: 7.112323, mean_q: 17.061594\n",
      "[99 84 73 80 25 82 84 26  5  5]\n",
      "  4941/10001: episode: 549, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  6.397, mean reward:  0.711 [-10.000,  4.782], mean action: 51.556 [5.000, 84.000],  loss: 18.409502, mae: 6.721129, mean_q: 16.170414\n",
      "[73 13 73 78 25 82 32 84 43 41]\n",
      "  4950/10001: episode: 550, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 21.443, mean reward:  2.383 [-10.000,  6.145], mean action: 52.333 [13.000, 84.000],  loss: 15.536030, mae: 6.724647, mean_q: 16.366417\n",
      "[20 73 45 92 78 40 35 77 40 77]\n",
      "  4959/10001: episode: 551, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  5.864, mean reward:  0.652 [-10.000,  4.758], mean action: 61.889 [35.000, 92.000],  loss: 16.347553, mae: 6.489188, mean_q: 16.045698\n",
      "[57 73 78 73 13 13 96 85 85 35]\n",
      "  4968/10001: episode: 552, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -6.771, mean reward: -0.752 [-10.000,  4.927], mean action: 61.222 [13.000, 96.000],  loss: 18.748646, mae: 6.478472, mean_q: 15.755807\n",
      "[41 94 43  8 75 65 80  2  2  2]\n",
      "  4977/10001: episode: 553, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  8.550, mean reward:  0.950 [-10.000,  7.036], mean action: 41.222 [2.000, 94.000],  loss: 17.600407, mae: 6.802181, mean_q: 16.315914\n",
      "[76 80 27 80 47 47 94 85 89 43]\n",
      "  4986/10001: episode: 554, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 10.130, mean reward:  1.126 [-10.000,  6.760], mean action: 65.778 [27.000, 94.000],  loss: 14.798262, mae: 6.996959, mean_q: 16.910994\n",
      "[64 47 10 77 96 35 85 77 40 85]\n",
      "  4995/10001: episode: 555, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  6.590, mean reward:  0.732 [-10.000,  6.080], mean action: 61.333 [10.000, 96.000],  loss: 18.225122, mae: 6.132468, mean_q: 15.347935\n",
      "[33 49 77 32 96 35 93 27 94 41]\n",
      "  5004/10001: episode: 556, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 35.531, mean reward:  3.948 [ 2.032,  6.557], mean action: 60.444 [27.000, 96.000],  loss: 15.701470, mae: 6.919999, mean_q: 17.053595\n",
      "[ 6 11 89 33 13 33 65 85 84 61]\n",
      "  5013/10001: episode: 557, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 15.594, mean reward:  1.733 [-10.000,  4.865], mean action: 52.667 [11.000, 89.000],  loss: 18.505939, mae: 6.635227, mean_q: 16.093376\n",
      "[93 78 78 49 97 80 80 25 61 43]\n",
      "  5022/10001: episode: 558, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 11.491, mean reward:  1.277 [-10.000,  7.645], mean action: 65.667 [25.000, 97.000],  loss: 11.631399, mae: 7.060722, mean_q: 17.151234\n",
      "[46 24 89 82 40 35 84 20 94 20]\n",
      "  5031/10001: episode: 559, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 17.083, mean reward:  1.898 [-10.000,  4.334], mean action: 54.222 [20.000, 94.000],  loss: 17.005287, mae: 6.773882, mean_q: 16.350758\n",
      "[20 13 93 93  4 78 82 85 41 35]\n",
      "  5040/10001: episode: 560, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.111, mean reward:  2.679 [-10.000,  5.762], mean action: 58.222 [4.000, 93.000],  loss: 15.966032, mae: 6.902445, mean_q: 16.376829\n",
      "[88 40 80 49 78 40 80 85 89 39]\n",
      "  5049/10001: episode: 561, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  7.038, mean reward:  0.782 [-10.000,  5.207], mean action: 64.444 [39.000, 89.000],  loss: 12.802326, mae: 7.104704, mean_q: 17.015842\n",
      "[63 77 38 78 96  8 40 85 44 40]\n",
      "  5058/10001: episode: 562, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 21.516, mean reward:  2.391 [-10.000,  5.406], mean action: 56.222 [8.000, 96.000],  loss: 15.569187, mae: 6.719958, mean_q: 16.558187\n",
      "[58  8 97 96 13 84 47 85 85 10]\n",
      "  5067/10001: episode: 563, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 20.297, mean reward:  2.255 [-10.000,  5.559], mean action: 58.333 [8.000, 97.000],  loss: 16.139185, mae: 7.242018, mean_q: 17.567204\n",
      "[21  4 94 47 47 40 35 68 43 55]\n",
      "  5076/10001: episode: 564, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 19.394, mean reward:  2.155 [-10.000,  6.194], mean action: 48.111 [4.000, 94.000],  loss: 16.451378, mae: 6.861358, mean_q: 16.420729\n",
      "[89 73 97 96  4 47 96 80 94 41]\n",
      "  5085/10001: episode: 565, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 21.822, mean reward:  2.425 [-10.000,  6.040], mean action: 69.778 [4.000, 97.000],  loss: 16.568550, mae: 7.125731, mean_q: 16.734221\n",
      "[43 27 33 73 44 89 80 47 85 35]\n",
      "  5094/10001: episode: 566, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 34.612, mean reward:  3.846 [ 2.450,  4.785], mean action: 57.000 [27.000, 89.000],  loss: 15.467175, mae: 7.159325, mean_q: 16.523386\n",
      "[38 80 10 97 96 45 89  2 47 89]\n",
      "  5103/10001: episode: 567, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.056, mean reward:  2.451 [-10.000,  6.869], mean action: 61.667 [2.000, 97.000],  loss: 17.053013, mae: 6.974475, mean_q: 16.463997\n",
      "[46 11 33 38 47 89 77 78 85 22]\n",
      "  5112/10001: episode: 568, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 33.556, mean reward:  3.728 [ 2.853,  5.032], mean action: 53.333 [11.000, 89.000],  loss: 13.882461, mae: 6.859637, mean_q: 16.444275\n",
      "[38 93 33 10 75 80 10 82 47 41]\n",
      "  5121/10001: episode: 569, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 23.155, mean reward:  2.573 [-10.000,  5.639], mean action: 52.333 [10.000, 93.000],  loss: 15.031115, mae: 6.636681, mean_q: 16.126411\n",
      "[43 10 33 17 41 35 47  8 73  2]\n",
      "  5130/10001: episode: 570, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 39.805, mean reward:  4.423 [ 2.692,  8.501], mean action: 29.556 [2.000, 73.000],  loss: 17.338505, mae: 6.723227, mean_q: 16.132450\n",
      "[60 15 33 97 47 89 82 43 40 15]\n",
      "  5139/10001: episode: 571, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 26.078, mean reward:  2.898 [-10.000,  6.077], mean action: 51.222 [15.000, 97.000],  loss: 15.471837, mae: 6.668479, mean_q: 15.938494\n",
      "[23 96 33 33 97 35 97 40 47 85]\n",
      "  5148/10001: episode: 572, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  7.815, mean reward:  0.868 [-10.000,  5.124], mean action: 62.556 [33.000, 97.000],  loss: 17.654612, mae: 6.919214, mean_q: 16.321859\n",
      "[ 7 16 25 33 33 41 96 26 41 40]\n",
      "  5157/10001: episode: 573, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 11.448, mean reward:  1.272 [-10.000,  7.482], mean action: 39.000 [16.000, 96.000],  loss: 15.488020, mae: 6.577738, mean_q: 16.055569\n",
      "[52 41 41 41 61 41 41 85 85  2]\n",
      "  5166/10001: episode: 574, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -31.338, mean reward: -3.482 [-10.000,  7.839], mean action: 48.667 [2.000, 85.000],  loss: 15.603550, mae: 6.629782, mean_q: 17.761856\n",
      "[32 41 41 41 41 40 41 47 85 35]\n",
      "  5175/10001: episode: 575, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -17.293, mean reward: -1.921 [-10.000,  5.573], mean action: 45.778 [35.000, 85.000],  loss: 19.474295, mae: 6.445434, mean_q: 17.923899\n",
      "[93 93 41 93 38 37 78 85 40 43]\n",
      "  5184/10001: episode: 576, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 11.996, mean reward:  1.333 [-10.000,  7.026], mean action: 60.889 [37.000, 93.000],  loss: 14.363351, mae: 6.749726, mean_q: 16.585325\n",
      "[14  8 13 82  4 78 53 89 94  2]\n",
      "  5193/10001: episode: 577, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 38.694, mean reward:  4.299 [ 2.634,  9.499], mean action: 47.000 [2.000, 94.000],  loss: 16.202946, mae: 6.574514, mean_q: 15.997789\n",
      "[51 93 82 82 44 11 44  8 53 27]\n",
      "  5202/10001: episode: 578, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 10.447, mean reward:  1.161 [-10.000,  8.112], mean action: 49.333 [8.000, 93.000],  loss: 18.043453, mae: 6.738264, mean_q: 16.547235\n",
      "[12 37 80 82 78 40 13 85 89 61]\n",
      "  5211/10001: episode: 579, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 35.531, mean reward:  3.948 [ 2.674,  5.632], mean action: 62.778 [13.000, 89.000],  loss: 16.746828, mae: 5.844055, mean_q: 14.542502\n",
      "[25 94 85 82 61 37 97 85 53 53]\n",
      "  5220/10001: episode: 580, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  4.398, mean reward:  0.489 [-10.000,  4.844], mean action: 71.889 [37.000, 97.000],  loss: 16.286659, mae: 6.595503, mean_q: 15.944183\n",
      "[ 5 13 82 15 17 93 93 26 10 93]\n",
      "  5229/10001: episode: 581, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  6.962, mean reward:  0.774 [-10.000,  5.223], mean action: 49.111 [10.000, 93.000],  loss: 14.114264, mae: 7.125822, mean_q: 17.296122\n",
      "[50 80 97 33 93 89 94 26 47 10]\n",
      "  5238/10001: episode: 582, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.233, mean reward:  4.026 [ 2.481,  5.511], mean action: 63.222 [10.000, 97.000],  loss: 16.789724, mae: 6.501810, mean_q: 16.267387\n",
      "[96 80 92 43 35 13 97 85 85 41]\n",
      "  5247/10001: episode: 583, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 21.503, mean reward:  2.389 [-10.000,  6.339], mean action: 63.444 [13.000, 97.000],  loss: 15.785595, mae: 6.868260, mean_q: 16.851234\n",
      "[35  2 80 45 39 43 47 26 40 65]\n",
      "  5256/10001: episode: 584, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.860, mean reward:  4.096 [ 3.093,  6.110], mean action: 43.000 [2.000, 80.000],  loss: 14.183299, mae: 6.783525, mean_q: 16.223640\n",
      "[ 0 10 17 39 42 80 35 85 94  2]\n",
      "  5265/10001: episode: 585, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 40.765, mean reward:  4.529 [ 2.965, 10.270], mean action: 44.889 [2.000, 94.000],  loss: 17.909431, mae: 6.450712, mean_q: 15.676965\n",
      "[42 49 92 96 38  2 73 47  2 53]\n",
      "  5274/10001: episode: 586, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 20.208, mean reward:  2.245 [-10.000,  5.749], mean action: 50.222 [2.000, 96.000],  loss: 16.473904, mae: 6.901357, mean_q: 16.065077\n",
      "[77 40 73 38 84 94 68 78 94 11]\n",
      "  5283/10001: episode: 587, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 19.054, mean reward:  2.117 [-10.000,  7.043], mean action: 64.444 [11.000, 94.000],  loss: 16.030479, mae: 6.911391, mean_q: 16.440123\n",
      "[ 6 40 89 45 42 11 65 47 89 20]\n",
      "  5292/10001: episode: 588, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 17.873, mean reward:  1.986 [-10.000,  4.900], mean action: 49.778 [11.000, 89.000],  loss: 15.559950, mae: 6.591040, mean_q: 15.899670\n",
      "[27 80 92 17  0 53 84 20 26 44]\n",
      "  5301/10001: episode: 589, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 31.477, mean reward:  3.497 [ 1.926,  5.445], mean action: 46.222 [0.000, 92.000],  loss: 15.655370, mae: 7.118597, mean_q: 16.710808\n",
      "[95 22 82 55 97  0  8  8 53 89]\n",
      "  5310/10001: episode: 590, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 19.146, mean reward:  2.127 [-10.000,  6.277], mean action: 46.000 [0.000, 97.000],  loss: 16.089134, mae: 6.090229, mean_q: 14.681920\n",
      "[93  2 97 97  4  4  2  4 15 16]\n",
      "  5319/10001: episode: 591, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: -24.237, mean reward: -2.693 [-10.000,  4.327], mean action: 26.778 [2.000, 97.000],  loss: 14.305825, mae: 6.719008, mean_q: 16.433788\n",
      "[ 3 17 17 84 42 80 15 85 84 11]\n",
      "  5328/10001: episode: 592, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  9.641, mean reward:  1.071 [-10.000,  6.893], mean action: 48.333 [11.000, 85.000],  loss: 16.971235, mae: 6.447162, mean_q: 15.695148\n",
      "[12 42 85 16 33  8 93 85 80 41]\n",
      "  5337/10001: episode: 593, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 26.534, mean reward:  2.948 [-10.000,  6.255], mean action: 53.667 [8.000, 93.000],  loss: 16.709476, mae: 6.753368, mean_q: 16.593569\n",
      "[46 39 15 80 78 78 93 92 43 55]\n",
      "  5346/10001: episode: 594, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 22.206, mean reward:  2.467 [-10.000,  6.539], mean action: 63.667 [15.000, 93.000],  loss: 13.451257, mae: 6.374024, mean_q: 15.570086\n",
      "[77 68 92 33 78 35 85 82 41 30]\n",
      "  5355/10001: episode: 595, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 40.128, mean reward:  4.459 [ 2.250,  6.919], mean action: 60.444 [30.000, 92.000],  loss: 15.940532, mae: 6.922582, mean_q: 16.396240\n",
      "[48 33 97 96 33  2 65 97 12 32]\n",
      "  5364/10001: episode: 596, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  2.389, mean reward:  0.265 [-10.000,  5.006], mean action: 51.889 [2.000, 97.000],  loss: 15.529158, mae: 6.298812, mean_q: 15.372387\n",
      "[98 22 80 96  2  2 96 53 50 94]\n",
      "  5373/10001: episode: 597, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  5.061, mean reward:  0.562 [-10.000,  5.392], mean action: 55.000 [2.000, 96.000],  loss: 15.298039, mae: 6.708242, mean_q: 16.049273\n",
      "[99 89 82 33 84 93 77 19 82 11]\n",
      "  5382/10001: episode: 598, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 19.641, mean reward:  2.182 [-10.000,  5.682], mean action: 63.333 [11.000, 93.000],  loss: 15.223785, mae: 6.744250, mean_q: 16.104244\n",
      "[85 89 82 50 47 80 94 11 82 30]\n",
      "  5391/10001: episode: 599, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 22.207, mean reward:  2.467 [-10.000,  7.049], mean action: 62.778 [11.000, 94.000],  loss: 14.139817, mae: 6.908424, mean_q: 16.342680\n",
      "[30 44 13 25 49  2  2 35 27  5]\n",
      "  5400/10001: episode: 600, duration: 0.066s, episode steps:   9, steps per second: 136, episode reward: 23.062, mean reward:  2.562 [-10.000,  7.271], mean action: 22.444 [2.000, 49.000],  loss: 17.999781, mae: 6.475354, mean_q: 15.714763\n",
      "[80 33 25 38 53 50 53 20 26 10]\n",
      "  5409/10001: episode: 601, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.450, mean reward:  2.272 [-10.000,  5.922], mean action: 34.222 [10.000, 53.000],  loss: 16.774403, mae: 6.344656, mean_q: 15.269399\n",
      "[53 73 77 82 40 33 40 85 27 89]\n",
      "  5418/10001: episode: 602, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 25.319, mean reward:  2.813 [-10.000,  6.099], mean action: 60.667 [27.000, 89.000],  loss: 17.486082, mae: 6.257107, mean_q: 15.060940\n",
      "[ 1 25  8 82 38 40 82 97  8 43]\n",
      "  5427/10001: episode: 603, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 10.130, mean reward:  1.126 [-10.000,  6.064], mean action: 47.000 [8.000, 97.000],  loss: 15.940783, mae: 6.681523, mean_q: 15.975596\n",
      "[29 65 82 32 47 35 80 97 47 30]\n",
      "  5436/10001: episode: 604, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 22.117, mean reward:  2.457 [-10.000,  7.223], mean action: 57.222 [30.000, 97.000],  loss: 16.025049, mae: 6.570462, mean_q: 15.931808\n",
      "[13 92 82 32 53 65 53 11 85 30]\n",
      "  5445/10001: episode: 605, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 24.387, mean reward:  2.710 [-10.000,  7.841], mean action: 55.889 [11.000, 92.000],  loss: 16.507528, mae: 6.649763, mean_q: 16.129002\n",
      "[59 89 96 96 78 17 39 85 43  8]\n",
      "  5454/10001: episode: 606, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 22.810, mean reward:  2.534 [-10.000,  5.988], mean action: 61.222 [8.000, 96.000],  loss: 12.845544, mae: 7.098956, mean_q: 17.027855\n",
      "[76 82 92 68 84 11 11 68 65 43]\n",
      "  5463/10001: episode: 607, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  7.301, mean reward:  0.811 [-10.000,  7.125], mean action: 58.222 [11.000, 92.000],  loss: 14.711223, mae: 6.708683, mean_q: 16.227352\n",
      "[62 97 92 82 96  2 40 77 42 43]\n",
      "  5472/10001: episode: 608, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 38.690, mean reward:  4.299 [ 3.122,  6.440], mean action: 63.444 [2.000, 97.000],  loss: 14.936338, mae: 6.132525, mean_q: 14.904515\n",
      "[54 97 92 96 82 89 93 40 73 41]\n",
      "  5481/10001: episode: 609, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 37.712, mean reward:  4.190 [ 3.250,  5.212], mean action: 78.111 [40.000, 97.000],  loss: 14.694260, mae: 6.669218, mean_q: 16.497864\n",
      "[64 73 94 84 61 73 96 41 50 40]\n",
      "  5490/10001: episode: 610, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.643, mean reward:  2.294 [-10.000,  7.989], mean action: 68.000 [40.000, 96.000],  loss: 16.683653, mae: 6.367494, mean_q: 16.020048\n",
      "[94 16 97 96 82 17 94 97 43 94]\n",
      "  5499/10001: episode: 611, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: -6.036, mean reward: -0.671 [-10.000,  5.865], mean action: 70.667 [16.000, 97.000],  loss: 15.338187, mae: 6.746946, mean_q: 16.560600\n",
      "[88  8 97 35 96 45 94 41  2 41]\n",
      "  5508/10001: episode: 612, duration: 0.070s, episode steps:   9, steps per second: 128, episode reward: 23.330, mean reward:  2.592 [-10.000,  8.018], mean action: 51.000 [2.000, 97.000],  loss: 14.606896, mae: 6.834348, mean_q: 16.724545\n",
      "[90 80  8 10 82 35 94 97 11 40]\n",
      "  5517/10001: episode: 613, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 38.486, mean reward:  4.276 [ 2.699,  7.164], mean action: 50.778 [8.000, 97.000],  loss: 15.680721, mae: 6.781773, mean_q: 16.599203\n",
      "[95 63 68 97 82 39 94 85 43 10]\n",
      "  5526/10001: episode: 614, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 36.238, mean reward:  4.026 [ 2.114,  6.031], mean action: 64.556 [10.000, 97.000],  loss: 17.786308, mae: 6.929318, mean_q: 16.897301\n",
      "[73 93 13 43 97 40 89 85 65 30]\n",
      "  5535/10001: episode: 615, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward: 37.347, mean reward:  4.150 [ 2.383,  7.475], mean action: 61.667 [13.000, 97.000],  loss: 15.250090, mae: 6.566225, mean_q: 16.088711\n",
      "[85  8 35 39 32 82 85 47 30 40]\n",
      "  5544/10001: episode: 616, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward: 23.532, mean reward:  2.615 [-10.000,  6.875], mean action: 44.222 [8.000, 85.000],  loss: 13.836090, mae: 6.621130, mean_q: 15.853497\n",
      "[40 92 92 92 82 89 93 27 85 40]\n",
      "  5553/10001: episode: 617, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -4.635, mean reward: -0.515 [-10.000,  5.782], mean action: 76.889 [27.000, 93.000],  loss: 15.435337, mae: 6.586936, mean_q: 15.598197\n",
      "[31 39  2 10 85 40 78 85  2 40]\n",
      "  5562/10001: episode: 618, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -6.628, mean reward: -0.736 [-10.000,  4.790], mean action: 42.333 [2.000, 85.000],  loss: 16.065636, mae: 6.738616, mean_q: 16.034414\n",
      "[64 33 72 53 47 80 44 27 77 12]\n",
      "  5571/10001: episode: 619, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 34.083, mean reward:  3.787 [ 2.387,  6.502], mean action: 49.444 [12.000, 80.000],  loss: 18.302464, mae: 6.845987, mean_q: 16.350311\n",
      "[ 8 97 85 19 47 85 85 47 41 41]\n",
      "  5580/10001: episode: 620, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: -22.240, mean reward: -2.471 [-10.000,  5.526], mean action: 60.778 [19.000, 97.000],  loss: 17.225649, mae: 7.038782, mean_q: 16.723061\n",
      "[46 38 10 50 89 40 35 85 43 15]\n",
      "  5589/10001: episode: 621, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 34.936, mean reward:  3.882 [ 1.959,  5.360], mean action: 45.000 [10.000, 89.000],  loss: 15.396273, mae: 6.810212, mean_q: 16.375204\n",
      "[91  2 82 96 19 43 78 47 40 42]\n",
      "  5598/10001: episode: 622, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 33.910, mean reward:  3.768 [ 1.988,  6.522], mean action: 49.889 [2.000, 96.000],  loss: 20.264456, mae: 6.954433, mean_q: 16.326132\n",
      "[41  2 15 68 15 94 94 78 65 85]\n",
      "  5607/10001: episode: 623, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  6.885, mean reward:  0.765 [-10.000,  6.997], mean action: 57.333 [2.000, 94.000],  loss: 18.536755, mae: 6.363700, mean_q: 15.188849\n",
      "[63  2 44  2  2 53 61 19 53 53]\n",
      "  5616/10001: episode: 624, duration: 0.058s, episode steps:   9, steps per second: 157, episode reward: -26.398, mean reward: -2.933 [-10.000,  3.997], mean action: 32.111 [2.000, 61.000],  loss: 17.775541, mae: 6.988052, mean_q: 16.643934\n",
      "[90 47 43 97 78 80 44 47 77 94]\n",
      "  5625/10001: episode: 625, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.156, mean reward:  2.240 [-10.000,  4.693], mean action: 67.444 [43.000, 97.000],  loss: 17.358452, mae: 6.046080, mean_q: 14.756514\n",
      "[ 8  2 97 82 96 45 80 85 77 30]\n",
      "  5634/10001: episode: 626, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 38.743, mean reward:  4.305 [ 3.219,  6.862], mean action: 66.000 [2.000, 97.000],  loss: 14.011237, mae: 7.070902, mean_q: 16.744270\n",
      "[69 73 94 25 96 80 39 47 43 41]\n",
      "  5643/10001: episode: 627, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 37.917, mean reward:  4.213 [ 2.353,  6.142], mean action: 59.778 [25.000, 96.000],  loss: 15.947303, mae: 6.782539, mean_q: 16.123127\n",
      "[ 8 49 43 94 82 94 94 78 73 30]\n",
      "  5652/10001: episode: 628, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 11.213, mean reward:  1.246 [-10.000,  7.342], mean action: 70.778 [30.000, 94.000],  loss: 14.609377, mae: 6.933751, mean_q: 16.605343\n",
      "[21 27 82  0 33 89 44 68 32 30]\n",
      "  5661/10001: episode: 629, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 34.665, mean reward:  3.852 [ 1.981,  7.932], mean action: 45.000 [0.000, 89.000],  loss: 13.996293, mae: 7.348742, mean_q: 17.399466\n",
      "[20  2 25 82  2  0 15 50 40 39]\n",
      "  5670/10001: episode: 630, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 21.633, mean reward:  2.404 [-10.000,  7.251], mean action: 28.333 [0.000, 82.000],  loss: 17.392757, mae: 6.782606, mean_q: 16.640642\n",
      "[50  2 25 61 39  4 22 12 65 30]\n",
      "  5679/10001: episode: 631, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 37.353, mean reward:  4.150 [ 2.267,  8.546], mean action: 28.889 [2.000, 65.000],  loss: 17.762209, mae: 6.839623, mean_q: 16.657066\n",
      "[39 49 43  2 82 44 77 78 30 30]\n",
      "  5688/10001: episode: 632, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.991, mean reward:  2.555 [-10.000,  6.964], mean action: 48.333 [2.000, 82.000],  loss: 16.706949, mae: 6.727388, mean_q: 16.278841\n",
      "[62 89 75 97 47 80 61 33 65 55]\n",
      "  5697/10001: episode: 633, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 35.815, mean reward:  3.979 [ 2.934,  8.193], mean action: 66.889 [33.000, 97.000],  loss: 17.331770, mae: 6.317279, mean_q: 15.162377\n",
      "[84 45 35 16  2 82 61 43 85 34]\n",
      "  5706/10001: episode: 634, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 32.936, mean reward:  3.660 [ 0.700,  5.484], mean action: 44.778 [2.000, 85.000],  loss: 15.560887, mae: 6.923318, mean_q: 16.560608\n",
      "[28 80 45 33 38 85 17 10 82 89]\n",
      "  5715/10001: episode: 635, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.627, mean reward:  4.292 [ 3.176,  6.242], mean action: 53.222 [10.000, 89.000],  loss: 11.624721, mae: 6.756285, mean_q: 16.208092\n",
      "[75  2 77 61 84 10 10 92 89 30]\n",
      "  5724/10001: episode: 636, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 22.778, mean reward:  2.531 [-10.000,  7.127], mean action: 50.556 [2.000, 92.000],  loss: 15.584941, mae: 6.749487, mean_q: 16.504368\n",
      "[ 4 80 33 33 25 89 84 89 15 65]\n",
      "  5733/10001: episode: 637, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  2.387, mean reward:  0.265 [-10.000,  4.513], mean action: 57.000 [15.000, 89.000],  loss: 16.671852, mae: 6.424184, mean_q: 15.698149\n",
      "[34 73 45 82 73 97 39 85  2  2]\n",
      "  5742/10001: episode: 638, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 12.068, mean reward:  1.341 [-10.000,  7.949], mean action: 55.333 [2.000, 97.000],  loss: 15.572664, mae: 6.830708, mean_q: 16.473658\n",
      "[87 94 80 33 78 80 17 82 50 35]\n",
      "  5751/10001: episode: 639, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.484, mean reward:  2.276 [-10.000,  5.808], mean action: 61.000 [17.000, 94.000],  loss: 15.639376, mae: 6.468409, mean_q: 15.570057\n",
      "[76 17 75 96 11 61 80 77 50 17]\n",
      "  5760/10001: episode: 640, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 18.633, mean reward:  2.070 [-10.000,  5.649], mean action: 53.778 [11.000, 96.000],  loss: 16.144407, mae: 6.902792, mean_q: 16.312706\n",
      "[ 9  8  2 33 61 80 13 11 89 30]\n",
      "  5769/10001: episode: 641, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 40.312, mean reward:  4.479 [ 2.346,  7.104], mean action: 36.333 [2.000, 89.000],  loss: 16.365898, mae: 6.942118, mean_q: 16.749107\n",
      "[11 10 25 45 82 45 80 68 15 89]\n",
      "  5778/10001: episode: 642, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 20.946, mean reward:  2.327 [-10.000,  6.376], mean action: 51.000 [10.000, 89.000],  loss: 15.076935, mae: 6.529169, mean_q: 15.852534\n",
      "[89  2 97 84 78 11 22 50 80  2]\n",
      "  5787/10001: episode: 643, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 19.083, mean reward:  2.120 [-10.000,  6.913], mean action: 47.333 [2.000, 97.000],  loss: 17.229233, mae: 6.732186, mean_q: 16.212189\n",
      "[44 38 27 75 89 80 40 85 53 10]\n",
      "  5796/10001: episode: 644, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 37.328, mean reward:  4.148 [ 3.256,  5.594], mean action: 55.222 [10.000, 89.000],  loss: 16.955378, mae: 6.687571, mean_q: 16.141178\n",
      "[83 13 25 53 40 22 39 78 92 17]\n",
      "  5805/10001: episode: 645, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 33.662, mean reward:  3.740 [ 2.464,  4.841], mean action: 42.111 [13.000, 92.000],  loss: 13.639622, mae: 6.390624, mean_q: 15.647125\n",
      "[82 38 33 61 13 25 84 68 73 10]\n",
      "  5814/10001: episode: 646, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 33.334, mean reward:  3.704 [ 2.089,  6.412], mean action: 45.000 [10.000, 84.000],  loss: 19.056767, mae: 6.771602, mean_q: 16.574770\n",
      "[81  2 25 82 82 17  0  4 41 10]\n",
      "  5823/10001: episode: 647, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 22.762, mean reward:  2.529 [-10.000,  6.301], mean action: 29.222 [0.000, 82.000],  loss: 12.578815, mae: 6.817231, mean_q: 17.005817\n",
      "[20 93 25 84 38 45 84 10 10 73]\n",
      "  5832/10001: episode: 648, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward:  8.506, mean reward:  0.945 [-10.000,  6.256], mean action: 51.333 [10.000, 93.000],  loss: 18.413675, mae: 7.104901, mean_q: 17.296076\n",
      "[88 22 97 96 75 22 17 80 10 50]\n",
      "  5841/10001: episode: 649, duration: 0.061s, episode steps:   9, steps per second: 146, episode reward: 20.837, mean reward:  2.315 [-10.000,  6.052], mean action: 52.111 [10.000, 97.000],  loss: 14.290043, mae: 7.003055, mean_q: 17.004908\n",
      "[63 22 43 38 15 97 78 26 78  2]\n",
      "  5850/10001: episode: 650, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.744, mean reward:  2.860 [-10.000,  9.725], mean action: 44.333 [2.000, 97.000],  loss: 12.032710, mae: 7.507647, mean_q: 17.528423\n",
      "[10 38 85 45 78 73 80 10 40 55]\n",
      "  5859/10001: episode: 651, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 21.778, mean reward:  2.420 [-10.000,  6.132], mean action: 56.000 [10.000, 85.000],  loss: 16.535561, mae: 6.523601, mean_q: 15.353865\n",
      "[66 93 43 38 40 39 80 10 12 80]\n",
      "  5868/10001: episode: 652, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 19.786, mean reward:  2.198 [-10.000,  5.265], mean action: 48.333 [10.000, 93.000],  loss: 15.372825, mae: 6.979961, mean_q: 16.404545\n",
      "[61 45 15 10 15 25 84 10 50 42]\n",
      "  5877/10001: episode: 653, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  5.507, mean reward:  0.612 [-10.000,  4.524], mean action: 32.889 [10.000, 84.000],  loss: 15.103738, mae: 7.009467, mean_q: 16.683178\n",
      "[99 49 80 55 47 11 39 10 73 43]\n",
      "  5886/10001: episode: 654, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.926, mean reward:  4.103 [ 2.239,  6.495], mean action: 45.222 [10.000, 80.000],  loss: 14.506056, mae: 6.746757, mean_q: 15.545051\n",
      "[16  8 25 80 61 85 80 12 92 35]\n",
      "  5895/10001: episode: 655, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 19.744, mean reward:  2.194 [-10.000,  4.767], mean action: 53.111 [8.000, 92.000],  loss: 14.797997, mae: 7.117815, mean_q: 16.675652\n",
      "[66 24 85 89 61 94 77 10 73 30]\n",
      "  5904/10001: episode: 656, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 37.538, mean reward:  4.171 [ 2.354,  7.087], mean action: 60.333 [10.000, 94.000],  loss: 14.357250, mae: 7.130283, mean_q: 16.557961\n",
      "[15 26 85 33 61 94 77 10 73 30]\n",
      "  5913/10001: episode: 657, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 39.261, mean reward:  4.362 [ 2.319,  6.977], mean action: 54.333 [10.000, 94.000],  loss: 15.341404, mae: 6.977851, mean_q: 16.246498\n",
      "[70  2 75 40 47 94 80 33  2 10]\n",
      "  5922/10001: episode: 658, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.573, mean reward:  2.841 [-10.000,  7.790], mean action: 42.556 [2.000, 94.000],  loss: 12.689094, mae: 7.259692, mean_q: 16.835014\n",
      "[99  2 73 49 61 93 53 10 10 10]\n",
      "  5931/10001: episode: 659, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  5.106, mean reward:  0.567 [-10.000,  4.877], mean action: 40.111 [2.000, 93.000],  loss: 15.372791, mae: 6.970064, mean_q: 16.463781\n",
      "[ 1 16 75 33 78 42 77 73 43 40]\n",
      "  5940/10001: episode: 660, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 41.020, mean reward:  4.558 [ 2.754,  7.104], mean action: 53.000 [16.000, 78.000],  loss: 14.888657, mae: 6.757160, mean_q: 16.003771\n",
      "[17 65 85 17 47 77 25 41 77  2]\n",
      "  5949/10001: episode: 661, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 13.075, mean reward:  1.453 [-10.000,  9.192], mean action: 48.444 [2.000, 85.000],  loss: 14.752060, mae: 6.921522, mean_q: 16.269884\n",
      "[47  8 84 47 78 42 80 78 12 12]\n",
      "  5958/10001: episode: 662, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -8.338, mean reward: -0.926 [-10.000,  5.403], mean action: 49.000 [8.000, 84.000],  loss: 13.561809, mae: 7.282404, mean_q: 17.150917\n",
      "[74 85 85 89 44 97 77 27 12 77]\n",
      "  5967/10001: episode: 663, duration: 0.057s, episode steps:   9, steps per second: 159, episode reward:  7.139, mean reward:  0.793 [-10.000,  6.513], mean action: 65.889 [12.000, 97.000],  loss: 14.240605, mae: 7.107747, mean_q: 16.477318\n",
      "[50 40 93 78 78 80 44  2 78  2]\n",
      "  5976/10001: episode: 664, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -4.172, mean reward: -0.464 [-10.000,  7.662], mean action: 55.000 [2.000, 93.000],  loss: 13.276020, mae: 7.371202, mean_q: 17.089861\n",
      "[12 93 33 43 97 35 93 40 47  5]\n",
      "  5985/10001: episode: 665, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.108, mean reward:  2.234 [-10.000,  5.435], mean action: 54.000 [5.000, 97.000],  loss: 13.582526, mae: 7.051867, mean_q: 16.314241\n",
      "[24 39 68 82 75 45 93 85 73 40]\n",
      "  5994/10001: episode: 666, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.835, mean reward:  4.426 [ 2.383,  6.811], mean action: 66.667 [39.000, 93.000],  loss: 14.850170, mae: 6.959354, mean_q: 16.048449\n",
      "[56  2 89 45 45 72 77 63 84 30]\n",
      "  6003/10001: episode: 667, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 20.890, mean reward:  2.321 [-10.000,  7.988], mean action: 56.333 [2.000, 89.000],  loss: 18.983887, mae: 6.427944, mean_q: 15.148464\n",
      "[37 73 10 43  2  2  2 13 94 61]\n",
      "  6012/10001: episode: 668, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  4.018, mean reward:  0.446 [-10.000,  5.068], mean action: 33.333 [2.000, 94.000],  loss: 12.423736, mae: 6.874473, mean_q: 16.184229\n",
      "[67 33 33 15 82 17 97 10 85 40]\n",
      "  6021/10001: episode: 669, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 23.226, mean reward:  2.581 [-10.000,  6.854], mean action: 45.778 [10.000, 97.000],  loss: 16.319071, mae: 7.091411, mean_q: 16.516781\n",
      "[ 8 93 10 10 85 77 39 77 27 73]\n",
      "  6030/10001: episode: 670, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 10.914, mean reward:  1.213 [-10.000,  7.305], mean action: 54.556 [10.000, 93.000],  loss: 15.299955, mae: 6.808984, mean_q: 15.948724\n",
      "[36 38 85 15 97 40 82 65 85  2]\n",
      "  6039/10001: episode: 671, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 26.048, mean reward:  2.894 [-10.000,  8.893], mean action: 56.556 [2.000, 97.000],  loss: 16.627138, mae: 7.311489, mean_q: 16.788221\n",
      "[37 32 85 93 75 35 77  8 61 65]\n",
      "  6048/10001: episode: 672, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 34.919, mean reward:  3.880 [ 2.277,  5.271], mean action: 59.000 [8.000, 93.000],  loss: 14.580968, mae: 6.591333, mean_q: 15.683014\n",
      "[11 96 85 78 75 97 97 27 20 41]\n",
      "  6057/10001: episode: 673, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 23.086, mean reward:  2.565 [-10.000,  6.421], mean action: 68.444 [20.000, 97.000],  loss: 13.090771, mae: 7.568345, mean_q: 17.377174\n",
      "[81 72 85 61  2 84 89 20 43  5]\n",
      "  6066/10001: episode: 674, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 33.899, mean reward:  3.767 [ 2.325,  5.870], mean action: 51.222 [2.000, 89.000],  loss: 16.597904, mae: 6.795526, mean_q: 16.000298\n",
      "[28  8 85 47 44 53 65 82 85 10]\n",
      "  6075/10001: episode: 675, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 21.805, mean reward:  2.423 [-10.000,  6.478], mean action: 53.222 [8.000, 85.000],  loss: 14.336404, mae: 7.039466, mean_q: 16.346073\n",
      "[93  2 82 33 96  0  4  4  4 89]\n",
      "  6084/10001: episode: 676, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  7.215, mean reward:  0.802 [-10.000,  5.261], mean action: 34.889 [0.000, 96.000],  loss: 15.598675, mae: 7.269802, mean_q: 16.836082\n",
      "[65  8 25 75 84 42 85 11 10 11]\n",
      "  6093/10001: episode: 677, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 21.229, mean reward:  2.359 [-10.000,  5.414], mean action: 39.000 [8.000, 85.000],  loss: 16.791546, mae: 7.226675, mean_q: 16.610258\n",
      "[63  8 33  8 39 63 84 20 96 30]\n",
      "  6102/10001: episode: 678, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 10.696, mean reward:  1.188 [-10.000,  7.721], mean action: 42.333 [8.000, 96.000],  loss: 15.840735, mae: 7.298243, mean_q: 17.508717\n",
      "[17 63 16 25 53 45 80 26 40 30]\n",
      "  6111/10001: episode: 679, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 40.950, mean reward:  4.550 [ 2.238,  6.969], mean action: 42.000 [16.000, 80.000],  loss: 14.737950, mae: 6.993637, mean_q: 16.702869\n",
      "[92  8 63 50 50 78 39 19 82 40]\n",
      "  6120/10001: episode: 680, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 24.336, mean reward:  2.704 [-10.000,  8.072], mean action: 47.667 [8.000, 82.000],  loss: 14.729771, mae: 6.839727, mean_q: 16.185417\n",
      "[98  8 39 97 78 17 92 92 43 40]\n",
      "  6129/10001: episode: 681, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.769, mean reward:  2.863 [-10.000,  7.683], mean action: 56.222 [8.000, 97.000],  loss: 14.611774, mae: 7.103296, mean_q: 16.303122\n",
      "[86 49 78 97 47 11 89 73 73 40]\n",
      "  6138/10001: episode: 682, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.815, mean reward:  2.646 [-10.000,  6.303], mean action: 61.889 [11.000, 97.000],  loss: 13.557714, mae: 7.094693, mean_q: 16.281301\n",
      "[60 17 73 33 84 85 55 94 85 48]\n",
      "  6147/10001: episode: 683, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.480, mean reward:  2.498 [-10.000,  7.512], mean action: 63.778 [17.000, 94.000],  loss: 15.583193, mae: 6.800499, mean_q: 15.795469\n",
      "[66  4  2 43 96 93 93 89 89 30]\n",
      "  6156/10001: episode: 684, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  8.611, mean reward:  0.957 [-10.000,  6.409], mean action: 59.889 [2.000, 96.000],  loss: 14.443624, mae: 7.435627, mean_q: 17.193647\n",
      "[51 44 43 89 44 40 92 89 30 30]\n",
      "  6165/10001: episode: 685, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: -5.838, mean reward: -0.649 [-10.000,  6.163], mean action: 55.667 [30.000, 92.000],  loss: 18.000351, mae: 6.199543, mean_q: 14.852260\n",
      "[95 73 97 97 47 37 25 89 30 10]\n",
      "  6174/10001: episode: 686, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.088, mean reward:  2.565 [-10.000,  5.897], mean action: 56.111 [10.000, 97.000],  loss: 16.951469, mae: 6.973154, mean_q: 16.570713\n",
      "[57  8 13 68 82 93 39 50 94 30]\n",
      "  6183/10001: episode: 687, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.419, mean reward:  4.047 [ 2.276,  8.418], mean action: 53.000 [8.000, 94.000],  loss: 16.958469, mae: 6.888475, mean_q: 16.408607\n",
      "[94 61 25 61 27 97 73 68 75 11]\n",
      "  6192/10001: episode: 688, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 23.790, mean reward:  2.643 [-10.000,  6.370], mean action: 55.333 [11.000, 97.000],  loss: 15.500841, mae: 7.115667, mean_q: 16.837629\n",
      "[62  8 25 38 82 17 17 11 41 39]\n",
      "  6201/10001: episode: 689, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.262, mean reward:  2.474 [-10.000,  5.113], mean action: 30.889 [8.000, 82.000],  loss: 12.672139, mae: 7.146470, mean_q: 16.073845\n",
      "[38  8  8 82 47 94 25 47 44 11]\n",
      "  6210/10001: episode: 690, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  7.640, mean reward:  0.849 [-10.000,  6.153], mean action: 40.667 [8.000, 94.000],  loss: 18.311382, mae: 7.206802, mean_q: 16.435261\n",
      "[21  2 97 84 53 11 50 85 12 43]\n",
      "  6219/10001: episode: 691, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 34.302, mean reward:  3.811 [ 2.418,  6.880], mean action: 48.556 [2.000, 97.000],  loss: 18.240751, mae: 6.815450, mean_q: 15.655575\n",
      "[89 10 25 82 96 17 94 85 30 27]\n",
      "  6228/10001: episode: 692, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 38.707, mean reward:  4.301 [ 2.308,  7.799], mean action: 51.778 [10.000, 96.000],  loss: 15.550044, mae: 7.385623, mean_q: 17.076180\n",
      "[97 92 63 38 38 97 82 78 85  1]\n",
      "  6237/10001: episode: 693, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  5.471, mean reward:  0.608 [-10.000,  5.476], mean action: 63.778 [1.000, 97.000],  loss: 16.683674, mae: 7.374074, mean_q: 16.946360\n",
      "[28 37 85 33  2 84 78 85 22 40]\n",
      "  6246/10001: episode: 694, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 23.019, mean reward:  2.558 [-10.000,  7.441], mean action: 51.778 [2.000, 85.000],  loss: 16.657507, mae: 6.812612, mean_q: 15.764455\n",
      "[93  2 97 97 89  0 11 26 38 11]\n",
      "  6255/10001: episode: 695, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  5.818, mean reward:  0.646 [-10.000,  4.889], mean action: 41.222 [0.000, 97.000],  loss: 13.048548, mae: 7.401515, mean_q: 16.988537\n",
      "[35 17 53 94 47 32 68 84 30 47]\n",
      "  6264/10001: episode: 696, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 17.895, mean reward:  1.988 [-10.000,  7.681], mean action: 52.444 [17.000, 94.000],  loss: 17.029228, mae: 6.833727, mean_q: 15.756557\n",
      "[22 45 73 15 33 16 39 89 94 53]\n",
      "  6273/10001: episode: 697, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 33.745, mean reward:  3.749 [ 2.531,  5.859], mean action: 50.778 [15.000, 94.000],  loss: 15.957027, mae: 6.747353, mean_q: 15.496037\n",
      "[45 11 97 84 53 65 39 93 84 35]\n",
      "  6282/10001: episode: 698, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 19.736, mean reward:  2.193 [-10.000,  5.478], mean action: 62.333 [11.000, 97.000],  loss: 14.201789, mae: 7.131689, mean_q: 16.560133\n",
      "[25 92 13 80 38 35 13 73 15 15]\n",
      "  6291/10001: episode: 699, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  5.328, mean reward:  0.592 [-10.000,  5.727], mean action: 41.556 [13.000, 92.000],  loss: 15.200237, mae: 6.763641, mean_q: 16.052959\n",
      "[84 11 97 19 44 39 44 19 73  2]\n",
      "  6300/10001: episode: 700, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 12.383, mean reward:  1.376 [-10.000,  9.855], mean action: 38.667 [2.000, 97.000],  loss: 18.793562, mae: 6.640428, mean_q: 15.721059\n",
      "[71 45 33 13 89 45 44 27 85 43]\n",
      "  6309/10001: episode: 701, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.479, mean reward:  2.831 [-10.000,  6.701], mean action: 47.111 [13.000, 89.000],  loss: 14.278761, mae: 7.022138, mean_q: 15.975100\n",
      "[76 45 38 84 47 85 78 19 65  2]\n",
      "  6318/10001: episode: 702, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.104, mean reward:  4.012 [ 2.114,  9.841], mean action: 51.444 [2.000, 85.000],  loss: 14.408637, mae: 7.221871, mean_q: 16.530962\n",
      "[96 10 33 84 96 85 78 97 93 39]\n",
      "  6327/10001: episode: 703, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 22.043, mean reward:  2.449 [-10.000,  5.065], mean action: 68.333 [10.000, 97.000],  loss: 11.492002, mae: 7.197501, mean_q: 16.407722\n",
      "[20 39 15 45 75 40 45 10 47 43]\n",
      "  6336/10001: episode: 704, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 23.715, mean reward:  2.635 [-10.000,  6.817], mean action: 39.889 [10.000, 75.000],  loss: 13.738612, mae: 7.353322, mean_q: 16.705864\n",
      "[53 40 27 40 82 32 80 40 94 30]\n",
      "  6345/10001: episode: 705, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.466, mean reward:  0.941 [-10.000,  7.058], mean action: 51.667 [27.000, 94.000],  loss: 15.899499, mae: 6.830327, mean_q: 15.770861\n",
      "[68 40 38 13 13 53 80 97 40 44]\n",
      "  6354/10001: episode: 706, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  7.696, mean reward:  0.855 [-10.000,  5.637], mean action: 46.444 [13.000, 97.000],  loss: 15.387131, mae: 7.629779, mean_q: 17.838104\n",
      "[48 37 37 73 82 45 43 97 27 65]\n",
      "  6363/10001: episode: 707, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 25.243, mean reward:  2.805 [-10.000,  6.810], mean action: 56.222 [27.000, 97.000],  loss: 13.257098, mae: 7.129547, mean_q: 16.797859\n",
      "[63 40 77 77 82 93 44 63 85  2]\n",
      "  6372/10001: episode: 708, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 13.333, mean reward:  1.481 [-10.000,  9.745], mean action: 62.556 [2.000, 93.000],  loss: 15.276021, mae: 7.072427, mean_q: 16.550711\n",
      "[14  8 85 32 82 37 93 94 40 84]\n",
      "  6381/10001: episode: 709, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward: 34.086, mean reward:  3.787 [ 2.134,  6.996], mean action: 61.667 [8.000, 94.000],  loss: 13.316652, mae: 7.077956, mean_q: 16.253845\n",
      "[74 78 63 80 44 11 82 11 53 77]\n",
      "  6390/10001: episode: 710, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 19.973, mean reward:  2.219 [-10.000,  5.782], mean action: 55.444 [11.000, 82.000],  loss: 15.696269, mae: 7.223324, mean_q: 16.667627\n",
      "[33 40 13 63 75 45 84 11 93 30]\n",
      "  6399/10001: episode: 711, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward: 35.544, mean reward:  3.949 [ 1.996,  7.050], mean action: 50.444 [11.000, 93.000],  loss: 13.830608, mae: 7.149659, mean_q: 16.466387\n",
      "[96 10 82 72 82 85 43 40 75 40]\n",
      "  6408/10001: episode: 712, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  7.372, mean reward:  0.819 [-10.000,  5.537], mean action: 58.778 [10.000, 85.000],  loss: 16.437920, mae: 7.206044, mean_q: 16.595791\n",
      "[52 49 27 25 75 80 94  8 78 72]\n",
      "  6417/10001: episode: 713, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 33.995, mean reward:  3.777 [ 2.587,  4.847], mean action: 56.444 [8.000, 94.000],  loss: 13.868404, mae: 6.626284, mean_q: 15.195114\n",
      "[11  2 80 82 75 42 96  8 94 35]\n",
      "  6426/10001: episode: 714, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 33.325, mean reward:  3.703 [ 2.656,  5.119], mean action: 57.111 [2.000, 96.000],  loss: 14.623496, mae: 7.235418, mean_q: 16.720690\n",
      "[66 10 16 53 44 11 45 84 30 27]\n",
      "  6435/10001: episode: 715, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 37.935, mean reward:  4.215 [ 2.563,  7.913], mean action: 35.556 [10.000, 84.000],  loss: 13.583212, mae: 7.203757, mean_q: 16.620577\n",
      "[27 45 80 32 39 40 45 11 78 30]\n",
      "  6444/10001: episode: 716, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 21.724, mean reward:  2.414 [-10.000,  7.017], mean action: 44.444 [11.000, 80.000],  loss: 15.142644, mae: 7.284844, mean_q: 16.978271\n",
      "[68 35 80 32 47 80 17 85 21 72]\n",
      "  6453/10001: episode: 717, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 19.319, mean reward:  2.147 [-10.000,  5.176], mean action: 52.111 [17.000, 85.000],  loss: 14.589949, mae: 6.964067, mean_q: 16.239841\n",
      "[83 16 38 33 96 45 37 80 97  2]\n",
      "  6462/10001: episode: 718, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 41.684, mean reward:  4.632 [ 2.634,  9.238], mean action: 49.333 [2.000, 97.000],  loss: 11.609649, mae: 7.214014, mean_q: 16.688354\n",
      "[51  0 33 84 78 45 17 85 27 10]\n",
      "  6471/10001: episode: 719, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.625, mean reward:  4.292 [ 2.003,  7.436], mean action: 42.111 [0.000, 85.000],  loss: 16.644857, mae: 7.137469, mean_q: 16.511696\n",
      "[44 85 73  0 33 65 77 77 35 48]\n",
      "  6480/10001: episode: 720, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 23.053, mean reward:  2.561 [-10.000,  6.639], mean action: 54.778 [0.000, 85.000],  loss: 13.347190, mae: 7.287468, mean_q: 16.672497\n",
      "[57  2 82 33 77 42 65 97 96  2]\n",
      "  6489/10001: episode: 721, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 20.366, mean reward:  2.263 [-10.000,  5.030], mean action: 55.111 [2.000, 97.000],  loss: 16.463263, mae: 7.102079, mean_q: 16.674057\n",
      "[87  8 92 39 32 65 47 84 89 72]\n",
      "  6498/10001: episode: 722, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 30.765, mean reward:  3.418 [ 2.063,  5.602], mean action: 58.667 [8.000, 92.000],  loss: 18.175026, mae: 6.648573, mean_q: 15.517178\n",
      "[43 33  2 25 75 32 53 40 40  1]\n",
      "  6507/10001: episode: 723, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 19.148, mean reward:  2.128 [-10.000,  5.880], mean action: 33.444 [1.000, 75.000],  loss: 14.805984, mae: 7.272679, mean_q: 16.695139\n",
      "[ 5  2 82 33 61 89 39 97 85 85]\n",
      "  6516/10001: episode: 724, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 22.408, mean reward:  2.490 [-10.000,  5.058], mean action: 63.667 [2.000, 97.000],  loss: 14.837017, mae: 6.792385, mean_q: 15.801244\n",
      "[71  2 84 96 39 94 82 94 80 85]\n",
      "  6525/10001: episode: 725, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 24.831, mean reward:  2.759 [-10.000,  7.118], mean action: 72.889 [2.000, 96.000],  loss: 14.720347, mae: 7.292234, mean_q: 16.857908\n",
      "[43 33  2 82 97 39 45 82 27 30]\n",
      "  6534/10001: episode: 726, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.003, mean reward:  2.778 [-10.000,  6.085], mean action: 48.556 [2.000, 97.000],  loss: 12.957662, mae: 6.840996, mean_q: 15.997149\n",
      "[71 13 33 53 75 40 96  2 94 50]\n",
      "  6543/10001: episode: 727, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 35.341, mean reward:  3.927 [ 2.590,  6.635], mean action: 50.667 [2.000, 96.000],  loss: 14.293752, mae: 7.572456, mean_q: 17.182735\n",
      "[13 96 45 33 75 84 94 94 43 72]\n",
      "  6552/10001: episode: 728, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 20.023, mean reward:  2.225 [-10.000,  6.315], mean action: 70.667 [33.000, 96.000],  loss: 15.123256, mae: 7.426738, mean_q: 17.014652\n",
      "[73 16 33 97 47 40 89 26 85 96]\n",
      "  6561/10001: episode: 729, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 35.615, mean reward:  3.957 [ 2.538,  4.703], mean action: 58.778 [16.000, 97.000],  loss: 14.006035, mae: 7.413622, mean_q: 16.681305\n",
      "[90  2  2 33 89 92 38 85 49 49]\n",
      "  6570/10001: episode: 730, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward:  7.510, mean reward:  0.834 [-10.000,  4.866], mean action: 48.778 [2.000, 92.000],  loss: 13.942802, mae: 7.097860, mean_q: 16.385925\n",
      "[75 45 75 50 55 93 94 11 82 30]\n",
      "  6579/10001: episode: 731, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 25.404, mean reward:  2.823 [-10.000,  7.316], mean action: 59.444 [11.000, 94.000],  loss: 15.027717, mae: 6.936738, mean_q: 16.493519\n",
      "[42 45  2 33 75 17 80 40 30 10]\n",
      "  6588/10001: episode: 732, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.895, mean reward:  4.433 [ 2.632,  5.812], mean action: 36.889 [2.000, 80.000],  loss: 15.526362, mae: 7.174268, mean_q: 17.023272\n",
      "[30  8 82 33 96 42 96 11 30 66]\n",
      "  6597/10001: episode: 733, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  8.740, mean reward:  0.971 [-10.000,  6.511], mean action: 51.556 [8.000, 96.000],  loss: 15.987679, mae: 7.223636, mean_q: 16.653807\n",
      "[92  2 73 94 96 42 92 10 82 35]\n",
      "  6606/10001: episode: 734, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 21.453, mean reward:  2.384 [-10.000,  6.406], mean action: 58.444 [2.000, 96.000],  loss: 15.615261, mae: 6.693495, mean_q: 15.607346\n",
      "[69 13 53 53 75 26 53 97 65 35]\n",
      "  6615/10001: episode: 735, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  8.381, mean reward:  0.931 [-10.000,  5.734], mean action: 52.222 [13.000, 97.000],  loss: 13.633013, mae: 7.585299, mean_q: 17.025486\n",
      "[83 78 94 43 75 93 65 93 50 85]\n",
      "  6624/10001: episode: 736, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 19.265, mean reward:  2.141 [-10.000,  6.097], mean action: 75.111 [43.000, 94.000],  loss: 13.683971, mae: 7.368978, mean_q: 17.022854\n",
      "[93 10 32 97 96  0  0  0  0  0]\n",
      "  6633/10001: episode: 737, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -24.915, mean reward: -2.768 [-10.000,  3.789], mean action: 26.111 [0.000, 97.000],  loss: 15.302351, mae: 7.495739, mean_q: 17.143415\n",
      "[ 5 93 27 92 75 11 39 97 27 27]\n",
      "  6642/10001: episode: 738, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  7.129, mean reward:  0.792 [-10.000,  4.953], mean action: 54.222 [11.000, 97.000],  loss: 15.631701, mae: 6.841077, mean_q: 15.702586\n",
      "[25 93 97 32 47 11 55 11 80  2]\n",
      "  6651/10001: episode: 739, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 25.301, mean reward:  2.811 [-10.000,  9.250], mean action: 47.556 [2.000, 97.000],  loss: 16.531523, mae: 7.085724, mean_q: 16.102953\n",
      "[68 45 38 93 96 32 93 97 27 12]\n",
      "  6660/10001: episode: 740, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 22.194, mean reward:  2.466 [-10.000,  7.194], mean action: 59.222 [12.000, 97.000],  loss: 13.059170, mae: 7.255829, mean_q: 16.554432\n",
      "[31 89 38 55 75 40 37 47 65 74]\n",
      "  6669/10001: episode: 741, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward: 36.402, mean reward:  4.045 [ 2.396,  6.942], mean action: 57.778 [37.000, 89.000],  loss: 14.574389, mae: 7.068641, mean_q: 16.201803\n",
      "[53 89 77 78 75 40 77 65 11 40]\n",
      "  6678/10001: episode: 742, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  7.251, mean reward:  0.806 [-10.000,  5.535], mean action: 61.333 [11.000, 89.000],  loss: 14.459939, mae: 7.276007, mean_q: 16.790291\n",
      "[80 40 50 65 75 32 65 68 43 43]\n",
      "  6687/10001: episode: 743, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  3.381, mean reward:  0.376 [-10.000,  6.864], mean action: 53.444 [32.000, 75.000],  loss: 12.880346, mae: 7.471628, mean_q: 16.936384\n",
      "[41 89 84 33 65 13 80 85 30 30]\n",
      "  6696/10001: episode: 744, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.297, mean reward:  2.477 [-10.000,  6.470], mean action: 56.556 [13.000, 89.000],  loss: 12.424987, mae: 7.032543, mean_q: 16.140417\n",
      "[40 33 15 82 82 85 82 85 27 12]\n",
      "  6705/10001: episode: 745, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -7.198, mean reward: -0.800 [-10.000,  6.359], mean action: 55.889 [12.000, 85.000],  loss: 15.548009, mae: 7.226786, mean_q: 16.573082\n",
      "[10 15 92 39 89 11 92 89 40 55]\n",
      "  6714/10001: episode: 746, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  6.696, mean reward:  0.744 [-10.000,  6.177], mean action: 58.000 [11.000, 92.000],  loss: 13.586012, mae: 7.135095, mean_q: 16.430456\n",
      "[ 8 49 92 16 94 32 55 85 50 10]\n",
      "  6723/10001: episode: 747, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 32.616, mean reward:  3.624 [ 2.200,  6.938], mean action: 53.667 [10.000, 94.000],  loss: 15.560781, mae: 6.830748, mean_q: 15.917099\n",
      "[78 38 82 33 84 80 93 27 30 30]\n",
      "  6732/10001: episode: 748, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 25.078, mean reward:  2.786 [-10.000,  6.065], mean action: 55.222 [27.000, 93.000],  loss: 13.506913, mae: 7.377543, mean_q: 17.014462\n",
      "[63 43  2 55 80 11 17 77 27 92]\n",
      "  6741/10001: episode: 749, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 38.639, mean reward:  4.293 [ 2.325,  7.705], mean action: 44.889 [2.000, 92.000],  loss: 15.043550, mae: 7.281335, mean_q: 16.768440\n",
      "[50  8 68 39 47 80 84 30 82 94]\n",
      "  6750/10001: episode: 750, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 37.362, mean reward:  4.151 [ 2.309,  6.951], mean action: 59.111 [8.000, 94.000],  loss: 13.936408, mae: 7.245487, mean_q: 16.678724\n",
      "[17 30 27 53 96 49 40 94 30 30]\n",
      "  6759/10001: episode: 751, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  5.838, mean reward:  0.649 [-10.000,  4.971], mean action: 49.889 [27.000, 96.000],  loss: 14.628850, mae: 7.516753, mean_q: 16.746117\n",
      "[65 40 25 97 82 11 38 97 30 44]\n",
      "  6768/10001: episode: 752, duration: 0.064s, episode steps:   9, steps per second: 141, episode reward: 23.001, mean reward:  2.556 [-10.000,  6.069], mean action: 51.556 [11.000, 97.000],  loss: 14.012752, mae: 7.465993, mean_q: 16.672447\n",
      "[59 72 27 93 82 45 50 97 12 43]\n",
      "  6777/10001: episode: 753, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 34.212, mean reward:  3.801 [ 2.089,  6.692], mean action: 57.889 [12.000, 97.000],  loss: 13.544933, mae: 7.017215, mean_q: 16.275368\n",
      "[96 30 25 80 96 11 85 97  8 35]\n",
      "  6786/10001: episode: 754, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 23.725, mean reward:  2.636 [-10.000,  5.092], mean action: 51.889 [8.000, 97.000],  loss: 14.556249, mae: 7.394984, mean_q: 17.245039\n",
      "[93 43 10 38 96 45 39 40 89 30]\n",
      "  6795/10001: episode: 755, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 38.033, mean reward:  4.226 [ 2.952,  6.795], mean action: 47.778 [10.000, 96.000],  loss: 12.907729, mae: 7.395071, mean_q: 16.927420\n",
      "[81  8 75 19 53 82 39 61 78 26]\n",
      "  6804/10001: episode: 756, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward: 34.548, mean reward:  3.839 [ 2.092,  6.452], mean action: 49.000 [8.000, 82.000],  loss: 16.750908, mae: 7.331299, mean_q: 16.712799\n",
      "[71  4 97 82 96 45 82 25 92 48]\n",
      "  6813/10001: episode: 757, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward: 24.078, mean reward:  2.675 [-10.000,  6.387], mean action: 63.444 [4.000, 97.000],  loss: 14.122901, mae: 7.530245, mean_q: 17.042398\n",
      "[ 2 40 43 33 44 40 80 11 30 11]\n",
      "  6822/10001: episode: 758, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  8.134, mean reward:  0.904 [-10.000,  5.879], mean action: 36.889 [11.000, 80.000],  loss: 14.932425, mae: 7.702781, mean_q: 17.360451\n",
      "[42  4 15 97 17 43 16 97 12 43]\n",
      "  6831/10001: episode: 759, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  4.196, mean reward:  0.466 [-10.000,  4.417], mean action: 38.222 [4.000, 97.000],  loss: 11.921639, mae: 7.326993, mean_q: 16.801374\n",
      "[32 30 37 39 44 82 77 82 12 49]\n",
      "  6840/10001: episode: 760, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 20.797, mean reward:  2.311 [-10.000,  5.428], mean action: 50.222 [12.000, 82.000],  loss: 13.305259, mae: 7.993668, mean_q: 18.223436\n",
      "[19  4 27 82 44 11 45 65  5 43]\n",
      "  6849/10001: episode: 761, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 37.299, mean reward:  4.144 [ 2.582,  7.028], mean action: 36.222 [4.000, 82.000],  loss: 12.889297, mae: 7.769713, mean_q: 17.846518\n",
      "[66  4 82  0 33 93 82 19 50 30]\n",
      "  6858/10001: episode: 762, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.331, mean reward:  2.259 [-10.000,  7.552], mean action: 43.667 [0.000, 93.000],  loss: 12.474999, mae: 7.657845, mean_q: 17.208696\n",
      "[45  4 80  0  0  0 93 50 40 48]\n",
      "  6867/10001: episode: 763, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 11.366, mean reward:  1.263 [-10.000,  7.669], mean action: 35.000 [0.000, 93.000],  loss: 13.265572, mae: 7.883018, mean_q: 17.542543\n",
      "[59 93  2 85 17 53 80 97 30 30]\n",
      "  6876/10001: episode: 764, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 21.598, mean reward:  2.400 [-10.000,  6.041], mean action: 54.111 [2.000, 97.000],  loss: 14.337276, mae: 7.436284, mean_q: 16.862806\n",
      "[ 6 85 85 82 82 43 92 40 89 35]\n",
      "  6885/10001: episode: 765, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  7.693, mean reward:  0.855 [-10.000,  5.195], mean action: 70.333 [35.000, 92.000],  loss: 13.147966, mae: 7.497008, mean_q: 17.313499\n",
      "[61 89 77 77  4 75 82 50 73 30]\n",
      "  6894/10001: episode: 766, duration: 0.065s, episode steps:   9, steps per second: 140, episode reward: 25.395, mean reward:  2.822 [-10.000,  7.011], mean action: 61.889 [4.000, 89.000],  loss: 15.485470, mae: 7.771383, mean_q: 17.591148\n",
      "[44 35  2  2 77 80 93 82 89  0]\n",
      "  6903/10001: episode: 767, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.213, mean reward:  2.468 [-10.000,  5.804], mean action: 51.111 [0.000, 93.000],  loss: 15.878084, mae: 7.147621, mean_q: 16.490486\n",
      "[79  4 43 33 78 26 37 50 80 35]\n",
      "  6912/10001: episode: 768, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 35.884, mean reward:  3.987 [ 2.507,  6.786], mean action: 42.889 [4.000, 80.000],  loss: 15.082332, mae: 7.317326, mean_q: 16.186193\n",
      "[23 33 97  0 39 35 94 10 80  2]\n",
      "  6921/10001: episode: 769, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 41.378, mean reward:  4.598 [ 1.842, 10.101], mean action: 43.333 [0.000, 97.000],  loss: 12.765274, mae: 7.366896, mean_q: 16.242876\n",
      "[64 73  2 27 25 45 77 11 10 10]\n",
      "  6930/10001: episode: 770, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward: 22.327, mean reward:  2.481 [-10.000,  4.592], mean action: 31.111 [2.000, 77.000],  loss: 13.246628, mae: 7.828084, mean_q: 16.950861\n",
      "[80 92 15 63 75 73 97 41 20 41]\n",
      "  6939/10001: episode: 771, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 19.703, mean reward:  2.189 [-10.000,  4.923], mean action: 57.444 [15.000, 97.000],  loss: 14.052497, mae: 7.798185, mean_q: 17.157274\n",
      "[85 45 25 75 82  2  2  2 39  0]\n",
      "  6948/10001: episode: 772, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  5.603, mean reward:  0.623 [-10.000,  5.502], mean action: 30.222 [0.000, 82.000],  loss: 17.298271, mae: 8.166997, mean_q: 17.580589\n",
      "[28 41  2 97 75 41 77 97 73 53]\n",
      "  6957/10001: episode: 773, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  7.744, mean reward:  0.860 [-10.000,  5.326], mean action: 61.778 [2.000, 97.000],  loss: 11.716899, mae: 7.709280, mean_q: 16.863831\n",
      "[86 38 38 75 96 41 96 11 41  2]\n",
      "  6966/10001: episode: 774, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -2.648, mean reward: -0.294 [-10.000,  8.060], mean action: 48.667 [2.000, 96.000],  loss: 15.811481, mae: 7.503751, mean_q: 16.623405\n",
      "[70 85 85 43 15 38 93 85 97 48]\n",
      "  6975/10001: episode: 775, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 11.463, mean reward:  1.274 [-10.000,  7.357], mean action: 65.444 [15.000, 97.000],  loss: 15.010509, mae: 7.719335, mean_q: 16.970987\n",
      "[79  4 97 33 43 96 50 41 75  2]\n",
      "  6984/10001: episode: 776, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 39.363, mean reward:  4.374 [ 2.171,  8.478], mean action: 49.000 [2.000, 97.000],  loss: 12.108492, mae: 8.032637, mean_q: 17.751390\n",
      "[22  8 75 53 97 26 11 41 73  2]\n",
      "  6993/10001: episode: 777, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 40.304, mean reward:  4.478 [ 2.834,  7.894], mean action: 42.889 [2.000, 97.000],  loss: 14.050910, mae: 8.043156, mean_q: 17.867664\n",
      "[ 5 10  2  2  2 84 84 10 80  2]\n",
      "  7002/10001: episode: 778, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -33.378, mean reward: -3.709 [-10.000,  6.613], mean action: 30.667 [2.000, 84.000],  loss: 15.149628, mae: 7.863429, mean_q: 17.470428\n",
      "[12 13  2  2 97 20 85 39 80  2]\n",
      "  7011/10001: episode: 779, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  9.339, mean reward:  1.038 [-10.000,  6.911], mean action: 37.778 [2.000, 97.000],  loss: 12.799210, mae: 8.051100, mean_q: 17.879599\n",
      "[36 38 33 53 97 11 40 26 80  2]\n",
      "  7020/10001: episode: 780, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.645, mean reward:  4.738 [ 2.736,  8.243], mean action: 42.222 [2.000, 97.000],  loss: 15.592756, mae: 7.652023, mean_q: 17.079702\n",
      "[13  4 53 65 78 85 25 97 93 26]\n",
      "  7029/10001: episode: 781, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 37.877, mean reward:  4.209 [ 2.499,  5.525], mean action: 58.444 [4.000, 97.000],  loss: 15.635139, mae: 7.893693, mean_q: 17.578791\n",
      "[97 72 27 13 35 92 97 41 92 12]\n",
      "  7038/10001: episode: 782, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward:  4.462, mean reward:  0.496 [-10.000,  5.144], mean action: 53.444 [12.000, 97.000],  loss: 16.101299, mae: 7.923367, mean_q: 17.482698\n",
      "[44 73 33 73 75  2 65  8 40 10]\n",
      "  7047/10001: episode: 783, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 25.451, mean reward:  2.828 [-10.000,  5.944], mean action: 42.111 [2.000, 75.000],  loss: 15.117578, mae: 7.361872, mean_q: 16.628193\n",
      "[20 96 37 72 96 85 39 40 65  2]\n",
      "  7056/10001: episode: 784, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 26.669, mean reward:  2.963 [-10.000,  9.532], mean action: 59.111 [2.000, 96.000],  loss: 13.694318, mae: 7.506172, mean_q: 16.911404\n",
      "[86 96 32 93 75 17 80 77 11 44]\n",
      "  7065/10001: episode: 785, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward: 36.734, mean reward:  4.082 [ 2.231,  5.639], mean action: 58.333 [11.000, 96.000],  loss: 13.926812, mae: 7.915404, mean_q: 17.521082\n",
      "[96 35 92 96 80 11 37 30  5 92]\n",
      "  7074/10001: episode: 786, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward:  7.040, mean reward:  0.782 [-10.000,  5.776], mean action: 53.111 [5.000, 96.000],  loss: 13.955471, mae: 7.932899, mean_q: 17.983858\n",
      "[67 80  8 10  2 92 44 10 80 30]\n",
      "  7083/10001: episode: 787, duration: 0.077s, episode steps:   9, steps per second: 118, episode reward:  8.577, mean reward:  0.953 [-10.000,  6.736], mean action: 39.556 [2.000, 92.000],  loss: 14.780535, mae: 7.704819, mean_q: 17.547258\n",
      "[90 80 15  2 97 80 78 20 39  2]\n",
      "  7092/10001: episode: 788, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  6.432, mean reward:  0.715 [-10.000,  5.135], mean action: 45.889 [2.000, 97.000],  loss: 13.221403, mae: 7.751002, mean_q: 17.652748\n",
      "[24 92 25 43 75 11 45 89  5 12]\n",
      "  7101/10001: episode: 789, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 33.622, mean reward:  3.736 [ 2.966,  4.773], mean action: 44.111 [5.000, 92.000],  loss: 14.875434, mae: 8.090091, mean_q: 18.155125\n",
      "[22 63  2  2 80 11 45 78 92 41]\n",
      "  7110/10001: episode: 790, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 23.348, mean reward:  2.594 [-10.000,  5.880], mean action: 46.000 [2.000, 92.000],  loss: 15.190242, mae: 7.441827, mean_q: 16.697262\n",
      "[63 45 77 15 53 92 77 11 82 82]\n",
      "  7119/10001: episode: 791, duration: 0.072s, episode steps:   9, steps per second: 126, episode reward:  8.527, mean reward:  0.947 [-10.000,  6.875], mean action: 59.333 [11.000, 92.000],  loss: 12.267521, mae: 7.609102, mean_q: 17.025383\n",
      "[ 7 65 82 82 96 40 84 27 80 26]\n",
      "  7128/10001: episode: 792, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 25.297, mean reward:  2.811 [-10.000,  6.392], mean action: 64.667 [26.000, 96.000],  loss: 13.487203, mae: 8.161865, mean_q: 17.886503\n",
      "[49 42 27 15 78 39 44  2 40 35]\n",
      "  7137/10001: episode: 793, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.501, mean reward:  4.389 [ 2.593,  7.740], mean action: 35.778 [2.000, 78.000],  loss: 16.469597, mae: 7.966868, mean_q: 17.911215\n",
      "[15 30 33 33 97 45 65 10 73 48]\n",
      "  7146/10001: episode: 794, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 25.409, mean reward:  2.823 [-10.000,  6.852], mean action: 48.222 [10.000, 97.000],  loss: 14.482215, mae: 7.832083, mean_q: 17.709538\n",
      "[85 13 33 53 53 45 93  8 20 53]\n",
      "  7155/10001: episode: 795, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward:  7.854, mean reward:  0.873 [-10.000,  5.001], mean action: 41.222 [8.000, 93.000],  loss: 15.266602, mae: 7.526803, mean_q: 16.596809\n",
      "[82 35 97 68 96 65 39 26 96  2]\n",
      "  7164/10001: episode: 796, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 23.415, mean reward:  2.602 [-10.000,  9.384], mean action: 58.222 [2.000, 97.000],  loss: 16.346941, mae: 8.141680, mean_q: 18.082336\n",
      "[ 4 94 27 20 82 80 80 49 40 55]\n",
      "  7173/10001: episode: 797, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.328, mean reward:  2.259 [-10.000,  5.601], mean action: 58.556 [20.000, 94.000],  loss: 16.551422, mae: 7.898782, mean_q: 17.331099\n",
      "[36 80 92 16  0 37 77 10 80  2]\n",
      "  7182/10001: episode: 798, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 24.930, mean reward:  2.770 [-10.000, 10.301], mean action: 43.778 [0.000, 92.000],  loss: 14.193417, mae: 7.976005, mean_q: 17.417849\n",
      "[83 75 73 84 73 20 55  8 73 40]\n",
      "  7191/10001: episode: 799, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  6.997, mean reward:  0.777 [-10.000,  6.940], mean action: 55.667 [8.000, 84.000],  loss: 14.088061, mae: 7.759374, mean_q: 17.371584\n",
      "[77 80 77 33 73 32 77 97 73 39]\n",
      "  7200/10001: episode: 800, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -6.113, mean reward: -0.679 [-10.000,  5.293], mean action: 64.556 [32.000, 97.000],  loss: 13.566916, mae: 7.895742, mean_q: 17.257010\n",
      "[17 30 77 78 25 32 80 20 97 43]\n",
      "  7209/10001: episode: 801, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 38.859, mean reward:  4.318 [ 2.262,  7.185], mean action: 53.556 [20.000, 97.000],  loss: 13.747121, mae: 7.541225, mean_q: 16.538033\n",
      "[67 94 25 80 73 39 65 11 41 58]\n",
      "  7218/10001: episode: 802, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 35.221, mean reward:  3.913 [ 2.345,  5.500], mean action: 54.000 [11.000, 94.000],  loss: 14.282536, mae: 7.715771, mean_q: 17.152803\n",
      "[14 39 33 33 96 17 45 26 40 68]\n",
      "  7227/10001: episode: 803, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 19.567, mean reward:  2.174 [-10.000,  5.954], mean action: 44.111 [17.000, 96.000],  loss: 13.088142, mae: 7.377479, mean_q: 16.669922\n",
      "[99 85 27 80 38  2 39 13 80 39]\n",
      "  7236/10001: episode: 804, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  5.913, mean reward:  0.657 [-10.000,  5.356], mean action: 44.778 [2.000, 85.000],  loss: 16.290657, mae: 8.301551, mean_q: 18.232269\n",
      "[60 24 53 33 75 89 84 11 40 35]\n",
      "  7245/10001: episode: 805, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 38.079, mean reward:  4.231 [ 2.627,  6.355], mean action: 49.333 [11.000, 89.000],  loss: 14.691654, mae: 7.493792, mean_q: 16.748943\n",
      "[66 17  2 77 78 89 94 11 11 89]\n",
      "  7254/10001: episode: 806, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  4.946, mean reward:  0.550 [-10.000,  4.850], mean action: 52.000 [2.000, 94.000],  loss: 14.545135, mae: 7.701161, mean_q: 16.996002\n",
      "[33  2 82 32 65 40 93 20 75 44]\n",
      "  7263/10001: episode: 807, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 32.714, mean reward:  3.635 [ 1.872,  4.712], mean action: 50.333 [2.000, 93.000],  loss: 12.592958, mae: 7.594602, mean_q: 16.678234\n",
      "[43 92 85 93 53 10 82 53 93 72]\n",
      "  7272/10001: episode: 808, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  5.655, mean reward:  0.628 [-10.000,  5.362], mean action: 70.333 [10.000, 93.000],  loss: 15.007030, mae: 7.895895, mean_q: 17.318642\n",
      "[46 96 94 33 38 53 89 11 30 65]\n",
      "  7281/10001: episode: 809, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 36.275, mean reward:  4.031 [ 2.416,  6.153], mean action: 56.556 [11.000, 96.000],  loss: 15.191354, mae: 7.497643, mean_q: 16.843510\n",
      "[71 40 10 80 78 11 40 80 20 89]\n",
      "  7290/10001: episode: 810, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  8.389, mean reward:  0.932 [-10.000,  6.381], mean action: 49.778 [10.000, 89.000],  loss: 13.041423, mae: 7.568766, mean_q: 16.824043\n",
      "[67 40 80 78 75 40 93 26 96 48]\n",
      "  7299/10001: episode: 811, duration: 0.066s, episode steps:   9, steps per second: 137, episode reward: 23.023, mean reward:  2.558 [-10.000,  6.683], mean action: 64.000 [26.000, 96.000],  loss: 14.958544, mae: 7.397490, mean_q: 16.807495\n",
      "[58 89 80 77 80 82 94  8 26 47]\n",
      "  7308/10001: episode: 812, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 21.559, mean reward:  2.395 [-10.000,  5.487], mean action: 64.778 [8.000, 94.000],  loss: 16.915026, mae: 7.990197, mean_q: 18.007179\n",
      "[87 94 94 38 97 89 94 11 61 89]\n",
      "  7317/10001: episode: 813, duration: 0.067s, episode steps:   9, steps per second: 135, episode reward: -7.262, mean reward: -0.807 [-10.000,  5.444], mean action: 74.111 [11.000, 97.000],  loss: 16.462440, mae: 6.994045, mean_q: 15.572377\n",
      "[77 80 25 27 53 65 44 80 47 30]\n",
      "  7326/10001: episode: 814, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 21.404, mean reward:  2.378 [-10.000,  7.536], mean action: 50.111 [25.000, 80.000],  loss: 15.557297, mae: 7.920569, mean_q: 16.982981\n",
      "[15  2 15 53 53 40 85 40 30 20]\n",
      "  7335/10001: episode: 815, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -3.167, mean reward: -0.352 [-10.000,  6.395], mean action: 37.556 [2.000, 85.000],  loss: 16.185402, mae: 7.657794, mean_q: 16.777693\n",
      "[11 63 25 72 55 11 45 40 20 44]\n",
      "  7344/10001: episode: 816, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 20.270, mean reward:  2.252 [-10.000,  5.747], mean action: 41.667 [11.000, 72.000],  loss: 13.704105, mae: 7.806135, mean_q: 17.366905\n",
      "[48 45 45 85 17 84 93 77 12 47]\n",
      "  7353/10001: episode: 817, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 19.051, mean reward:  2.117 [-10.000,  4.638], mean action: 56.111 [12.000, 93.000],  loss: 15.413694, mae: 7.730044, mean_q: 17.204943\n",
      "[ 2 45 92 78 47 39 78 26 66 47]\n",
      "  7362/10001: episode: 818, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  7.802, mean reward:  0.867 [-10.000,  6.601], mean action: 57.556 [26.000, 92.000],  loss: 14.505090, mae: 8.203444, mean_q: 17.647013\n",
      "[ 9 39 85 33 47 15 30 30 66 30]\n",
      "  7371/10001: episode: 819, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  9.876, mean reward:  1.097 [-10.000,  6.414], mean action: 41.667 [15.000, 85.000],  loss: 13.892841, mae: 7.812656, mean_q: 17.035320\n",
      "[93  2 80 78 17 89 80 27 43 26]\n",
      "  7380/10001: episode: 820, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 24.740, mean reward:  2.749 [-10.000,  6.240], mean action: 49.111 [2.000, 89.000],  loss: 13.906348, mae: 8.222150, mean_q: 17.817230\n",
      "[16  2  8 33 97 85 40  8 26 48]\n",
      "  7389/10001: episode: 821, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 26.150, mean reward:  2.906 [-10.000,  6.490], mean action: 38.556 [2.000, 97.000],  loss: 14.862159, mae: 7.647718, mean_q: 16.860058\n",
      "[38  8 43 93 17 53 93 85 92 66]\n",
      "  7398/10001: episode: 822, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.477, mean reward:  2.497 [-10.000,  7.178], mean action: 61.111 [8.000, 93.000],  loss: 13.926984, mae: 7.216599, mean_q: 15.772518\n",
      "[90 11 78 96 17 96 89 73 82 50]\n",
      "  7407/10001: episode: 823, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 23.090, mean reward:  2.566 [-10.000,  6.626], mean action: 65.778 [11.000, 96.000],  loss: 14.925005, mae: 7.338113, mean_q: 15.978971\n",
      "[84 39  8 16 89 11 85  8 80 48]\n",
      "  7416/10001: episode: 824, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 25.776, mean reward:  2.864 [-10.000,  6.935], mean action: 42.667 [8.000, 89.000],  loss: 14.469864, mae: 7.798063, mean_q: 16.987738\n",
      "[40 15 73 33 82 40 43 40 75  5]\n",
      "  7425/10001: episode: 825, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  6.805, mean reward:  0.756 [-10.000,  5.076], mean action: 45.111 [5.000, 82.000],  loss: 15.758946, mae: 7.623394, mean_q: 16.688683\n",
      "[53 75  8 77  2 53 78 27 50 66]\n",
      "  7434/10001: episode: 826, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 24.985, mean reward:  2.776 [-10.000,  7.104], mean action: 48.444 [2.000, 78.000],  loss: 14.090092, mae: 7.896130, mean_q: 16.935719\n",
      "[19 73 50 84 42 15 85 45 30 65]\n",
      "  7443/10001: episode: 827, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.201, mean reward:  4.133 [ 1.906,  6.782], mean action: 54.333 [15.000, 85.000],  loss: 12.176892, mae: 8.224183, mean_q: 17.669165\n",
      "[17 33  8 97 53 85 93 10 85 66]\n",
      "  7452/10001: episode: 828, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.028, mean reward:  2.670 [-10.000,  7.062], mean action: 58.889 [8.000, 97.000],  loss: 14.487813, mae: 8.003864, mean_q: 16.923622\n",
      "[ 8 38 92 33 47 11 13 12 85 43]\n",
      "  7461/10001: episode: 829, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 36.076, mean reward:  4.008 [ 2.656,  6.884], mean action: 41.556 [11.000, 92.000],  loss: 11.726667, mae: 7.921820, mean_q: 16.574224\n",
      "[99 80 72 27 42  2 10 97 27 21]\n",
      "  7470/10001: episode: 830, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 20.954, mean reward:  2.328 [-10.000,  5.549], mean action: 42.000 [2.000, 97.000],  loss: 10.395592, mae: 8.003342, mean_q: 16.956451\n",
      "[36 82 77 33 84 44 43 85 43 32]\n",
      "  7479/10001: episode: 831, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 17.495, mean reward:  1.944 [-10.000,  5.177], mean action: 58.111 [32.000, 85.000],  loss: 16.070398, mae: 8.464232, mean_q: 17.946236\n",
      "[54 42 33 25 33 94 96 11 42 66]\n",
      "  7488/10001: episode: 832, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  8.623, mean reward:  0.958 [-10.000,  7.171], mean action: 49.111 [11.000, 96.000],  loss: 15.933590, mae: 8.065617, mean_q: 17.327816\n",
      "[33 27 73 32 75 37 35 27 66 66]\n",
      "  7497/10001: episode: 833, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  5.463, mean reward:  0.607 [-10.000,  6.225], mean action: 48.667 [27.000, 75.000],  loss: 14.442596, mae: 8.023170, mean_q: 17.291645\n",
      "[27 73 38 97 61 42 11 73 43 26]\n",
      "  7506/10001: episode: 834, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 21.178, mean reward:  2.353 [-10.000,  5.686], mean action: 51.556 [11.000, 97.000],  loss: 15.898180, mae: 7.711232, mean_q: 16.772427\n",
      "[39 33 77 11 39 93 82 25 40  1]\n",
      "  7515/10001: episode: 835, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 25.201, mean reward:  2.800 [-10.000,  6.307], mean action: 44.556 [1.000, 93.000],  loss: 14.014769, mae: 8.411561, mean_q: 17.917446\n",
      "[46 82  8 85 75 93 65 11 80 66]\n",
      "  7524/10001: episode: 836, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 39.265, mean reward:  4.363 [ 2.808,  6.862], mean action: 62.778 [8.000, 93.000],  loss: 14.042385, mae: 7.677864, mean_q: 16.381529\n",
      "[97 17 27 38 40 40 47 40  5 66]\n",
      "  7533/10001: episode: 837, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  7.060, mean reward:  0.784 [-10.000,  6.773], mean action: 35.556 [5.000, 66.000],  loss: 14.003366, mae: 7.978502, mean_q: 16.888533\n",
      "[10 16 73 78 30 10 89 12 66 35]\n",
      "  7542/10001: episode: 838, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 22.577, mean reward:  2.509 [-10.000,  6.412], mean action: 45.444 [10.000, 89.000],  loss: 13.128555, mae: 7.805371, mean_q: 16.654100\n",
      "[82 80 97 10 50 16 77 92 73 48]\n",
      "  7551/10001: episode: 839, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 34.916, mean reward:  3.880 [ 2.021,  6.617], mean action: 60.333 [10.000, 97.000],  loss: 17.364180, mae: 7.621028, mean_q: 16.361694\n",
      "[41 96 13 13 27 85 93 77 65 48]\n",
      "  7560/10001: episode: 840, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 26.051, mean reward:  2.895 [-10.000,  7.672], mean action: 57.444 [13.000, 96.000],  loss: 14.367929, mae: 7.215334, mean_q: 15.808295\n",
      "[47 92 85 89 47 78 38 10 84 48]\n",
      "  7569/10001: episode: 841, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.939, mean reward:  2.549 [-10.000,  7.708], mean action: 63.444 [10.000, 92.000],  loss: 15.222897, mae: 8.317781, mean_q: 17.490261\n",
      "[67  2 77 33 96 11 40 94 48 26]\n",
      "  7578/10001: episode: 842, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 37.585, mean reward:  4.176 [ 2.794,  6.031], mean action: 47.444 [2.000, 96.000],  loss: 12.628899, mae: 8.363591, mean_q: 17.917664\n",
      "[33  2 27 68 93 41 45 93 92 66]\n",
      "  7587/10001: episode: 843, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 22.184, mean reward:  2.465 [-10.000,  6.728], mean action: 58.556 [2.000, 93.000],  loss: 12.488119, mae: 8.524683, mean_q: 18.126163\n",
      "[43 97 27 37 61 65 43 25 50 43]\n",
      "  7596/10001: episode: 844, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  3.862, mean reward:  0.429 [-10.000,  5.239], mean action: 49.778 [25.000, 97.000],  loss: 13.762311, mae: 7.934469, mean_q: 16.945858\n",
      "[59 27 43 37 73 26 94 45 20 66]\n",
      "  7605/10001: episode: 845, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward: 35.189, mean reward:  3.910 [ 2.190,  7.010], mean action: 47.889 [20.000, 94.000],  loss: 14.442328, mae: 8.269077, mean_q: 17.724335\n",
      "[42 35  2 27 53 44 12 12 49 48]\n",
      "  7614/10001: episode: 846, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 23.396, mean reward:  2.600 [-10.000,  7.920], mean action: 31.333 [2.000, 53.000],  loss: 14.835259, mae: 7.829770, mean_q: 16.797020\n",
      "[23  2 45 78 78 26 45 49 20 92]\n",
      "  7623/10001: episode: 847, duration: 0.057s, episode steps:   9, steps per second: 158, episode reward:  8.162, mean reward:  0.907 [-10.000,  4.946], mean action: 48.333 [2.000, 92.000],  loss: 12.618661, mae: 8.204651, mean_q: 17.495550\n",
      "[40 72  6 27 50 35 77 78 20 93]\n",
      "  7632/10001: episode: 848, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 34.538, mean reward:  3.838 [ 1.971,  6.902], mean action: 50.889 [6.000, 93.000],  loss: 16.956924, mae: 8.000974, mean_q: 16.950901\n",
      "[ 5 17 50 15 75 11 50 84 12 20]\n",
      "  7641/10001: episode: 849, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 18.864, mean reward:  2.096 [-10.000,  5.233], mean action: 37.111 [11.000, 84.000],  loss: 16.640390, mae: 7.580151, mean_q: 16.213558\n",
      "[97 38 55  2  2 38 77 27 94 30]\n",
      "  7650/10001: episode: 850, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 10.547, mean reward:  1.172 [-10.000,  6.988], mean action: 40.333 [2.000, 94.000],  loss: 14.102718, mae: 7.959783, mean_q: 17.214701\n",
      "[76  2 97 55 65 65 94 12 75 48]\n",
      "  7659/10001: episode: 851, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.950, mean reward:  2.328 [-10.000,  8.088], mean action: 57.000 [2.000, 97.000],  loss: 14.398524, mae: 7.642881, mean_q: 16.743227\n",
      "[27 89 27 55 61 11 85  8 27 43]\n",
      "  7668/10001: episode: 852, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  7.455, mean reward:  0.828 [-10.000,  6.137], mean action: 45.111 [8.000, 89.000],  loss: 12.330791, mae: 7.777869, mean_q: 17.405632\n",
      "[32 37 55 15 93 11 89 12 66 48]\n",
      "  7677/10001: episode: 853, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 40.537, mean reward:  4.504 [ 2.498,  8.185], mean action: 47.333 [11.000, 93.000],  loss: 12.334187, mae: 7.581548, mean_q: 17.049507\n",
      "[75 93 27 27 82 89 77 27 43 41]\n",
      "  7686/10001: episode: 854, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  9.112, mean reward:  1.012 [-10.000,  5.191], mean action: 56.222 [27.000, 93.000],  loss: 11.496210, mae: 8.466909, mean_q: 18.547129\n",
      "[54 85 27 27 93 40 94 27 48 48]\n",
      "  7695/10001: episode: 855, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -6.601, mean reward: -0.733 [-10.000,  5.697], mean action: 54.333 [27.000, 94.000],  loss: 17.424723, mae: 8.271141, mean_q: 17.819864\n",
      "[62 11 82 33 84 85 47 10 40 11]\n",
      "  7704/10001: episode: 856, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 22.717, mean reward:  2.524 [-10.000,  6.062], mean action: 44.778 [10.000, 85.000],  loss: 14.612773, mae: 8.592760, mean_q: 18.090284\n",
      "[26  2 55 33 89 85 11 48 94  2]\n",
      "  7713/10001: episode: 857, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 20.749, mean reward:  2.305 [-10.000,  5.633], mean action: 46.556 [2.000, 94.000],  loss: 16.328951, mae: 7.637998, mean_q: 16.513844\n",
      "[94  4 55 55 96 85 89 68 85 40]\n",
      "  7722/10001: episode: 858, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  9.261, mean reward:  1.029 [-10.000,  7.542], mean action: 64.111 [4.000, 96.000],  loss: 11.963737, mae: 8.530770, mean_q: 18.054579\n",
      "[71 33 33 75 97 17 85 45 61 30]\n",
      "  7731/10001: episode: 859, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 22.816, mean reward:  2.535 [-10.000,  7.305], mean action: 52.889 [17.000, 97.000],  loss: 12.969332, mae: 7.890866, mean_q: 16.771769\n",
      "[77  2 82 43 96  8 85 97 65 53]\n",
      "  7740/10001: episode: 860, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 33.988, mean reward:  3.776 [ 3.028,  4.986], mean action: 59.000 [2.000, 97.000],  loss: 14.086876, mae: 8.186619, mean_q: 17.105082\n",
      "[11 15 25 82 75 89 43 80 20 40]\n",
      "  7749/10001: episode: 861, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 39.940, mean reward:  4.438 [ 2.606,  7.398], mean action: 52.111 [15.000, 89.000],  loss: 15.077996, mae: 8.481235, mean_q: 17.403639\n",
      "[53 11 82 33 97 44 40 73 47 39]\n",
      "  7758/10001: episode: 862, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 35.037, mean reward:  3.893 [ 2.644,  5.054], mean action: 51.778 [11.000, 97.000],  loss: 14.411412, mae: 8.046121, mean_q: 16.893236\n",
      "[77 49 97 15 89 65 40 80 40 26]\n",
      "  7767/10001: episode: 863, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 23.766, mean reward:  2.641 [-10.000,  6.404], mean action: 55.667 [15.000, 97.000],  loss: 14.146226, mae: 7.664057, mean_q: 16.482742\n",
      "[77 49 43 53 96 13 97 80 80 43]\n",
      "  7776/10001: episode: 864, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  6.429, mean reward:  0.714 [-10.000,  5.883], mean action: 61.556 [13.000, 97.000],  loss: 12.637958, mae: 8.024626, mean_q: 17.188513\n",
      "[15 49 82 93 30  2 80 27 53 94]\n",
      "  7785/10001: episode: 865, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.285, mean reward:  4.254 [ 2.805,  6.078], mean action: 56.667 [2.000, 94.000],  loss: 12.169480, mae: 8.244152, mean_q: 17.409258\n",
      "[50 84 45 33 30 15 80 85 20 13]\n",
      "  7794/10001: episode: 866, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.280, mean reward:  3.920 [ 2.759,  5.303], mean action: 45.000 [13.000, 85.000],  loss: 14.637192, mae: 8.059539, mean_q: 16.897789\n",
      "[17  4 33 93 47 11 22 89  5  5]\n",
      "  7803/10001: episode: 867, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 18.184, mean reward:  2.020 [-10.000,  4.550], mean action: 34.333 [4.000, 93.000],  loss: 14.523290, mae: 7.791962, mean_q: 16.353485\n",
      "[39 35 33 78 25 13 47 48  5 65]\n",
      "  7812/10001: episode: 868, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 35.299, mean reward:  3.922 [ 2.623,  5.949], mean action: 38.778 [5.000, 78.000],  loss: 13.643709, mae: 7.681154, mean_q: 16.179052\n",
      "[36 15 25 80 32  2 42 85 20 53]\n",
      "  7821/10001: episode: 869, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 34.809, mean reward:  3.868 [ 2.229,  6.117], mean action: 39.333 [2.000, 85.000],  loss: 12.647112, mae: 8.143403, mean_q: 16.867710\n",
      "[96 92 20 82 89 26 21 40 27 40]\n",
      "  7830/10001: episode: 870, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 24.069, mean reward:  2.674 [-10.000,  6.743], mean action: 48.556 [20.000, 92.000],  loss: 15.991516, mae: 7.967122, mean_q: 16.682739\n",
      "[61  8 25 97 47  2  2 65  5 27]\n",
      "  7839/10001: episode: 871, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 24.774, mean reward:  2.753 [-10.000,  9.044], mean action: 30.889 [2.000, 97.000],  loss: 14.037404, mae: 7.951086, mean_q: 17.141033\n",
      "[ 4  2 97 25 78 30 13 40 80 21]\n",
      "  7848/10001: episode: 872, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 35.893, mean reward:  3.988 [ 2.377,  5.953], mean action: 42.889 [2.000, 97.000],  loss: 12.520385, mae: 8.091697, mean_q: 17.937157\n",
      "[21  8 48 96 25 94 80 48 48 77]\n",
      "  7857/10001: episode: 873, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward:  6.358, mean reward:  0.706 [-10.000,  5.212], mean action: 58.222 [8.000, 96.000],  loss: 15.921227, mae: 7.789281, mean_q: 18.089834\n",
      "[79 85 48 48 73 96 82 48 48 73]\n",
      "  7866/10001: episode: 874, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -19.985, mean reward: -2.221 [-10.000,  5.004], mean action: 66.778 [48.000, 96.000],  loss: 15.262920, mae: 7.954546, mean_q: 18.478836\n",
      "[16 48 48 43 61 68 89 48 48 43]\n",
      "  7875/10001: episode: 875, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: -23.321, mean reward: -2.591 [-10.000,  5.264], mean action: 55.111 [43.000, 89.000],  loss: 13.655424, mae: 8.230828, mean_q: 18.349657\n",
      "[99 13 80 17 61 96 89 48 66 10]\n",
      "  7884/10001: episode: 876, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 38.616, mean reward:  4.291 [ 2.483,  6.385], mean action: 53.333 [10.000, 96.000],  loss: 13.701741, mae: 8.010156, mean_q: 17.550680\n",
      "[ 4 17 33 25 30 39 25 72 94 21]\n",
      "  7893/10001: episode: 877, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 17.837, mean reward:  1.982 [-10.000,  4.916], mean action: 39.556 [17.000, 94.000],  loss: 16.179424, mae: 8.213525, mean_q: 17.712477\n",
      "[ 9  4 33 33 89 26 17 94 12 40]\n",
      "  7902/10001: episode: 878, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 20.707, mean reward:  2.301 [-10.000,  7.597], mean action: 38.667 [4.000, 94.000],  loss: 15.470871, mae: 8.045020, mean_q: 16.935104\n",
      "[85 30 80 45 84 65 47 80 43 21]\n",
      "  7911/10001: episode: 879, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.065, mean reward:  2.229 [-10.000,  6.193], mean action: 55.000 [21.000, 84.000],  loss: 13.936198, mae: 7.815894, mean_q: 16.672188\n",
      "[69 30 33 97 30 15 92 73 20 78]\n",
      "  7920/10001: episode: 880, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 19.895, mean reward:  2.211 [-10.000,  4.847], mean action: 52.000 [15.000, 97.000],  loss: 12.378352, mae: 7.923635, mean_q: 16.836388\n",
      "[51 30 38 96 30 75 33 20 75 21]\n",
      "  7929/10001: episode: 881, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  7.528, mean reward:  0.836 [-10.000,  6.855], mean action: 46.444 [20.000, 96.000],  loss: 13.877479, mae: 7.959564, mean_q: 16.751120\n",
      "[28 11 97 13 37 37 68 97 12 92]\n",
      "  7938/10001: episode: 882, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  4.139, mean reward:  0.460 [-10.000,  5.127], mean action: 51.556 [11.000, 97.000],  loss: 14.968192, mae: 8.079128, mean_q: 16.910061\n",
      "[25 89 38 38 40 11 40 66 11 41]\n",
      "  7947/10001: episode: 883, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -5.091, mean reward: -0.566 [-10.000,  5.106], mean action: 41.556 [11.000, 89.000],  loss: 13.077709, mae: 7.597307, mean_q: 16.437279\n",
      "[80 92 61 15 39 17 94 26 66 58]\n",
      "  7956/10001: episode: 884, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 34.079, mean reward:  3.787 [ 2.234,  6.922], mean action: 52.000 [15.000, 94.000],  loss: 16.124361, mae: 7.812529, mean_q: 16.873650\n",
      "[72 82 73 85 82 45 47 12 73 35]\n",
      "  7965/10001: episode: 885, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  4.851, mean reward:  0.539 [-10.000,  4.145], mean action: 59.333 [12.000, 85.000],  loss: 13.359534, mae: 7.830660, mean_q: 16.752235\n",
      "[62  2 97 33 40  6 85 25 80 12]\n",
      "  7974/10001: episode: 886, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 37.726, mean reward:  4.192 [ 2.439,  5.714], mean action: 42.222 [2.000, 97.000],  loss: 15.727875, mae: 8.059620, mean_q: 16.836269\n",
      "[87 40 27 43 75 50 89 66 66 43]\n",
      "  7983/10001: episode: 887, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  5.612, mean reward:  0.624 [-10.000,  5.213], mean action: 55.444 [27.000, 89.000],  loss: 14.246294, mae: 7.774160, mean_q: 16.384422\n",
      "[30 45 50 77 97 65 77 27 66 94]\n",
      "  7992/10001: episode: 888, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 21.775, mean reward:  2.419 [-10.000,  6.955], mean action: 66.444 [27.000, 97.000],  loss: 11.151237, mae: 8.634216, mean_q: 17.949434\n",
      "[99 49 77 27 38 39 77 40 65 41]\n",
      "  8001/10001: episode: 889, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 23.379, mean reward:  2.598 [-10.000,  6.516], mean action: 50.333 [27.000, 77.000],  loss: 12.813897, mae: 8.664972, mean_q: 17.835657\n",
      "[22  4 38 94 92 26 26 11 11 13]\n",
      "  8010/10001: episode: 890, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  5.481, mean reward:  0.609 [-10.000,  4.934], mean action: 35.000 [4.000, 94.000],  loss: 12.519611, mae: 7.795649, mean_q: 16.188913\n",
      "[ 4 11 55 43 10 26 94 66 37 35]\n",
      "  8019/10001: episode: 891, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 34.072, mean reward:  3.786 [ 2.128,  5.631], mean action: 41.889 [10.000, 94.000],  loss: 14.247358, mae: 8.281968, mean_q: 17.446980\n",
      "[ 6  4 82 33 77 11 39 85 66 48]\n",
      "  8028/10001: episode: 892, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.609, mean reward:  4.290 [ 2.905,  6.455], mean action: 49.444 [4.000, 85.000],  loss: 13.084536, mae: 7.891375, mean_q: 16.934170\n",
      "[35 38 77 65 82 30 11 66 66 21]\n",
      "  8037/10001: episode: 893, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.907, mean reward:  2.545 [-10.000,  5.537], mean action: 50.667 [11.000, 82.000],  loss: 11.592327, mae: 8.337201, mean_q: 17.705851\n",
      "[15 11 10 80 96 26 26 66 94 55]\n",
      "  8046/10001: episode: 894, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 20.379, mean reward:  2.264 [-10.000,  5.561], mean action: 51.556 [10.000, 96.000],  loss: 14.367713, mae: 7.774786, mean_q: 16.717400\n",
      "[ 6 11 80 65 75 26  8 80 61 43]\n",
      "  8055/10001: episode: 895, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward: 21.235, mean reward:  2.359 [-10.000,  6.935], mean action: 49.889 [8.000, 80.000],  loss: 15.388470, mae: 7.748873, mean_q: 16.348686\n",
      "[ 4  8 33 55 53 26 65 26 47 21]\n",
      "  8064/10001: episode: 896, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 18.071, mean reward:  2.008 [-10.000,  4.608], mean action: 37.111 [8.000, 65.000],  loss: 12.222892, mae: 8.415665, mean_q: 17.352642\n",
      "[42 37 55 45 78  4 40 65 12 75]\n",
      "  8073/10001: episode: 897, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 35.380, mean reward:  3.931 [ 2.477,  5.426], mean action: 45.667 [4.000, 78.000],  loss: 15.232008, mae: 8.162576, mean_q: 17.139290\n",
      "[11 39 43 94 96 89 43 12 89 58]\n",
      "  8082/10001: episode: 898, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward:  3.274, mean reward:  0.364 [-10.000,  4.233], mean action: 62.556 [12.000, 96.000],  loss: 16.505333, mae: 8.544436, mean_q: 17.632725\n",
      "[47 44 33 33 47 13 55 47 75 21]\n",
      "  8091/10001: episode: 899, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: -7.837, mean reward: -0.871 [-10.000,  4.869], mean action: 40.889 [13.000, 75.000],  loss: 15.238615, mae: 8.033985, mean_q: 16.505901\n",
      "[98  2 75 17 33 13 65 20 75 21]\n",
      "  8100/10001: episode: 900, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 19.155, mean reward:  2.128 [-10.000,  5.630], mean action: 35.667 [2.000, 75.000],  loss: 13.346462, mae: 7.877849, mean_q: 16.713608\n",
      "[ 4 17 16 25 25 80 84 20 92 58]\n",
      "  8109/10001: episode: 901, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 18.456, mean reward:  2.051 [-10.000,  4.834], mean action: 46.333 [16.000, 92.000],  loss: 17.113161, mae: 8.057196, mean_q: 17.265129\n",
      "[73 94 92 43 78 45 78  8 66 41]\n",
      "  8118/10001: episode: 902, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.803, mean reward:  2.645 [-10.000,  6.547], mean action: 60.556 [8.000, 94.000],  loss: 11.718633, mae: 8.540413, mean_q: 18.347147\n",
      "[85  8 82 55 96 89 77 66 97 39]\n",
      "  8127/10001: episode: 903, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 36.517, mean reward:  4.057 [ 2.273,  5.547], mean action: 67.667 [8.000, 97.000],  loss: 15.749536, mae: 7.931413, mean_q: 16.790606\n",
      "[19  0 43 39 82 45 26 40 43 10]\n",
      "  8136/10001: episode: 904, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.480, mean reward:  2.720 [-10.000,  5.883], mean action: 36.444 [0.000, 82.000],  loss: 14.164982, mae: 8.108994, mean_q: 17.091274\n",
      "[ 7 96 45  2 38 85 80 11 66 48]\n",
      "  8145/10001: episode: 905, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 42.636, mean reward:  4.737 [ 3.457,  7.050], mean action: 52.333 [2.000, 96.000],  loss: 13.969197, mae: 7.967282, mean_q: 16.650187\n",
      "[75  8 78 45 96 42 26 85 92 26]\n",
      "  8154/10001: episode: 906, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 19.769, mean reward:  2.197 [-10.000,  4.798], mean action: 55.333 [8.000, 96.000],  loss: 15.850108, mae: 8.168413, mean_q: 17.149632\n",
      "[81 39 97 16 73 94 77 11 72 75]\n",
      "  8163/10001: episode: 907, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 34.970, mean reward:  3.886 [ 2.713,  5.274], mean action: 61.556 [11.000, 97.000],  loss: 12.830522, mae: 8.294773, mean_q: 17.382185\n",
      "[10 13 97 33 30 13 97 11 10 85]\n",
      "  8172/10001: episode: 908, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: -4.350, mean reward: -0.483 [-10.000,  5.229], mean action: 43.222 [10.000, 97.000],  loss: 16.787931, mae: 8.564409, mean_q: 17.973442\n",
      "[48 11 39 33 78 93 84 66 21 32]\n",
      "  8181/10001: episode: 909, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 32.972, mean reward:  3.664 [ 2.162,  5.838], mean action: 50.778 [11.000, 93.000],  loss: 17.612825, mae: 7.138150, mean_q: 15.257745\n",
      "[81 10 75 17 96 42 65 61 85 41]\n",
      "  8190/10001: episode: 910, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 34.095, mean reward:  3.788 [ 2.847,  6.149], mean action: 54.667 [10.000, 96.000],  loss: 14.198064, mae: 7.801733, mean_q: 16.350512\n",
      "[ 1 42 39 33 84 17 77 10 66 74]\n",
      "  8199/10001: episode: 911, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 39.663, mean reward:  4.407 [ 2.694,  6.813], mean action: 49.111 [10.000, 84.000],  loss: 12.624935, mae: 7.988123, mean_q: 16.786491\n",
      "[39 73 92 78 25 33 93 41 30 66]\n",
      "  8208/10001: episode: 912, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 41.430, mean reward:  4.603 [ 3.014,  6.574], mean action: 59.000 [25.000, 93.000],  loss: 13.687201, mae: 8.090285, mean_q: 17.034065\n",
      "[89 85 94 85 25 35 84 66 89 48]\n",
      "  8217/10001: episode: 913, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  9.605, mean reward:  1.067 [-10.000,  7.467], mean action: 67.889 [25.000, 94.000],  loss: 13.925385, mae: 8.503660, mean_q: 17.953362\n",
      "[45 85 11 82 82 17 30 66 43 26]\n",
      "  8226/10001: episode: 914, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.307, mean reward:  2.701 [-10.000,  5.182], mean action: 49.111 [11.000, 85.000],  loss: 15.527636, mae: 7.854183, mean_q: 16.767702\n",
      "[78 30 17 78 80 85 44 66 66 20]\n",
      "  8235/10001: episode: 915, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  8.298, mean reward:  0.922 [-10.000,  5.962], mean action: 54.000 [17.000, 85.000],  loss: 14.400701, mae: 7.507077, mean_q: 16.224586\n",
      "[41 39 66 89 42 35 66 61 77 10]\n",
      "  8244/10001: episode: 916, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 20.613, mean reward:  2.290 [-10.000,  6.007], mean action: 53.889 [10.000, 89.000],  loss: 16.388189, mae: 7.875160, mean_q: 17.086338\n",
      "[29 92 66 82 61 89 77 78 96 17]\n",
      "  8253/10001: episode: 917, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 33.302, mean reward:  3.700 [ 2.085,  4.551], mean action: 73.111 [17.000, 96.000],  loss: 12.058095, mae: 7.983616, mean_q: 17.519108\n",
      "[39 72 85 33 40 66 65 66 66 21]\n",
      "  8262/10001: episode: 918, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  7.017, mean reward:  0.780 [-10.000,  5.127], mean action: 57.111 [21.000, 85.000],  loss: 14.105792, mae: 7.782610, mean_q: 17.441414\n",
      "[24 85 77 45 40 85 65 11 66  5]\n",
      "  8271/10001: episode: 919, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.258, mean reward:  2.473 [-10.000,  6.314], mean action: 53.222 [5.000, 85.000],  loss: 15.035625, mae: 8.016872, mean_q: 17.748768\n",
      "[19  8 66 37 17 45 35 66 22 77]\n",
      "  8280/10001: episode: 920, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.462, mean reward:  2.274 [-10.000,  5.374], mean action: 41.444 [8.000, 77.000],  loss: 15.993761, mae: 8.100631, mean_q: 17.375050\n",
      "[98  0 40 82 75 11 40 49 89 43]\n",
      "  8289/10001: episode: 921, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 24.380, mean reward:  2.709 [-10.000,  6.883], mean action: 47.667 [0.000, 89.000],  loss: 12.347114, mae: 8.129832, mean_q: 17.055473\n",
      "[85 89 43 45 96 92 93 20 85  2]\n",
      "  8298/10001: episode: 922, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.030, mean reward:  2.670 [-10.000,  8.718], mean action: 62.778 [2.000, 96.000],  loss: 12.556705, mae: 7.896150, mean_q: 16.471992\n",
      "[74 35 94 82 78 80 89 66 66 10]\n",
      "  8307/10001: episode: 923, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 22.842, mean reward:  2.538 [-10.000,  5.734], mean action: 66.667 [10.000, 94.000],  loss: 13.982624, mae: 7.823809, mean_q: 16.133741\n",
      "[98 43 33 37 53 17 65 30 11 10]\n",
      "  8316/10001: episode: 924, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 38.023, mean reward:  4.225 [ 2.874,  6.657], mean action: 33.222 [10.000, 65.000],  loss: 17.045631, mae: 7.693209, mean_q: 16.058676\n",
      "[84 35 33 35 97 89 32 66 89 43]\n",
      "  8325/10001: episode: 925, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  9.683, mean reward:  1.076 [-10.000,  6.329], mean action: 57.667 [32.000, 97.000],  loss: 17.636593, mae: 7.558529, mean_q: 15.875545\n",
      "[64 89 84 39 27 89 89 72 20 75]\n",
      "  8334/10001: episode: 926, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  5.710, mean reward:  0.634 [-10.000,  4.958], mean action: 64.889 [20.000, 89.000],  loss: 11.601247, mae: 7.766048, mean_q: 16.411709\n",
      "[90 27 85 50 97 11 13 66 43 77]\n",
      "  8343/10001: episode: 927, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 38.209, mean reward:  4.245 [ 1.947,  6.023], mean action: 52.111 [11.000, 97.000],  loss: 15.220901, mae: 8.432542, mean_q: 17.792652\n",
      "[18  2 84 43 38 85 50 97 21 47]\n",
      "  8352/10001: episode: 928, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 33.808, mean reward:  3.756 [ 2.387,  5.370], mean action: 51.889 [2.000, 97.000],  loss: 14.736442, mae: 7.625491, mean_q: 16.266739\n",
      "[ 9  0 97 33 94 11 11 11 39 13]\n",
      "  8361/10001: episode: 929, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  6.481, mean reward:  0.720 [-10.000,  5.531], mean action: 34.333 [0.000, 97.000],  loss: 13.140997, mae: 8.669190, mean_q: 18.267960\n",
      "[ 7  4 38 65 96 11 42 85 11 13]\n",
      "  8370/10001: episode: 930, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 19.816, mean reward:  2.202 [-10.000,  5.234], mean action: 40.556 [4.000, 96.000],  loss: 14.394023, mae: 8.088325, mean_q: 16.723120\n",
      "[86 96 92  0  6 85 55 73 89 21]\n",
      "  8379/10001: episode: 931, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 36.986, mean reward:  4.110 [ 2.048,  5.638], mean action: 57.444 [0.000, 96.000],  loss: 12.051440, mae: 7.914966, mean_q: 16.450636\n",
      "[ 9 13 20  0 33 40 43 41 65 10]\n",
      "  8388/10001: episode: 932, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 39.227, mean reward:  4.359 [ 2.350,  6.292], mean action: 29.444 [0.000, 65.000],  loss: 13.479990, mae: 8.176880, mean_q: 16.736862\n",
      "[10  6 84 33 20  0 85 21 66 40]\n",
      "  8397/10001: episode: 933, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.002, mean reward:  4.222 [ 2.252,  7.207], mean action: 39.444 [0.000, 85.000],  loss: 15.023644, mae: 7.708924, mean_q: 15.789561\n",
      "[59 39 66 97 97 13 37 21 66 55]\n",
      "  8406/10001: episode: 934, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  2.311, mean reward:  0.257 [-10.000,  3.777], mean action: 54.556 [13.000, 97.000],  loss: 13.552466, mae: 8.259499, mean_q: 16.806358\n",
      "[87  0 73 77 96  8 22 85 78 74]\n",
      "  8415/10001: episode: 935, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 35.232, mean reward:  3.915 [ 1.936,  6.363], mean action: 57.000 [0.000, 96.000],  loss: 14.280075, mae: 8.654568, mean_q: 17.363420\n",
      "[37 15 33 77 97 35 13 85 20 89]\n",
      "  8424/10001: episode: 936, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.348, mean reward:  4.039 [ 2.582,  6.931], mean action: 51.556 [13.000, 97.000],  loss: 13.348042, mae: 8.058348, mean_q: 16.286654\n",
      "[12  0  5 61 97 30 30 92 43  2]\n",
      "  8433/10001: episode: 937, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 26.730, mean reward:  2.970 [-10.000,  9.140], mean action: 40.000 [0.000, 97.000],  loss: 13.965398, mae: 8.383229, mean_q: 17.011528\n",
      "[37  2 84 82 73 30 13 85 73 74]\n",
      "  8442/10001: episode: 938, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 22.862, mean reward:  2.540 [-10.000,  6.207], mean action: 57.333 [2.000, 85.000],  loss: 16.195518, mae: 8.507954, mean_q: 17.335171\n",
      "[33 30  6 16 97 10 80 66 11 77]\n",
      "  8451/10001: episode: 939, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 36.398, mean reward:  4.044 [ 2.327,  5.447], mean action: 43.667 [6.000, 97.000],  loss: 15.808041, mae: 7.695078, mean_q: 15.582496\n",
      "[32  2 73 25 25 82 84 12 66 65]\n",
      "  8460/10001: episode: 940, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.987, mean reward:  2.332 [-10.000,  6.138], mean action: 48.222 [2.000, 84.000],  loss: 14.990191, mae: 8.289715, mean_q: 16.757095\n",
      "[89  2 92 15 63 93 93 53 89 17]\n",
      "  8469/10001: episode: 941, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  3.189, mean reward:  0.354 [-10.000,  4.191], mean action: 57.444 [2.000, 93.000],  loss: 14.599125, mae: 8.112873, mean_q: 16.537600\n",
      "[92 22 15 66 78 26 26 35 12 54]\n",
      "  8478/10001: episode: 942, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 21.412, mean reward:  2.379 [-10.000,  7.270], mean action: 37.111 [12.000, 78.000],  loss: 15.643426, mae: 7.470873, mean_q: 15.659466\n",
      "[31 63 43 16 35 94 89 78 73 35]\n",
      "  8487/10001: episode: 943, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.852, mean reward:  2.317 [-10.000,  6.385], mean action: 58.444 [16.000, 94.000],  loss: 14.039405, mae: 8.091179, mean_q: 16.570564\n",
      "[ 9  6 66 97 35 45 65 10 11  3]\n",
      "  8496/10001: episode: 944, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 32.226, mean reward:  3.581 [ 0.427,  5.405], mean action: 37.556 [3.000, 97.000],  loss: 12.667469, mae: 7.850651, mean_q: 16.446795\n",
      "[20 11 38 66 38 89 39 78 11 42]\n",
      "  8505/10001: episode: 945, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  6.052, mean reward:  0.672 [-10.000,  4.095], mean action: 45.778 [11.000, 89.000],  loss: 14.059883, mae: 8.020875, mean_q: 16.955685\n",
      "[35 65 39 50 65 39 92 45 73 10]\n",
      "  8514/10001: episode: 946, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 10.018, mean reward:  1.113 [-10.000,  6.306], mean action: 53.111 [10.000, 92.000],  loss: 16.089277, mae: 7.527929, mean_q: 15.682068\n",
      "[59 30 66 92 97 26 97 11 65 41]\n",
      "  8523/10001: episode: 947, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.079, mean reward:  2.231 [-10.000,  5.379], mean action: 58.333 [11.000, 97.000],  loss: 12.852702, mae: 7.930296, mean_q: 16.433912\n",
      "[51  8 15 96 40 40 39 11 20 10]\n",
      "  8532/10001: episode: 948, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 22.062, mean reward:  2.451 [-10.000,  5.705], mean action: 31.000 [8.000, 96.000],  loss: 12.763536, mae: 8.204052, mean_q: 16.874294\n",
      "[43 20 25 73 40 85 93 10 66 84]\n",
      "  8541/10001: episode: 949, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 35.894, mean reward:  3.988 [ 2.233,  6.082], mean action: 55.111 [10.000, 93.000],  loss: 13.499714, mae: 7.689602, mean_q: 16.047642\n",
      "[58  6 97 33 40 45 35 85 47 40]\n",
      "  8550/10001: episode: 950, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 20.159, mean reward:  2.240 [-10.000,  4.887], mean action: 47.556 [6.000, 97.000],  loss: 12.119395, mae: 8.180840, mean_q: 17.344687\n",
      "[59  4 96 93 53 89 77 73 66 66]\n",
      "  8559/10001: episode: 951, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.121, mean reward:  2.347 [-10.000,  5.641], mean action: 68.556 [4.000, 96.000],  loss: 18.395037, mae: 7.591669, mean_q: 16.196907\n",
      "[70  6 80 61 16 47  8 82 10 35]\n",
      "  8568/10001: episode: 952, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 35.591, mean reward:  3.955 [ 2.480,  5.999], mean action: 38.333 [6.000, 82.000],  loss: 14.057487, mae: 7.615928, mean_q: 15.822769\n",
      "[59  8  2 33 93 20  5 75 30 68]\n",
      "  8577/10001: episode: 953, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 32.034, mean reward:  3.559 [ 1.562,  5.996], mean action: 37.111 [2.000, 93.000],  loss: 14.862976, mae: 7.799384, mean_q: 16.218182\n",
      "[72 20 33 33 97 85 53 78 94 21]\n",
      "  8586/10001: episode: 954, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 17.614, mean reward:  1.957 [-10.000,  4.760], mean action: 57.111 [20.000, 97.000],  loss: 15.993953, mae: 7.663622, mean_q: 15.833644\n",
      "[46 45 33 68 30 11 45 21 30 41]\n",
      "  8595/10001: episode: 955, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  7.458, mean reward:  0.829 [-10.000,  5.654], mean action: 36.000 [11.000, 68.000],  loss: 14.436298, mae: 8.385084, mean_q: 17.142399\n",
      "[98 65 77 17 10 11 47 73 92 72]\n",
      "  8604/10001: episode: 956, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 35.106, mean reward:  3.901 [ 2.354,  5.564], mean action: 51.556 [10.000, 92.000],  loss: 12.585643, mae: 8.470650, mean_q: 17.039789\n",
      "[49 20 85 66  4 65 72 53 75 10]\n",
      "  8613/10001: episode: 957, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 35.298, mean reward:  3.922 [ 2.873,  6.011], mean action: 50.000 [4.000, 85.000],  loss: 14.885503, mae: 8.565087, mean_q: 17.492720\n",
      "[ 7 33 20 20 84 26 20 80 21 93]\n",
      "  8622/10001: episode: 958, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 10.381, mean reward:  1.153 [-10.000,  6.673], mean action: 44.111 [20.000, 93.000],  loss: 12.247190, mae: 8.276544, mean_q: 17.691360\n",
      "[34 20 13 27 73 22 80 11 66 48]\n",
      "  8631/10001: episode: 959, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 42.179, mean reward:  4.687 [ 2.769,  7.232], mean action: 40.000 [11.000, 80.000],  loss: 13.059625, mae: 8.346632, mean_q: 17.880898\n",
      "[ 7 20 20 97 66 26 26 21  5 80]\n",
      "  8640/10001: episode: 960, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 10.787, mean reward:  1.199 [-10.000,  8.088], mean action: 40.111 [5.000, 97.000],  loss: 14.232161, mae: 8.601753, mean_q: 18.490578\n",
      "[87 37 65 82 13 20 26 85 21  5]\n",
      "  8649/10001: episode: 961, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 32.119, mean reward:  3.569 [ 2.194,  4.557], mean action: 39.333 [5.000, 85.000],  loss: 11.289062, mae: 8.333843, mean_q: 17.832558\n",
      "[84  2 39 66 93 26 26 66 92 48]\n",
      "  8658/10001: episode: 962, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.138, mean reward:  1.015 [-10.000,  7.011], mean action: 50.889 [2.000, 93.000],  loss: 18.165588, mae: 8.201070, mean_q: 17.285681\n",
      "[26  4 82 43 27 10 45 68 65  8]\n",
      "  8667/10001: episode: 963, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 34.366, mean reward:  3.818 [ 1.564,  5.944], mean action: 39.111 [4.000, 82.000],  loss: 15.032294, mae: 7.622104, mean_q: 16.023663\n",
      "[ 5 45 97 33 73  5 45 10 65 48]\n",
      "  8676/10001: episode: 964, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 12.385, mean reward:  1.376 [-10.000,  7.605], mean action: 46.778 [5.000, 97.000],  loss: 15.856096, mae: 7.780223, mean_q: 16.298525\n",
      "[64  4 49 78 97 11  5 41 65 39]\n",
      "  8685/10001: episode: 965, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 34.906, mean reward:  3.878 [ 2.569,  5.468], mean action: 43.222 [4.000, 97.000],  loss: 11.899486, mae: 8.436963, mean_q: 17.552492\n",
      "[50 17 84 55 13 75 92 10 61 65]\n",
      "  8694/10001: episode: 966, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 32.640, mean reward:  3.627 [ 2.497,  5.235], mean action: 52.444 [10.000, 92.000],  loss: 13.423486, mae: 8.489443, mean_q: 17.723940\n",
      "[94 11 25 73 38 11 26 78 30 30]\n",
      "  8703/10001: episode: 967, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.653, mean reward:  0.961 [-10.000,  6.189], mean action: 35.778 [11.000, 78.000],  loss: 14.071215, mae: 8.065531, mean_q: 16.705513\n",
      "[95  4 15 66 47 11 38 61 92 30]\n",
      "  8712/10001: episode: 968, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward: 35.643, mean reward:  3.960 [ 2.429,  7.166], mean action: 40.444 [4.000, 92.000],  loss: 15.806512, mae: 8.194635, mean_q: 16.712036\n",
      "[87 42 97 97 77 17 11 78 40 85]\n",
      "  8721/10001: episode: 969, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 23.692, mean reward:  2.632 [-10.000,  6.573], mean action: 60.444 [11.000, 97.000],  loss: 10.112395, mae: 8.825700, mean_q: 18.193861\n",
      "[93  4 73 38 32 30  5 94 65 19]\n",
      "  8730/10001: episode: 970, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward: 29.437, mean reward:  3.271 [ 1.860,  4.802], mean action: 40.000 [4.000, 94.000],  loss: 12.246835, mae: 8.814136, mean_q: 18.305861\n",
      "[39 11  2 20 40 11 94 85 26 65]\n",
      "  8739/10001: episode: 971, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 21.730, mean reward:  2.414 [-10.000,  5.151], mean action: 39.333 [2.000, 94.000],  loss: 14.641720, mae: 7.993803, mean_q: 16.854574\n",
      "[18  4 92 39 84 26 17 66 85 35]\n",
      "  8748/10001: episode: 972, duration: 0.068s, episode steps:   9, steps per second: 133, episode reward: 36.055, mean reward:  4.006 [ 2.719,  5.811], mean action: 49.778 [4.000, 92.000],  loss: 11.977800, mae: 8.410945, mean_q: 17.087685\n",
      "[ 0 75 47 75 97 26 13 85 72  8]\n",
      "  8757/10001: episode: 973, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 22.261, mean reward:  2.473 [-10.000,  7.069], mean action: 55.333 [8.000, 97.000],  loss: 16.630516, mae: 7.885815, mean_q: 16.101089\n",
      "[18  4 77 93 40 26 35 66 37 48]\n",
      "  8766/10001: episode: 974, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.167, mean reward:  4.352 [ 3.004,  7.066], mean action: 47.333 [4.000, 93.000],  loss: 12.776684, mae: 8.123011, mean_q: 16.819073\n",
      "[37 40 11 93 66 25 42 73 78 68]\n",
      "  8775/10001: episode: 975, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 33.003, mean reward:  3.667 [ 1.691,  5.163], mean action: 55.111 [11.000, 93.000],  loss: 18.501366, mae: 8.422045, mean_q: 17.065876\n",
      "[26 13 77 66 47 11 11 85 82 62]\n",
      "  8784/10001: episode: 976, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 22.450, mean reward:  2.494 [-10.000,  6.945], mean action: 50.444 [11.000, 85.000],  loss: 15.597983, mae: 8.398625, mean_q: 17.515205\n",
      "[98 38 11 97 42 11 38 10 17 66]\n",
      "  8793/10001: episode: 977, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  9.968, mean reward:  1.108 [-10.000,  7.486], mean action: 36.667 [10.000, 97.000],  loss: 13.431583, mae: 8.439054, mean_q: 17.851479\n",
      "[ 3 45 37 11 84 17 93 11 89  3]\n",
      "  8802/10001: episode: 978, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward:  7.039, mean reward:  0.782 [-10.000,  5.178], mean action: 43.333 [3.000, 93.000],  loss: 19.896172, mae: 8.118094, mean_q: 17.125069\n",
      "[52 15 73 77 82 80 47 10 47 58]\n",
      "  8811/10001: episode: 979, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 20.130, mean reward:  2.237 [-10.000,  4.674], mean action: 54.333 [10.000, 82.000],  loss: 19.324902, mae: 7.677174, mean_q: 15.600786\n",
      "[ 3 40 39 82 80 11 11 49 77 66]\n",
      "  8820/10001: episode: 980, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward: 25.475, mean reward:  2.831 [-10.000,  7.502], mean action: 50.556 [11.000, 82.000],  loss: 14.623055, mae: 7.747318, mean_q: 15.812878\n",
      "[92 78 84 73 97 89 50 10 21 75]\n",
      "  8829/10001: episode: 981, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 34.297, mean reward:  3.811 [ 2.389,  5.336], mean action: 64.111 [10.000, 97.000],  loss: 14.533358, mae: 8.118003, mean_q: 16.189270\n",
      "[91  4 84 15 40 27 77 26 94 74]\n",
      "  8838/10001: episode: 982, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 35.959, mean reward:  3.995 [ 2.511,  6.134], mean action: 49.000 [4.000, 94.000],  loss: 14.107876, mae: 7.818752, mean_q: 15.652289\n",
      "[59 27 20 75 75 50 89 10 48 89]\n",
      "  8847/10001: episode: 983, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  7.526, mean reward:  0.836 [-10.000,  6.403], mean action: 53.667 [10.000, 89.000],  loss: 11.841346, mae: 8.379262, mean_q: 17.122160\n",
      "[28 89 97 50 75 89 40 11 43 48]\n",
      "  8856/10001: episode: 984, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 25.980, mean reward:  2.887 [-10.000,  7.643], mean action: 60.222 [11.000, 97.000],  loss: 12.144851, mae: 8.492675, mean_q: 17.232262\n",
      "[72 17 77 66 17  4 48 10 89  2]\n",
      "  8865/10001: episode: 985, duration: 0.064s, episode steps:   9, steps per second: 140, episode reward: 28.862, mean reward:  3.207 [-10.000,  8.615], mean action: 36.667 [2.000, 89.000],  loss: 16.223351, mae: 8.372132, mean_q: 17.187170\n",
      "[90 17 48 85 17 17 40 10 89  2]\n",
      "  8874/10001: episode: 986, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 14.548, mean reward:  1.616 [-10.000,  8.605], mean action: 36.111 [2.000, 89.000],  loss: 13.736084, mae: 7.975563, mean_q: 16.346809\n",
      "[29 20 40 40 96 11 40 92 77 30]\n",
      "  8883/10001: episode: 987, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  8.361, mean reward:  0.929 [-10.000,  6.793], mean action: 49.556 [11.000, 96.000],  loss: 14.971905, mae: 8.112710, mean_q: 16.361933\n",
      "[56 20 73 73 93 45 77 20 12 97]\n",
      "  8892/10001: episode: 988, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward:  8.601, mean reward:  0.956 [-10.000,  6.495], mean action: 56.667 [12.000, 97.000],  loss: 12.977612, mae: 8.383470, mean_q: 17.103580\n",
      "[30 20 20 25 66 30 30 65 61 74]\n",
      "  8901/10001: episode: 989, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: -5.788, mean reward: -0.643 [-10.000,  6.842], mean action: 43.444 [20.000, 74.000],  loss: 16.295267, mae: 7.720324, mean_q: 16.240633\n",
      "[88  4 55 40 37 26 89 10 85  2]\n",
      "  8910/10001: episode: 990, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: 40.021, mean reward:  4.447 [ 2.158,  8.797], mean action: 38.667 [2.000, 89.000],  loss: 14.954127, mae: 8.193993, mean_q: 17.149559\n",
      "[66  2 43 84 15 30 11 77 96 74]\n",
      "  8919/10001: episode: 991, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 35.881, mean reward:  3.987 [ 2.379,  6.093], mean action: 48.000 [2.000, 96.000],  loss: 17.464182, mae: 8.139202, mean_q: 16.627285\n",
      "[32 30 55 13 25 39 84 45 84  2]\n",
      "  8928/10001: episode: 992, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 25.420, mean reward:  2.824 [-10.000,  9.825], mean action: 41.889 [2.000, 84.000],  loss: 13.636555, mae: 7.960508, mean_q: 16.237631\n",
      "[71 11 25 40 78 30 30 30 97 30]\n",
      "  8937/10001: episode: 993, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -5.493, mean reward: -0.610 [-10.000,  5.046], mean action: 41.222 [11.000, 97.000],  loss: 15.116857, mae: 8.296163, mean_q: 16.842916\n",
      "[46 25 38 44 85 85 17 40 41 40]\n",
      "  8946/10001: episode: 994, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  7.629, mean reward:  0.848 [-10.000,  5.874], mean action: 46.111 [17.000, 85.000],  loss: 16.723724, mae: 8.344463, mean_q: 16.803566\n",
      "[77 44 40 43 11 65 65 97 97  8]\n",
      "  8955/10001: episode: 995, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward:  8.188, mean reward:  0.910 [-10.000,  6.739], mean action: 52.222 [8.000, 97.000],  loss: 17.176222, mae: 8.111606, mean_q: 16.067396\n",
      "[83  0 66 19 19 13 65 50 96 74]\n",
      "  8964/10001: episode: 996, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 20.973, mean reward:  2.330 [-10.000,  7.377], mean action: 44.667 [0.000, 96.000],  loss: 14.762711, mae: 7.561774, mean_q: 15.409822\n",
      "[53  0 66 19 40 53 66 10 94 74]\n",
      "  8973/10001: episode: 997, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward:  7.883, mean reward:  0.876 [-10.000,  6.665], mean action: 46.889 [0.000, 94.000],  loss: 15.575555, mae: 7.510745, mean_q: 15.351803\n",
      "[74 38 75 92 33 33 93 10 10 10]\n",
      "  8982/10001: episode: 998, duration: 0.059s, episode steps:   9, steps per second: 154, episode reward: -6.915, mean reward: -0.768 [-10.000,  5.497], mean action: 43.778 [10.000, 93.000],  loss: 17.096903, mae: 8.132895, mean_q: 16.821213\n",
      "[ 2 20 66 45 66 96 61  2 26 55]\n",
      "  8991/10001: episode: 999, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  3.296, mean reward:  0.366 [-10.000,  4.569], mean action: 48.556 [2.000, 96.000],  loss: 12.712746, mae: 7.924121, mean_q: 16.419220\n",
      "[97  2 80 19 78 25 13 66 97  2]\n",
      "  9000/10001: episode: 1000, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  5.778, mean reward:  0.642 [-10.000,  6.170], mean action: 42.444 [2.000, 97.000],  loss: 13.515110, mae: 8.488111, mean_q: 16.544136\n",
      "[70 49 27  2 33 89 47 89 20 77]\n",
      "  9009/10001: episode: 1001, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward: 23.952, mean reward:  2.661 [-10.000,  5.746], mean action: 48.111 [2.000, 89.000],  loss: 13.710659, mae: 8.181984, mean_q: 15.879144\n",
      "[20 65 94 25 38 17 35 10 85  8]\n",
      "  9018/10001: episode: 1002, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.784, mean reward:  4.087 [ 2.213,  6.707], mean action: 41.889 [8.000, 94.000],  loss: 15.502773, mae: 8.658334, mean_q: 16.979811\n",
      "[55 27 61 25 92 80 80 10 84 74]\n",
      "  9027/10001: episode: 1003, duration: 0.063s, episode steps:   9, steps per second: 143, episode reward: 21.565, mean reward:  2.396 [-10.000,  5.885], mean action: 59.222 [10.000, 92.000],  loss: 11.121812, mae: 8.634506, mean_q: 17.129335\n",
      "[30 42 47 45 27 15 96 25 85 35]\n",
      "  9036/10001: episode: 1004, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 36.211, mean reward:  4.023 [ 2.595,  5.385], mean action: 46.333 [15.000, 96.000],  loss: 14.146520, mae: 8.220048, mean_q: 16.449461\n",
      "[ 3 49 72 16 21 65 47 11 21 66]\n",
      "  9045/10001: episode: 1005, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.125, mean reward:  2.681 [-10.000,  8.685], mean action: 40.889 [11.000, 72.000],  loss: 15.268459, mae: 7.793256, mean_q: 15.873790\n",
      "[65  2 66 19 19 39 68  2 20 40]\n",
      "  9054/10001: episode: 1006, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  8.431, mean reward:  0.937 [-10.000,  8.283], mean action: 30.556 [2.000, 68.000],  loss: 12.114094, mae: 8.259685, mean_q: 16.994362\n",
      "[88 19 80 37 32 26 40 85 21 21]\n",
      "  9063/10001: episode: 1007, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 20.322, mean reward:  2.258 [-10.000,  5.623], mean action: 40.111 [19.000, 85.000],  loss: 16.892921, mae: 7.661718, mean_q: 15.970836\n",
      "[94 73  2 96 10 13 92 26 21 50]\n",
      "  9072/10001: episode: 1008, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 33.469, mean reward:  3.719 [ 2.555,  4.593], mean action: 42.556 [2.000, 96.000],  loss: 13.226987, mae: 7.799683, mean_q: 16.073341\n",
      "[48 39 84 78 94 11 40 85 12 40]\n",
      "  9081/10001: episode: 1009, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 19.336, mean reward:  2.148 [-10.000,  5.563], mean action: 53.667 [11.000, 94.000],  loss: 16.987762, mae: 8.116322, mean_q: 16.241447\n",
      "[65 89 66 73 10 15 44 13 80 35]\n",
      "  9090/10001: episode: 1010, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 35.467, mean reward:  3.941 [ 2.710,  6.667], mean action: 47.222 [10.000, 89.000],  loss: 14.943058, mae: 8.274044, mean_q: 16.453388\n",
      "[87  2 33 97 43 32 45 85 65 62]\n",
      "  9099/10001: episode: 1011, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 32.382, mean reward:  3.598 [ 1.884,  4.589], mean action: 51.556 [2.000, 97.000],  loss: 13.158314, mae: 8.259215, mean_q: 16.554821\n",
      "[66  4 66 66 66  8 35 85 80  8]\n",
      "  9108/10001: episode: 1012, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: -19.197, mean reward: -2.133 [-10.000,  5.704], mean action: 46.444 [4.000, 85.000],  loss: 13.337079, mae: 8.178638, mean_q: 16.455709\n",
      "[63 38 96 66 33 43 80 27 89 74]\n",
      "  9117/10001: episode: 1013, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 41.581, mean reward:  4.620 [ 3.380,  6.073], mean action: 60.667 [27.000, 96.000],  loss: 16.746426, mae: 8.293019, mean_q: 16.859119\n",
      "[82  2 66 17 73 30 40 77 92 74]\n",
      "  9126/10001: episode: 1014, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 36.189, mean reward:  4.021 [ 2.552,  5.416], mean action: 52.333 [2.000, 92.000],  loss: 13.954288, mae: 7.916320, mean_q: 16.260607\n",
      "[32 17 66 66 61 42 17 26 94 17]\n",
      "  9135/10001: episode: 1015, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -10.616, mean reward: -1.180 [-10.000,  4.880], mean action: 45.111 [17.000, 94.000],  loss: 14.572564, mae: 8.410812, mean_q: 17.260067\n",
      "[85 25 61 82 66  5 17 38 65 48]\n",
      "  9144/10001: episode: 1016, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 35.626, mean reward:  3.958 [ 2.212,  8.327], mean action: 45.222 [5.000, 82.000],  loss: 11.423885, mae: 8.639690, mean_q: 17.515755\n",
      "[20 96 66 66 27 80 93 10 11 66]\n",
      "  9153/10001: episode: 1017, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  9.391, mean reward:  1.043 [-10.000,  4.929], mean action: 57.222 [10.000, 96.000],  loss: 16.161541, mae: 8.335972, mean_q: 16.832470\n",
      "[61  5 47 11 47 11 35 85 13 65]\n",
      "  9162/10001: episode: 1018, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  7.023, mean reward:  0.780 [-10.000,  5.195], mean action: 35.444 [5.000, 85.000],  loss: 12.881348, mae: 8.178452, mean_q: 16.381927\n",
      "[68 13 27  5 61 17 48 47 80 11]\n",
      "  9171/10001: episode: 1019, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 39.551, mean reward:  4.395 [ 2.642,  6.903], mean action: 34.333 [5.000, 80.000],  loss: 13.498590, mae: 8.825809, mean_q: 18.096207\n",
      "[46 25 27 37 38 80 89 85 42 73]\n",
      "  9180/10001: episode: 1020, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 36.196, mean reward:  4.022 [ 2.694,  6.135], mean action: 55.111 [25.000, 89.000],  loss: 13.524322, mae: 7.800091, mean_q: 16.455154\n",
      "[16  5 27 43 27 85 32 80 48  8]\n",
      "  9189/10001: episode: 1021, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.209, mean reward:  2.690 [-10.000,  6.009], mean action: 39.444 [5.000, 85.000],  loss: 12.215599, mae: 8.316877, mean_q: 17.573252\n",
      "[53  5 27 53 72 22 22 85 93 48]\n",
      "  9198/10001: episode: 1022, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 11.645, mean reward:  1.294 [-10.000,  7.536], mean action: 47.444 [5.000, 93.000],  loss: 15.470140, mae: 7.987656, mean_q: 16.687584\n",
      "[31 53  5 53 97 80 80 82 65 17]\n",
      "  9207/10001: episode: 1023, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  9.040, mean reward:  1.004 [-10.000,  6.026], mean action: 59.111 [5.000, 97.000],  loss: 14.018165, mae: 7.675089, mean_q: 15.984238\n",
      "[72  5 47 38 92 80 89 21 78 77]\n",
      "  9216/10001: episode: 1024, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.114, mean reward:  4.013 [ 2.359,  6.030], mean action: 58.556 [5.000, 92.000],  loss: 12.693583, mae: 8.111518, mean_q: 16.708393\n",
      "[ 6  5 80 38 96 39 80 65 65 74]\n",
      "  9225/10001: episode: 1025, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward:  6.098, mean reward:  0.678 [-10.000,  6.545], mean action: 60.222 [5.000, 96.000],  loss: 11.686334, mae: 7.957488, mean_q: 16.407095\n",
      "[ 7 72  6 97 78 80 11 45 10 53]\n",
      "  9234/10001: episode: 1026, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 37.506, mean reward:  4.167 [ 2.665,  5.234], mean action: 50.222 [6.000, 97.000],  loss: 13.389202, mae: 8.322478, mean_q: 17.091103\n",
      "[88  5 33 40 42 35 44 21 11  8]\n",
      "  9243/10001: episode: 1027, duration: 0.061s, episode steps:   9, steps per second: 149, episode reward: 36.253, mean reward:  4.028 [ 2.328,  6.447], mean action: 26.556 [5.000, 44.000],  loss: 12.610946, mae: 8.366155, mean_q: 17.197874\n",
      "[72  4 84 40 47 85 40 85 42 77]\n",
      "  9252/10001: episode: 1028, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward:  6.367, mean reward:  0.707 [-10.000,  5.436], mean action: 56.000 [4.000, 85.000],  loss: 16.044168, mae: 8.143763, mean_q: 16.891731\n",
      "[63 53 66 77 21 45 21 97 12 43]\n",
      "  9261/10001: episode: 1029, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 23.224, mean reward:  2.580 [-10.000,  7.092], mean action: 48.333 [12.000, 97.000],  loss: 13.297793, mae: 8.188242, mean_q: 17.338705\n",
      "[31 21 21 93 42 11 35 21 65 48]\n",
      "  9270/10001: episode: 1030, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 11.143, mean reward:  1.238 [-10.000,  7.909], mean action: 39.667 [11.000, 93.000],  loss: 13.852116, mae: 7.975160, mean_q: 17.386522\n",
      "[44  4 21 42 21 22 45 21 20 47]\n",
      "  9279/10001: episode: 1031, duration: 0.063s, episode steps:   9, steps per second: 142, episode reward:  6.315, mean reward:  0.702 [-10.000,  4.980], mean action: 27.000 [4.000, 47.000],  loss: 15.756099, mae: 8.098579, mean_q: 17.639500\n",
      "[81 21 66 21 82 44 40 21 77 40]\n",
      "  9288/10001: episode: 1032, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -5.935, mean reward: -0.659 [-10.000,  5.113], mean action: 45.778 [21.000, 82.000],  loss: 12.912558, mae: 9.038558, mean_q: 19.392565\n",
      "[96 21 38 21 96 93 68 21 92 20]\n",
      "  9297/10001: episode: 1033, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: -8.141, mean reward: -0.905 [-10.000,  4.901], mean action: 52.222 [20.000, 96.000],  loss: 14.475998, mae: 7.745283, mean_q: 16.611774\n",
      "[57  2 21 97 21 94 37  8 40 11]\n",
      "  9306/10001: episode: 1034, duration: 0.068s, episode steps:   9, steps per second: 132, episode reward: 25.338, mean reward:  2.815 [-10.000,  7.120], mean action: 36.778 [2.000, 97.000],  loss: 14.596488, mae: 8.763700, mean_q: 18.443840\n",
      "[94 85 47 38 47 38 93 12 84  8]\n",
      "  9315/10001: episode: 1035, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  7.865, mean reward:  0.874 [-10.000,  6.722], mean action: 50.222 [8.000, 93.000],  loss: 12.350392, mae: 8.144706, mean_q: 16.175564\n",
      "[ 0  4 15 63 75 93 37 47 21 65]\n",
      "  9324/10001: episode: 1036, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 34.154, mean reward:  3.795 [ 2.397,  4.697], mean action: 46.667 [4.000, 93.000],  loss: 13.707994, mae: 8.527792, mean_q: 17.250074\n",
      "[32  4 75 10 39 40 11 94  5 40]\n",
      "  9333/10001: episode: 1037, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 19.375, mean reward:  2.153 [-10.000,  4.869], mean action: 35.333 [4.000, 94.000],  loss: 11.462592, mae: 8.436255, mean_q: 17.354237\n",
      "[91 48 33 75 68 22 66 20 97 77]\n",
      "  9342/10001: episode: 1038, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 34.790, mean reward:  3.866 [ 1.970,  5.384], mean action: 56.222 [20.000, 97.000],  loss: 15.564541, mae: 8.230325, mean_q: 16.838036\n",
      "[66 11  8 33 92 48 39 73  5 21]\n",
      "  9351/10001: episode: 1039, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 34.795, mean reward:  3.866 [ 3.058,  5.120], mean action: 36.667 [5.000, 92.000],  loss: 12.720493, mae: 8.305650, mean_q: 16.460960\n",
      "[18 94 33 10 68 17 92 94 13 13]\n",
      "  9360/10001: episode: 1040, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  3.589, mean reward:  0.399 [-10.000,  4.812], mean action: 48.222 [10.000, 94.000],  loss: 14.857021, mae: 7.959768, mean_q: 15.996266\n",
      "[50 73 45 33 55 11 37 97 75 48]\n",
      "  9369/10001: episode: 1041, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.620, mean reward:  4.402 [ 2.309,  7.557], mean action: 52.667 [11.000, 97.000],  loss: 15.853816, mae: 7.947761, mean_q: 15.864064\n",
      "[64 48  2 43 80 17 13 65 73  8]\n",
      "  9378/10001: episode: 1042, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 35.975, mean reward:  3.997 [ 2.611,  6.366], mean action: 38.778 [2.000, 80.000],  loss: 13.611884, mae: 7.904706, mean_q: 15.965528\n",
      "[65 48 84 73 85 94 61 84 13 82]\n",
      "  9387/10001: episode: 1043, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 20.809, mean reward:  2.312 [-10.000,  8.323], mean action: 69.333 [13.000, 94.000],  loss: 12.668989, mae: 8.489300, mean_q: 16.853498\n",
      "[42 38 96 94 78 85 94 80 80 48]\n",
      "  9396/10001: episode: 1044, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  9.905, mean reward:  1.101 [-10.000,  6.674], mean action: 77.000 [38.000, 96.000],  loss: 13.128839, mae: 8.629869, mean_q: 17.084602\n",
      "[12 48 80 15 33  6 11 73 94 41]\n",
      "  9405/10001: episode: 1045, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 38.105, mean reward:  4.234 [ 2.675,  5.800], mean action: 44.556 [6.000, 94.000],  loss: 14.481045, mae: 8.437806, mean_q: 16.815329\n",
      "[37  8 33 80 16 93 77 73 80 41]\n",
      "  9414/10001: episode: 1046, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 24.217, mean reward:  2.691 [-10.000,  6.336], mean action: 55.667 [8.000, 93.000],  loss: 15.644512, mae: 8.440342, mean_q: 16.368755\n",
      "[20 45 80  2 97 78 39 44 80 41]\n",
      "  9423/10001: episode: 1047, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 22.737, mean reward:  2.526 [-10.000,  6.771], mean action: 56.222 [2.000, 97.000],  loss: 12.605191, mae: 8.147290, mean_q: 15.479673\n",
      "[83  2 47 80 32 48 84 20 85 74]\n",
      "  9432/10001: episode: 1048, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 34.498, mean reward:  3.833 [ 2.050,  5.909], mean action: 52.444 [2.000, 85.000],  loss: 12.373302, mae: 8.483850, mean_q: 16.335552\n",
      "[79 13 78 77  2 68 32 94 78 41]\n",
      "  9441/10001: episode: 1049, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 19.961, mean reward:  2.218 [-10.000,  7.046], mean action: 53.667 [2.000, 94.000],  loss: 12.955934, mae: 8.458233, mean_q: 16.291578\n",
      "[55 11 37  2 33 39 44 85 12 78]\n",
      "  9450/10001: episode: 1050, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 35.607, mean reward:  3.956 [ 2.741,  5.686], mean action: 37.889 [2.000, 85.000],  loss: 12.975769, mae: 8.788832, mean_q: 17.427137\n",
      "[44  2 75 33 27 50 43 89 78 48]\n",
      "  9459/10001: episode: 1051, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 39.029, mean reward:  4.337 [ 1.903,  7.167], mean action: 49.444 [2.000, 89.000],  loss: 13.338742, mae: 7.958867, mean_q: 15.965275\n",
      "[67 13 27 39 33 80 35 27 85 40]\n",
      "  9468/10001: episode: 1052, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 23.465, mean reward:  2.607 [-10.000,  6.079], mean action: 42.111 [13.000, 85.000],  loss: 13.539650, mae: 8.124096, mean_q: 16.284075\n",
      "[78 48 68 89 85 97 40 49 75  2]\n",
      "  9477/10001: episode: 1053, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.950, mean reward:  4.550 [ 2.207,  9.287], mean action: 61.444 [2.000, 97.000],  loss: 15.651461, mae: 8.034904, mean_q: 16.217079\n",
      "[64 16 96 94 89 77 42 85 27 66]\n",
      "  9486/10001: episode: 1054, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward: 39.722, mean reward:  4.414 [ 2.598,  7.439], mean action: 65.778 [16.000, 96.000],  loss: 12.313941, mae: 8.505065, mean_q: 17.243027\n",
      "[51 73 82 68 82 82 37 65 78  2]\n",
      "  9495/10001: episode: 1055, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward:  9.593, mean reward:  1.066 [-10.000,  9.751], mean action: 63.222 [2.000, 82.000],  loss: 13.926930, mae: 8.553963, mean_q: 16.524752\n",
      "[ 6  2 37  2 13 26 45 43 80 10]\n",
      "  9504/10001: episode: 1056, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 24.070, mean reward:  2.674 [-10.000,  6.629], mean action: 28.667 [2.000, 80.000],  loss: 15.467529, mae: 8.108840, mean_q: 15.608738\n",
      "[ 5  2 10 65 73 26 89 97 43 10]\n",
      "  9513/10001: episode: 1057, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 23.039, mean reward:  2.560 [-10.000,  5.571], mean action: 46.111 [2.000, 97.000],  loss: 13.721485, mae: 8.368764, mean_q: 16.360985\n",
      "[37 17 84 15 97 40 42 77 40 66]\n",
      "  9522/10001: episode: 1058, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 23.026, mean reward:  2.558 [-10.000,  7.195], mean action: 53.111 [15.000, 97.000],  loss: 16.518158, mae: 8.056378, mean_q: 16.098686\n",
      "[44 42 16 43 75 11 80 82 97 74]\n",
      "  9531/10001: episode: 1059, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 40.182, mean reward:  4.465 [ 2.783,  6.084], mean action: 57.778 [11.000, 97.000],  loss: 13.942426, mae: 8.234109, mean_q: 16.640154\n",
      "[63 44  2 72 75 80 55 11 85  8]\n",
      "  9540/10001: episode: 1060, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 37.876, mean reward:  4.208 [ 2.440,  6.566], mean action: 48.000 [2.000, 85.000],  loss: 13.401356, mae: 8.403970, mean_q: 16.855785\n",
      "[21 30 82 80 49 17 82 26 65 62]\n",
      "  9549/10001: episode: 1061, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 19.967, mean reward:  2.219 [-10.000,  4.834], mean action: 54.778 [17.000, 82.000],  loss: 13.973042, mae: 8.429718, mean_q: 17.004423\n",
      "[ 4 89 33 72 78 85  8 93 78 41]\n",
      "  9558/10001: episode: 1062, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 22.285, mean reward:  2.476 [-10.000,  6.203], mean action: 64.111 [8.000, 93.000],  loss: 13.587387, mae: 8.384657, mean_q: 16.726894\n",
      "[ 1 84 80 78 80 26 92 45 12 85]\n",
      "  9567/10001: episode: 1063, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 20.852, mean reward:  2.317 [-10.000,  5.425], mean action: 64.667 [12.000, 92.000],  loss: 14.893590, mae: 8.190414, mean_q: 16.004555\n",
      "[36 27 89 85 72 78 55  2 77  1]\n",
      "  9576/10001: episode: 1064, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 35.939, mean reward:  3.993 [ 2.563,  7.397], mean action: 54.000 [1.000, 89.000],  loss: 14.893810, mae: 8.272604, mean_q: 16.168907\n",
      "[69 73 94 47 92 37 85 73 92 85]\n",
      "  9585/10001: episode: 1065, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: -9.217, mean reward: -1.024 [-10.000,  4.451], mean action: 75.333 [37.000, 94.000],  loss: 13.700923, mae: 8.699068, mean_q: 16.680441\n",
      "[94 37  2 72 47 37 77 82 93 74]\n",
      "  9594/10001: episode: 1066, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 26.295, mean reward:  2.922 [-10.000,  6.354], mean action: 57.889 [2.000, 93.000],  loss: 11.924351, mae: 8.843896, mean_q: 17.001638\n",
      "[24 37 38 72 82 92 37 97 73 54]\n",
      "  9603/10001: episode: 1067, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 24.968, mean reward:  2.774 [-10.000,  7.071], mean action: 64.667 [37.000, 97.000],  loss: 14.334403, mae: 8.201591, mean_q: 16.341509\n",
      "[21 45 72 93 25 94 44 82 13 65]\n",
      "  9612/10001: episode: 1068, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 33.452, mean reward:  3.717 [ 2.484,  6.382], mean action: 59.222 [13.000, 94.000],  loss: 14.919209, mae: 8.300091, mean_q: 16.263798\n",
      "[69 24 65 85 75 42 26 77 26 26]\n",
      "  9621/10001: episode: 1069, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  5.750, mean reward:  0.639 [-10.000,  5.377], mean action: 49.556 [24.000, 85.000],  loss: 15.277159, mae: 8.346166, mean_q: 16.138863\n",
      "[98 33 66 33 75 17 42 10 89 54]\n",
      "  9630/10001: episode: 1070, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.012, mean reward:  2.557 [-10.000,  6.571], mean action: 46.556 [10.000, 89.000],  loss: 13.589800, mae: 9.017554, mean_q: 17.350920\n",
      "[14 50 84 96 96 17 94 80 11 65]\n",
      "  9639/10001: episode: 1071, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 21.639, mean reward:  2.404 [-10.000,  6.491], mean action: 65.889 [11.000, 96.000],  loss: 13.706692, mae: 8.245633, mean_q: 16.053455\n",
      "[11  2 25 16 80 11 42 42 94  8]\n",
      "  9648/10001: episode: 1072, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  7.202, mean reward:  0.800 [-10.000,  6.477], mean action: 35.556 [2.000, 94.000],  loss: 14.657436, mae: 8.325292, mean_q: 16.036190\n",
      "[ 4 82 75 93 43 73 37  8 93 27]\n",
      "  9657/10001: episode: 1073, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 23.562, mean reward:  2.618 [-10.000,  7.538], mean action: 59.000 [8.000, 93.000],  loss: 13.483485, mae: 8.385908, mean_q: 16.201805\n",
      "[42 45 25 45 82 17 48  8 26 73]\n",
      "  9666/10001: episode: 1074, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 24.693, mean reward:  2.744 [-10.000,  5.919], mean action: 41.000 [8.000, 82.000],  loss: 13.722134, mae: 8.680990, mean_q: 16.877394\n",
      "[99 75 96 85 72 97 27  2 45 27]\n",
      "  9675/10001: episode: 1075, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward: 24.686, mean reward:  2.743 [-10.000,  6.592], mean action: 58.444 [2.000, 97.000],  loss: 13.185614, mae: 8.392373, mean_q: 16.243029\n",
      "[95 30 75 30 94 26 42 85 53 60]\n",
      "  9684/10001: episode: 1076, duration: 0.062s, episode steps:   9, steps per second: 145, episode reward: 16.073, mean reward:  1.786 [-10.000,  4.510], mean action: 55.000 [26.000, 94.000],  loss: 15.033517, mae: 8.422670, mean_q: 16.745968\n",
      "[26 53 27 30 25 12 37 82 43 66]\n",
      "  9693/10001: episode: 1077, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 38.925, mean reward:  4.325 [ 2.328,  6.825], mean action: 41.667 [12.000, 82.000],  loss: 17.165045, mae: 8.525662, mean_q: 17.185312\n",
      "[11 17 80 75 78 12 42 43 40 85]\n",
      "  9702/10001: episode: 1078, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 37.681, mean reward:  4.187 [ 2.715,  6.835], mean action: 52.444 [12.000, 85.000],  loss: 16.266359, mae: 8.435426, mean_q: 16.647875\n",
      "[34 15 43 97 97 48 42 65 80 10]\n",
      "  9711/10001: episode: 1079, duration: 0.059s, episode steps:   9, steps per second: 153, episode reward: 24.389, mean reward:  2.710 [-10.000,  6.684], mean action: 55.222 [10.000, 97.000],  loss: 14.899582, mae: 8.045798, mean_q: 15.816548\n",
      "[ 7 20 50 25 97 22 42  5 47 82]\n",
      "  9720/10001: episode: 1080, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 37.186, mean reward:  4.132 [ 1.971,  8.987], mean action: 43.333 [5.000, 97.000],  loss: 14.053112, mae: 8.017920, mean_q: 15.739445\n",
      "[59  8 55  5 82 80 77 82 15 86]\n",
      "  9729/10001: episode: 1081, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 15.941, mean reward:  1.771 [-10.000,  4.715], mean action: 54.444 [5.000, 86.000],  loss: 14.159230, mae: 8.379280, mean_q: 16.324463\n",
      "[71 17 45 89 63 26 42 42 77 58]\n",
      "  9738/10001: episode: 1082, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 20.295, mean reward:  2.255 [-10.000,  6.083], mean action: 51.000 [17.000, 89.000],  loss: 12.986517, mae: 8.119155, mean_q: 16.058157\n",
      "[54 25 50 50 38 93 11 68 72  2]\n",
      "  9747/10001: episode: 1083, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 25.266, mean reward:  2.807 [-10.000, 10.165], mean action: 45.444 [2.000, 93.000],  loss: 12.935374, mae: 8.250992, mean_q: 16.130964\n",
      "[66 21  2 72 39 25 82 45 53 74]\n",
      "  9756/10001: episode: 1084, duration: 0.060s, episode steps:   9, steps per second: 149, episode reward: 36.591, mean reward:  4.066 [ 2.616,  5.952], mean action: 45.889 [2.000, 82.000],  loss: 12.755967, mae: 8.547353, mean_q: 16.589884\n",
      "[37 45 33 27 97 48 35 97 97 74]\n",
      "  9765/10001: episode: 1085, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward:  8.942, mean reward:  0.994 [-10.000,  5.596], mean action: 61.444 [27.000, 97.000],  loss: 11.569184, mae: 9.180130, mean_q: 17.819025\n",
      "[47 24 33 11 73 48  5 84 43 41]\n",
      "  9774/10001: episode: 1086, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 36.859, mean reward:  4.095 [ 2.741,  5.865], mean action: 40.222 [5.000, 84.000],  loss: 13.064838, mae: 9.010797, mean_q: 17.627768\n",
      "[12 26 27 27 42 42 92  8 80 74]\n",
      "  9783/10001: episode: 1087, duration: 0.057s, episode steps:   9, steps per second: 157, episode reward: 11.107, mean reward:  1.234 [-10.000,  6.362], mean action: 46.444 [8.000, 92.000],  loss: 15.915883, mae: 8.663834, mean_q: 17.259806\n",
      "[45 20 82 44 42 48 94 45 35 66]\n",
      "  9792/10001: episode: 1088, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 21.869, mean reward:  2.430 [-10.000,  7.690], mean action: 52.889 [20.000, 94.000],  loss: 14.766778, mae: 8.228038, mean_q: 16.680735\n",
      "[59 17 33 27 25 26 94 45 15 12]\n",
      "  9801/10001: episode: 1089, duration: 0.058s, episode steps:   9, steps per second: 155, episode reward: 30.035, mean reward:  3.337 [ 1.882,  4.552], mean action: 32.667 [12.000, 94.000],  loss: 14.371807, mae: 8.531038, mean_q: 16.954216\n",
      "[45 30 27 72 42 48 38 47 93  8]\n",
      "  9810/10001: episode: 1090, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 36.485, mean reward:  4.054 [ 2.417,  6.072], mean action: 45.000 [8.000, 93.000],  loss: 15.857734, mae: 8.293043, mean_q: 16.695683\n",
      "[16 20 38 80 80 41 42 85 53 65]\n",
      "  9819/10001: episode: 1091, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 20.907, mean reward:  2.323 [-10.000,  4.927], mean action: 56.000 [20.000, 85.000],  loss: 12.106157, mae: 9.041630, mean_q: 17.488791\n",
      "[86 27 80  8 27 48 93 26 85 50]\n",
      "  9828/10001: episode: 1092, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 21.301, mean reward:  2.367 [-10.000,  4.548], mean action: 49.333 [8.000, 93.000],  loss: 15.104756, mae: 8.944523, mean_q: 17.604088\n",
      "[20 30 27 93 97 48 42 78 73 48]\n",
      "  9837/10001: episode: 1093, duration: 0.067s, episode steps:   9, steps per second: 134, episode reward: 21.821, mean reward:  2.425 [-10.000,  5.651], mean action: 59.556 [27.000, 97.000],  loss: 15.101627, mae: 8.547741, mean_q: 17.070704\n",
      "[85 85 73 73 66 17 12 45 26 54]\n",
      "  9846/10001: episode: 1094, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  9.323, mean reward:  1.036 [-10.000,  6.419], mean action: 50.111 [12.000, 85.000],  loss: 15.115719, mae: 8.320319, mean_q: 15.950121\n",
      "[92 41 47 44 97 48 39 97 39 54]\n",
      "  9855/10001: episode: 1095, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  8.906, mean reward:  0.990 [-10.000,  7.290], mean action: 56.222 [39.000, 97.000],  loss: 11.391716, mae: 9.222279, mean_q: 17.831247\n",
      "[50 48 33 85  2 61 77 85 12 77]\n",
      "  9864/10001: episode: 1096, duration: 0.062s, episode steps:   9, steps per second: 144, episode reward:  5.309, mean reward:  0.590 [-10.000,  5.034], mean action: 53.333 [2.000, 85.000],  loss: 13.532166, mae: 9.028535, mean_q: 17.954836\n",
      "[92 48 33 80 75 48 78 45 41 54]\n",
      "  9873/10001: episode: 1097, duration: 0.065s, episode steps:   9, steps per second: 139, episode reward: 23.343, mean reward:  2.594 [-10.000,  5.890], mean action: 55.778 [33.000, 80.000],  loss: 14.594313, mae: 8.581088, mean_q: 17.350760\n",
      "[75 73 53 85 40 48 43 82 82 12]\n",
      "  9882/10001: episode: 1098, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 20.628, mean reward:  2.292 [-10.000,  5.439], mean action: 57.556 [12.000, 85.000],  loss: 14.877818, mae: 8.530909, mean_q: 16.944317\n",
      "[94 11 33 75 25 48 72 97 53 27]\n",
      "  9891/10001: episode: 1099, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 39.282, mean reward:  4.365 [ 2.546,  8.396], mean action: 49.000 [11.000, 97.000],  loss: 12.520968, mae: 9.269452, mean_q: 18.199257\n",
      "[40 26 15 55 75 20 17 61 73 27]\n",
      "  9900/10001: episode: 1100, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 37.509, mean reward:  4.168 [ 2.428,  8.914], mean action: 41.000 [15.000, 75.000],  loss: 15.391237, mae: 8.894518, mean_q: 17.444563\n",
      "[30 89 82 84 82 11 26 82 77  8]\n",
      "  9909/10001: episode: 1101, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward:  6.655, mean reward:  0.739 [-10.000,  4.793], mean action: 60.111 [8.000, 89.000],  loss: 15.573429, mae: 8.540421, mean_q: 16.612587\n",
      "[ 6 41 53 77 39 30 89  2 26 80]\n",
      "  9918/10001: episode: 1102, duration: 0.060s, episode steps:   9, steps per second: 150, episode reward: 40.932, mean reward:  4.548 [ 2.633,  7.584], mean action: 48.556 [2.000, 89.000],  loss: 12.680534, mae: 8.902063, mean_q: 17.429970\n",
      "[75 17 39 16 42 12 26 41 94 54]\n",
      "  9927/10001: episode: 1103, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward: 34.759, mean reward:  3.862 [ 2.687,  7.610], mean action: 37.889 [12.000, 94.000],  loss: 14.031456, mae: 9.359721, mean_q: 18.091980\n",
      "[25 45 53 84 49 12 12 85 10  5]\n",
      "  9936/10001: episode: 1104, duration: 0.059s, episode steps:   9, steps per second: 151, episode reward: 19.868, mean reward:  2.208 [-10.000,  5.833], mean action: 39.444 [5.000, 85.000],  loss: 11.771854, mae: 9.136868, mean_q: 17.706459\n",
      "[34 20 75 17 43 12 84 47 53 40]\n",
      "  9945/10001: episode: 1105, duration: 0.065s, episode steps:   9, steps per second: 137, episode reward: 35.978, mean reward:  3.998 [ 2.912,  8.083], mean action: 43.444 [12.000, 84.000],  loss: 15.392517, mae: 8.763055, mean_q: 17.087379\n",
      "[22  2 25 75 38 68 77 53 12 10]\n",
      "  9954/10001: episode: 1106, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward: 34.063, mean reward:  3.785 [ 2.191,  6.406], mean action: 40.000 [2.000, 77.000],  loss: 15.622598, mae: 8.557076, mean_q: 16.899361\n",
      "[53  8 10 92 84 12 48 27 12 53]\n",
      "  9963/10001: episode: 1107, duration: 0.062s, episode steps:   9, steps per second: 146, episode reward:  7.606, mean reward:  0.845 [-10.000,  6.759], mean action: 38.444 [8.000, 92.000],  loss: 13.416907, mae: 9.126553, mean_q: 17.866831\n",
      "[54  4 92 33 77 12 12 77 47 54]\n",
      "  9972/10001: episode: 1108, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: -9.072, mean reward: -1.008 [-10.000,  4.863], mean action: 45.333 [4.000, 92.000],  loss: 12.511187, mae: 8.859566, mean_q: 17.829571\n",
      "[19 25 33 75 92 12 12 65 73  2]\n",
      "  9981/10001: episode: 1109, duration: 0.060s, episode steps:   9, steps per second: 151, episode reward: 25.561, mean reward:  2.840 [-10.000,  9.021], mean action: 43.222 [2.000, 92.000],  loss: 14.881741, mae: 9.194468, mean_q: 18.870409\n",
      "[90  4 61 75 78 12 80 66 20 85]\n",
      "  9990/10001: episode: 1110, duration: 0.058s, episode steps:   9, steps per second: 154, episode reward: 36.550, mean reward:  4.061 [ 2.240,  5.924], mean action: 53.444 [4.000, 85.000],  loss: 12.289322, mae: 9.318092, mean_q: 18.259548\n",
      "[68  2 92 82 15 44 17 82 26 12]\n",
      "  9999/10001: episode: 1111, duration: 0.059s, episode steps:   9, steps per second: 152, episode reward: 19.456, mean reward:  2.162 [-10.000,  5.187], mean action: 41.333 [2.000, 92.000],  loss: 18.668715, mae: 9.034977, mean_q: 17.633247\n",
      "done, took 67.947 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-2), metrics=['mae'])\n",
    "history=dqn.fit(env, nb_steps=10001, visualize=False, verbose=2, action_repetition=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn.save_weights('models/dqn_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84230256  2.1544108  -0.17801356 -1.1244324  -6.917169    0.9025743\n",
      " -7.6003804  -0.60957295 -8.092312   -0.44187778  2.4565437  -4.1548615\n",
      " -5.9104066  -0.87728083 -0.8864699  -3.5877237  -6.3363647   0.56548584\n",
      " -0.62716854  3.6924474  -0.71010697 -0.7632155   6.258907    0.29557925\n",
      " -8.501481    1.5666356   4.008937   -2.253514    0.12275475  0.70297545\n",
      "  2.448017   -0.9082072   0.48594975 -2.756115    0.2613757   4.204152\n",
      "  0.8639873  -0.21606708 -2.5786698  -0.6176131   7.825904   -2.1188056\n",
      "  4.344573    6.1186395   1.6369576   1.6296175  -0.32672647 -0.6285348\n",
      "  3.7687993  -0.79880255  3.1155853  -0.16062757  0.6601752   0.8559015\n",
      "  0.20319605  2.5805862   0.93656695 -0.93185437  3.2977033   0.10073475\n",
      "  1.2613027   5.4626365   2.8852546   3.299739   -1.0771296   5.961876\n",
      "  6.5794244   0.29216272  2.6042218   0.07494382  0.05615321  0.44852164\n",
      "  1.4639846   5.386642    2.9918742   4.8707886  -0.10537417  4.2536306\n",
      "  6.867627   -0.23193343  7.4536047  -1.103358    5.863327   -0.14737633\n",
      "  1.407878    7.8298044   2.7877882   0.03778537 -0.70831674  7.4473915\n",
      " -0.33275452 -1.5065608   3.0466628  -1.5589929   4.724204   -0.5773688\n",
      "  3.902306    5.663827    0.24446996  0.728889  ]\n",
      "85\n",
      "40\n",
      "Episode = [38, 69, 25, 25, 97, 48, 11, 85, 92, 54]\n",
      "Reward = 125.42401123046875\n"
     ]
    }
   ],
   "source": [
    "a=[[ np.random.randint(0,100), np.random.randint(0,100), np.random.randint(0,100), np.random.randint(0,100), -1, -1, -1, -1, -1, -1]]\n",
    "a=[[ np.random.randint(0,100), np.random.randint(0,100), -1, -1, -1, -1, -1, -1, -1, -1]]\n",
    "t=0\n",
    "idx=2\n",
    "test= [[88, 41, 49, 42, 69, 43, 81, 6, 20, -1]]\n",
    "\n",
    "test_reward=dqn.compute_q_values(test)\n",
    "max_v=max(test_reward)\n",
    "print(test_reward)\n",
    "rrr=test_reward.tolist()\n",
    "index = rrr.index(max_v)\n",
    "print(index)\n",
    "rrr[index]=-100\n",
    "max_v=max(rrr)\n",
    "print(rrr.index(max_v))\n",
    "reward=[]\n",
    "\n",
    "for i in range(idx,10):\n",
    "        z=dqn.compute_q_values(a)\n",
    "        max_v=max(z)\n",
    "        t+=max_v\n",
    "        #print(max_v)\n",
    "        z=z.tolist()\n",
    "        index = z.index(max_v)\n",
    "        a[0][i]=index\n",
    "res=a[0]\n",
    "reward.append(t)\n",
    "    \n",
    "    \n",
    "print(\"Episode =\", a[0])\n",
    "print(\"Reward =\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maayush-singharoy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:46: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aayush/git/rl_recsys/src/recsim/trial/wandb/run-20230202_135256-clgq8pk7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/clgq8pk7\" target=\"_blank\">sweet-dumpling-54</a></strong> to <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial\" target=\"_blank\">https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/clgq8pk7\" target=\"_blank\">https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/clgq8pk7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN\n",
      "Input state = [[91, 3, 69, 62, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [91, 3, 69, 62, 97, 48, 42, 85, 92, 54]\n",
      "Reward = 84.7132077217102\n",
      "Input state = [[74, 22, 46, 5, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [74, 22, 46, 5, 25, 11, 17, 73, 77, 54]\n",
      "Reward = 86.76927995681763\n",
      "Input state = [[56, 55, 45, 71, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [56, 55, 45, 71, 97, 48, 11, 85, 20, 80]\n",
      "Reward = 85.8101806640625\n",
      "Input state = [[89, 1, 47, 74, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [89, 1, 47, 74, 42, 48, 11, 85, 20, 97]\n",
      "Reward = 88.00288772583008\n",
      "Input state = [[15, 90, 41, 13, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [15, 90, 41, 13, 25, 48, 11, 66, 77, 54]\n",
      "Reward = 82.22298669815063\n",
      "Input state = [[25, 96, 36, 9, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [25, 96, 36, 9, 97, 48, 11, 66, 77, 54]\n",
      "Reward = 81.11825132369995\n",
      "Input state = [[78, 84, 68, 84, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [78, 84, 68, 84, 97, 11, 17, 66, 92, 54]\n",
      "Reward = 79.28346967697144\n",
      "Input state = [[60, 52, 86, 31, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [60, 52, 86, 31, 97, 11, 17, 66, 77, 54]\n",
      "Reward = 81.15976905822754\n",
      "Input state = [[54, 70, 5, 75, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [54, 70, 5, 75, 97, 11, 48, 85, 20, 77]\n",
      "Reward = 89.03438091278076\n",
      "Input state = [[28, 36, 10, 53, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [28, 36, 10, 53, 42, 11, 17, 85, 20, 97]\n",
      "Reward = 88.14443778991699\n",
      "Input state = [[42, 28, 49, 5, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [42, 28, 49, 5, 25, 11, 17, 66, 77, 54]\n",
      "Reward = 83.34974002838135\n",
      "Input state = [[50, 72, 5, 94, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [50, 72, 5, 94, 97, 11, 48, 85, 20, 77]\n",
      "Reward = 87.89082908630371\n",
      "Input state = [[97, 54, 46, 62, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [97, 54, 46, 62, 75, 11, 17, 66, 92, 2]\n",
      "Reward = 79.89163970947266\n",
      "Input state = [[28, 55, 18, 74, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [28, 55, 18, 74, 42, 11, 17, 85, 20, 82]\n",
      "Reward = 85.42933654785156\n",
      "Input state = [[81, 49, 32, 1, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [81, 49, 32, 1, 25, 48, 77, 73, 92, 54]\n",
      "Reward = 88.83784484863281\n",
      "Input state = [[18, 27, 43, 30, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [18, 27, 43, 30, 42, 11, 17, 66, 77, 54]\n",
      "Reward = 83.09747791290283\n",
      "Input state = [[45, 8, 49, 95, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [45, 8, 49, 95, 97, 48, 42, 85, 92, 54]\n",
      "Reward = 85.60112237930298\n",
      "Input state = [[15, 86, 73, 94, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [15, 86, 73, 94, 97, 11, 17, 66, 92, 54]\n",
      "Reward = 80.39203691482544\n",
      "Input state = [[9, 91, 43, 45, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [9, 91, 43, 45, 97, 11, 48, 66, 92, 54]\n",
      "Reward = 85.04647970199585\n",
      "Input state = [[69, 32, 56, 27, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [69, 32, 56, 27, 97, 48, 77, 66, 92, 54]\n",
      "Reward = 82.11918210983276\n",
      "Input state = [[66, 19, 27, 70, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [66, 19, 27, 70, 42, 48, 11, 85, 20, 77]\n",
      "Reward = 90.82943630218506\n",
      "Input state = [[49, 35, 76, 96, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [49, 35, 76, 96, 97, 48, 11, 66, 77, 54]\n",
      "Reward = 79.0545973777771\n",
      "Input state = [[64, 12, 84, 60, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [64, 12, 84, 60, 97, 48, 42, 66, 77, 54]\n",
      "Reward = 83.0189962387085\n",
      "Input state = [[64, 55, 61, 40, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [64, 55, 61, 40, 97, 11, 17, 66, 77, 54]\n",
      "Reward = 80.63919448852539\n",
      "Input state = [[25, 11, 70, 27, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [25, 11, 70, 27, 97, 48, 42, 66, 77, 54]\n",
      "Reward = 85.39060354232788\n",
      "Input state = [[62, 92, 6, 73, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [62, 92, 6, 73, 97, 48, 11, 85, 20, 77]\n",
      "Reward = 89.2241621017456\n",
      "Input state = [[47, 54, 10, 51, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [47, 54, 10, 51, 42, 11, 48, 66, 20, 85]\n",
      "Reward = 87.65293502807617\n",
      "Input state = [[66, 13, 42, 59, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [66, 13, 42, 59, 97, 48, 11, 85, 20, 80]\n",
      "Reward = 86.70711517333984\n",
      "Input state = [[99, 2, 49, 30, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [99, 2, 49, 30, 42, 11, 17, 73, 77, 54]\n",
      "Reward = 89.053457736969\n",
      "Input state = [[43, 46, 76, 31, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [43, 46, 76, 31, 97, 48, 77, 66, 92, 54]\n",
      "Reward = 81.31059503555298\n",
      "Input state = [[11, 50, 50, 20, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [11, 50, 50, 20, 25, 17, 77, 66, 92, 54]\n",
      "Reward = 80.47108030319214\n",
      "Input state = [[41, 72, 12, 78, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [41, 72, 12, 78, 97, 11, 48, 66, 20, 85]\n",
      "Reward = 87.72845077514648\n",
      "Input state = [[83, 91, 34, 5, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [83, 91, 34, 5, 97, 48, 11, 85, 92, 54]\n",
      "Reward = 82.53460025787354\n",
      "Input state = [[96, 67, 53, 60, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [96, 67, 53, 60, 97, 11, 17, 66, 77, 54]\n",
      "Reward = 80.33854007720947\n",
      "Input state = [[91, 1, 49, 84, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [91, 1, 49, 84, 42, 48, 11, 85, 92, 54]\n",
      "Reward = 84.25958919525146\n",
      "Input state = [[10, 51, 17, 45, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [10, 51, 17, 45, 42, 11, 48, 66, 77, 54]\n",
      "Reward = 83.82766723632812\n",
      "Input state = [[28, 49, 40, 91, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [28, 49, 40, 91, 97, 48, 11, 85, 20, 77]\n",
      "Reward = 86.72532749176025\n",
      "Input state = [[1, 58, 29, 19, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [1, 58, 29, 19, 97, 48, 77, 66, 92, 54]\n",
      "Reward = 82.49082803726196\n",
      "Input state = [[47, 51, 69, 14, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [47, 51, 69, 14, 25, 11, 17, 66, 77, 54]\n",
      "Reward = 83.01218748092651\n",
      "Input state = [[91, 66, 76, 53, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [91, 66, 76, 53, 97, 11, 17, 82, 92, 54]\n",
      "Reward = 79.9051570892334\n",
      "Input state = [[45, 70, 59, 27, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [45, 70, 59, 27, 97, 11, 17, 66, 77, 54]\n",
      "Reward = 82.31696081161499\n",
      "Input state = [[72, 65, 55, 21, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [72, 65, 55, 21, 25, 11, 48, 66, 77, 54]\n",
      "Reward = 84.05081844329834\n",
      "Input state = [[68, 48, 33, 44, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [68, 48, 33, 44, 97, 11, 17, 66, 77, 54]\n",
      "Reward = 82.98076629638672\n",
      "Input state = [[7, 62, 87, 72, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [7, 62, 87, 72, 97, 48, 77, 2, 66, 40]\n",
      "Reward = 86.38084602355957\n",
      "Input state = [[70, 15, 28, 43, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [70, 15, 28, 43, 42, 48, 11, 85, 20, 97]\n",
      "Reward = 88.85163307189941\n",
      "Input state = [[94, 36, 86, 14, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [94, 36, 86, 14, 25, 11, 17, 73, 77, 54]\n",
      "Reward = 85.15213823318481\n",
      "Input state = [[13, 70, 83, 12, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [13, 70, 83, 12, 25, 11, 17, 2, 66, 54]\n",
      "Reward = 84.94802141189575\n",
      "Input state = [[13, 64, 84, 12, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [13, 64, 84, 12, 25, 11, 17, 2, 66, 54]\n",
      "Reward = 84.94328117370605\n",
      "Input state = [[84, 5, 35, 71, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [84, 5, 35, 71, 42, 48, 11, 85, 20, 80]\n",
      "Reward = 90.22866439819336\n",
      "Input state = [[39, 33, 18, 12, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [39, 33, 18, 12, 42, 48, 11, 85, 92, 54]\n",
      "Reward = 83.36240863800049\n",
      "Input state = [[84, 8, 94, 91, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [84, 8, 94, 91, 97, 48, 11, 85, 92, 54]\n",
      "Reward = 84.32432270050049\n",
      "Input state = [[30, 89, 43, 33, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [30, 89, 43, 33, 97, 48, 11, 66, 77, 54]\n",
      "Reward = 80.80145597457886\n",
      "Input state = [[81, 90, 60, 11, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [81, 90, 60, 11, 97, 48, 39, 66, 77, 54]\n",
      "Reward = 81.07998514175415\n",
      "Input state = [[52, 49, 81, 88, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [52, 49, 81, 88, 97, 48, 11, 66, 77, 54]\n",
      "Reward = 79.2937240600586\n",
      "Input state = [[54, 18, 53, 2, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [54, 18, 53, 2, 25, 11, 17, 73, 77, 48]\n",
      "Reward = 82.59274005889893\n",
      "Input state = [[15, 95, 24, 18, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [15, 95, 24, 18, 25, 48, 11, 66, 77, 54]\n",
      "Reward = 83.69043159484863\n",
      "Input state = [[91, 51, 20, 86, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [91, 51, 20, 86, 97, 48, 11, 85, 92, 54]\n",
      "Reward = 85.9355731010437\n",
      "Input state = [[50, 1, 80, 61, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [50, 1, 80, 61, 97, 48, 42, 82, 77, 54]\n",
      "Reward = 85.64310359954834\n",
      "Input state = [[85, 58, 4, 82, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [85, 58, 4, 82, 97, 48, 11, 80, 20, 77]\n",
      "Reward = 90.59305667877197\n",
      "Input state = [[65, 44, 99, 19, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [65, 44, 99, 19, 25, 11, 17, 66, 77, 54]\n",
      "Reward = 82.60407876968384\n",
      "Input state = [[99, 70, 27, 56, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [99, 70, 27, 56, 97, 48, 11, 85, 20, 82]\n",
      "Reward = 89.20003986358643\n",
      "Input state = [[66, 80, 71, 23, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [66, 80, 71, 23, 97, 11, 48, 2, 82, 54]\n",
      "Reward = 86.11814641952515\n",
      "Input state = [[13, 86, 79, 9, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [13, 86, 79, 9, 25, 2, 11, 85, 77, 54]\n",
      "Reward = 81.51179933547974\n",
      "Input state = [[14, 49, 72, 17, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [14, 49, 72, 17, 25, 11, 48, 2, 66, 54]\n",
      "Reward = 87.43943738937378\n",
      "Input state = [[49, 68, 65, 20, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [49, 68, 65, 20, 25, 11, 17, 66, 77, 54]\n",
      "Reward = 82.68581914901733\n",
      "Input state = [[7, 11, 91, 15, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [7, 11, 91, 15, 25, 17, 65, 2, 66, 40]\n",
      "Reward = 85.56039571762085\n",
      "Input state = [[29, 67, 48, 63, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [29, 67, 48, 63, 97, 11, 17, 66, 92, 54]\n",
      "Reward = 80.99176931381226\n",
      "Input state = [[99, 18, 47, 44, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [99, 18, 47, 44, 42, 11, 17, 66, 77, 54]\n",
      "Reward = 79.40921211242676\n",
      "Input state = [[89, 48, 35, 1, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [89, 48, 35, 1, 25, 11, 17, 73, 77, 54]\n",
      "Reward = 86.95144081115723\n",
      "Input state = [[22, 14, 67, 75, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [22, 14, 67, 75, 97, 48, 42, 66, 77, 54]\n",
      "Reward = 84.04324340820312\n",
      "Input state = [[39, 99, 94, 68, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [39, 99, 94, 68, 97, 11, 17, 2, 66, 54]\n",
      "Reward = 81.49944162368774\n",
      "Input state = [[63, 82, 25, 10, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [63, 82, 25, 10, 97, 48, 11, 85, 92, 54]\n",
      "Reward = 82.61040687561035\n",
      "Input state = [[69, 37, 17, 63, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [69, 37, 17, 63, 97, 48, 11, 85, 20, 75]\n",
      "Reward = 90.43241882324219\n",
      "Input state = [[53, 15, 11, 9, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [53, 15, 11, 9, 42, 48, 39, 85, 20, 82]\n",
      "Reward = 90.45717144012451\n",
      "Input state = [[79, 31, 36, 31, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [79, 31, 36, 31, 42, 11, 17, 66, 77, 54]\n",
      "Reward = 82.2999062538147\n",
      "Input state = [[46, 66, 7, 97, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [46, 66, 7, 97, 75, 11, 17, 85, 20, 77]\n",
      "Reward = 87.63494205474854\n",
      "Input state = [[70, 59, 4, 0, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [70, 59, 4, 0, 25, 48, 39, 73, 92, 54]\n",
      "Reward = 84.57608079910278\n",
      "Input state = [[29, 60, 47, 25, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [29, 60, 47, 25, 97, 11, 17, 66, 77, 54]\n",
      "Reward = 82.74140930175781\n",
      "Input state = [[64, 83, 53, 61, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [64, 83, 53, 61, 97, 11, 17, 66, 92, 54]\n",
      "Reward = 81.5260534286499\n",
      "Input state = [[30, 46, 56, 59, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [30, 46, 56, 59, 97, 48, 77, 66, 92, 54]\n",
      "Reward = 81.66623592376709\n",
      "Input state = [[82, 64, 65, 78, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [82, 64, 65, 78, 97, 11, 2, 17, 48, 54]\n",
      "Reward = 90.39094924926758\n",
      "Input state = [[29, 75, 96, 25, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [29, 75, 96, 25, 97, 11, 17, 2, 66, 54]\n",
      "Reward = 85.09654426574707\n",
      "Input state = [[34, 63, 39, 46, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [34, 63, 39, 46, 97, 11, 17, 66, 77, 54]\n",
      "Reward = 81.85148620605469\n",
      "Input state = [[80, 43, 16, 13, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [80, 43, 16, 13, 42, 48, 11, 85, 92, 54]\n",
      "Reward = 86.77771425247192\n",
      "Input state = [[77, 97, 27, 87, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [77, 97, 27, 87, 85, 11, 48, 66, 20, 82]\n",
      "Reward = 86.0506362915039\n",
      "Input state = [[33, 43, 8, 52, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [33, 43, 8, 52, 42, 11, 48, 66, 20, 85]\n",
      "Reward = 88.40409278869629\n",
      "Input state = [[88, 28, 38, 51, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [88, 28, 38, 51, 97, 48, 11, 85, 92, 54]\n",
      "Reward = 82.61547613143921\n",
      "Input state = [[92, 72, 53, 39, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [92, 72, 53, 39, 97, 11, 48, 66, 77, 54]\n",
      "Reward = 83.39575672149658\n",
      "Input state = [[51, 34, 66, 80, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [51, 34, 66, 80, 97, 48, 11, 82, 92, 54]\n",
      "Reward = 79.94297981262207\n",
      "Input state = [[71, 27, 87, 42, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [71, 27, 87, 42, 97, 48, 77, 66, 92, 54]\n",
      "Reward = 80.99084949493408\n",
      "Input state = [[96, 9, 77, 93, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [96, 9, 77, 93, 97, 48, 11, 85, 92, 54]\n",
      "Reward = 85.20264387130737\n",
      "Input state = [[35, 35, 93, 36, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [35, 35, 93, 36, 97, 48, 65, 2, 66, 54]\n",
      "Reward = 83.07162809371948\n",
      "Input state = [[60, 69, 53, 48, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [60, 69, 53, 48, 97, 11, 17, 66, 77, 54]\n",
      "Reward = 81.19291496276855\n",
      "Input state = [[96, 98, 65, 94, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [96, 98, 65, 94, 85, 11, 17, 66, 92, 54]\n",
      "Reward = 79.29356479644775\n",
      "Input state = [[86, 47, 60, 40, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [86, 47, 60, 40, 97, 11, 17, 66, 77, 54]\n",
      "Reward = 81.65233993530273\n",
      "Input state = [[12, 47, 38, 24, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [12, 47, 38, 24, 42, 11, 17, 66, 77, 54]\n",
      "Reward = 83.7071967124939\n",
      "Input state = [[21, 26, 71, 2, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [21, 26, 71, 2, 25, 11, 39, 66, 77, 54]\n",
      "Reward = 81.96131372451782\n",
      "Input state = [[22, 91, 92, 43, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [22, 91, 92, 43, 97, 11, 17, 2, 66, 54]\n",
      "Reward = 83.62640333175659\n",
      "Input state = [[67, 61, 12, 32, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [67, 61, 12, 32, 97, 48, 11, 85, 20, 82]\n",
      "Reward = 89.00094699859619\n",
      "Input state = [[65, 35, 84, 27, -1, -1, -1, -1, -1, -1]]\n",
      "Episode = [65, 35, 84, 27, 25, 11, 17, 66, 77, 54]\n",
      "Reward = 81.63015985488892\n"
     ]
    }
   ],
   "source": [
    "# a=env.observation_space.sample().ravel()\n",
    "# print(a)\n",
    "import wandb\n",
    "wandb.init()\n",
    "b=[]\n",
    "\n",
    "\n",
    "reward=[]\n",
    "print('DQN')\n",
    "\n",
    "for j in range(0, 100):\n",
    "    t=0\n",
    "    if(j<0):\n",
    "        idx=1\n",
    "        a=[[ np.random.randint(0,100), -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
    "        b.append(a[0][0])\n",
    "        print(\"Input state =\", a)\n",
    "    else:\n",
    "        idx=4\n",
    "        a=[[ np.random.randint(0,100), np.random.randint(0,100), np.random.randint(0,100), np.random.randint(0,100), -1, -1, -1, -1, -1, -1]]\n",
    "        b.append(a[0][0])\n",
    "        b.append(a[0][1])\n",
    "        b.append(a[0][2])\n",
    "        b.append(a[0][3])\n",
    "        print(\"Input state =\", a)\n",
    "\n",
    "    for i in range(idx,10):\n",
    "        z=dqn.compute_q_values(a)\n",
    "        max_v=max(z)\n",
    "        t+=max_v\n",
    "        #print(max_v)\n",
    "        z=z.tolist()\n",
    "        index = z.index(max_v)\n",
    "        while index in a[0]:\n",
    "            t-=max_v\n",
    "            # print(\"Nooo\")\n",
    "            z[index]=-100000000\n",
    "            max_v=max(z)\n",
    "            t+=max_v\n",
    "        #print(max_v)\n",
    "            index = z.index(max_v)\n",
    "        a[0][i]=index\n",
    "    res=a[0]\n",
    "    reward.append(t)\n",
    "    \n",
    "    \n",
    "    print(\"Episode =\", a[0])\n",
    "    print(\"Reward =\", t)\n",
    "xx=[i for i in range(0,100)]\n",
    "data = [[x, y] for (x, y) in zip(xx, reward)]\n",
    "table = wandb.Table(data=data, columns = [\"episode comp\", \"total reward\"])\n",
    "wandb.log({\"custom_plot\" : wandb.plot.line(table, \"episode comp\",\"total reward\",\n",
    "           title=\"No repeated item\")})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:clgq8pk7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweet-dumpling-54</strong> at: <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/clgq8pk7\" target=\"_blank\">https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/clgq8pk7</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230202_135256-clgq8pk7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:clgq8pk7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aayush/git/rl_recsys/.venv/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:58: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cda69d02884143bd046cf9abb9b02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668910052006443, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aayush/git/rl_recsys/src/recsim/trial/wandb/run-20230202_135258-rznn9mog</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/rznn9mog\" target=\"_blank\">incandescent-mandu-55</a></strong> to <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial\" target=\"_blank\">https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/rznn9mog\" target=\"_blank\">https://wandb.ai/aayush-singharoy/rl_recsys-src_recsim_trial/runs/rznn9mog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Algo\n",
      "Input state = [91, 3, 69, 62]\n",
      "Episode = [91, 3, 69, 62, 33, 2, 59, 29, 27, 48]\n",
      "Reward = 50.68802119482192\n",
      "Input state = [74, 22, 46, 5]\n",
      "Episode = [74, 22, 46, 5, 33, 59, 2, 27, 29, 6]\n",
      "Reward = 46.85860759284728\n",
      "Input state = [56, 55, 45, 71]\n",
      "Episode = [56, 55, 45, 71, 33, 2, 59, 27, 29, 6]\n",
      "Reward = 48.35832888885058\n",
      "Input state = [89, 1, 47, 74]\n",
      "Episode = [89, 1, 47, 74, 33, 2, 59, 29, 6, 27]\n",
      "Reward = 45.87997804598791\n",
      "Input state = [15, 90, 41, 13]\n",
      "Episode = [15, 90, 41, 13, 33, 59, 2, 82, 27, 48]\n",
      "Reward = 49.04306692016985\n",
      "Input state = [25, 96, 36, 9]\n",
      "Episode = [25, 96, 36, 9, 33, 59, 2, 80, 27, 82]\n",
      "Reward = 48.86917986714917\n",
      "Input state = [78, 84, 68, 84]\n",
      "Episode = [78, 84, 68, 84, 33, 59, 2, 27, 82, 29]\n",
      "Reward = 48.26502864163177\n",
      "Input state = [60, 52, 86, 31]\n",
      "Episode = [60, 52, 86, 31, 33, 59, 2, 27, 29, 6]\n",
      "Reward = 50.78923473743457\n",
      "Input state = [54, 70, 5, 75]\n",
      "Episode = [54, 70, 5, 75, 33, 59, 2, 27, 48, 6]\n",
      "Reward = 47.59340282477467\n",
      "Input state = [28, 36, 10, 53]\n",
      "Episode = [28, 36, 10, 53, 33, 59, 2, 80, 27, 82]\n",
      "Reward = 46.97647041271527\n",
      "Input state = [42, 28, 49, 5]\n",
      "Episode = [42, 28, 49, 5, 33, 59, 2, 48, 27, 6]\n",
      "Reward = 51.20036067537312\n",
      "Input state = [50, 72, 5, 94]\n",
      "Episode = [50, 72, 5, 94, 33, 59, 2, 27, 82, 29]\n",
      "Reward = 50.73506762969489\n",
      "Input state = [97, 54, 46, 62]\n",
      "Episode = [97, 54, 46, 62, 33, 2, 59, 82, 29, 27]\n",
      "Reward = 44.72268505002151\n",
      "Input state = [28, 55, 18, 74]\n",
      "Episode = [28, 55, 18, 74, 33, 59, 2, 27, 29, 6]\n",
      "Reward = 46.952761857449865\n",
      "Input state = [81, 49, 32, 1]\n",
      "Episode = [81, 49, 32, 1, 33, 2, 59, 27, 29, 6]\n",
      "Reward = 51.15658672099262\n",
      "Input state = [18, 27, 43, 30]\n",
      "Episode = [18, 27, 43, 30, 33, 59, 2, 80, 6, 29]\n",
      "Reward = 42.0358703166325\n",
      "Input state = [45, 8, 49, 95]\n",
      "Episode = [45, 8, 49, 95, 33, 2, 59, 82, 27, 29]\n",
      "Reward = 43.5828923240142\n",
      "Input state = [15, 86, 73, 94]\n",
      "Episode = [15, 86, 73, 94, 33, 59, 2, 82, 27, 29]\n",
      "Reward = 47.319531944582295\n",
      "Input state = [9, 91, 43, 45]\n",
      "Episode = [9, 91, 43, 45, 33, 2, 59, 29, 6, 80]\n",
      "Reward = 44.63305247159074\n",
      "Input state = [69, 32, 56, 27]\n",
      "Episode = [69, 32, 56, 27, 33, 2, 59, 29, 6, 48]\n",
      "Reward = 45.16894177775431\n",
      "Input state = [66, 19, 27, 70]\n",
      "Episode = [66, 19, 27, 70, 33, 2, 59, 82, 29, 6]\n",
      "Reward = 45.5859827170639\n",
      "Input state = [49, 35, 76, 96]\n",
      "Episode = [49, 35, 76, 96, 33, 2, 59, 6, 29, 27]\n",
      "Reward = 45.6122566803305\n",
      "Input state = [64, 12, 84, 60]\n",
      "Episode = [64, 12, 84, 60, 33, 59, 2, 27, 29, 6]\n",
      "Reward = 50.08639788780889\n",
      "Input state = [64, 55, 61, 40]\n",
      "Episode = [64, 55, 61, 40, 33, 2, 59, 29, 6, 27]\n",
      "Reward = 46.41997027223981\n",
      "Input state = [25, 11, 70, 27]\n",
      "Episode = [25, 11, 70, 27, 33, 2, 59, 82, 29, 6]\n",
      "Reward = 43.32233775468418\n",
      "Input state = [62, 92, 6, 73]\n",
      "Episode = [62, 92, 6, 73, 33, 2, 59, 82, 29, 27]\n",
      "Reward = 41.25052412208747\n",
      "Input state = [47, 54, 10, 51]\n",
      "Episode = [47, 54, 10, 51, 33, 59, 2, 80, 27, 82]\n",
      "Reward = 45.42863518078804\n",
      "Input state = [66, 13, 42, 59]\n",
      "Episode = [66, 13, 42, 59, 33, 2, 82, 27, 29, 6]\n",
      "Reward = 41.36187958532789\n",
      "Input state = [99, 2, 49, 30]\n",
      "Episode = [99, 2, 49, 30, 33, 59, 82, 27, 29, 6]\n",
      "Reward = 40.98164091033289\n",
      "Input state = [43, 46, 76, 31]\n",
      "Episode = [43, 46, 76, 31, 33, 2, 59, 29, 6, 27]\n",
      "Reward = 45.6630506845083\n",
      "Input state = [11, 50, 50, 20]\n",
      "Episode = [11, 50, 50, 20, 33, 59, 2, 6, 82, 27]\n",
      "Reward = 49.464215436789715\n",
      "Input state = [41, 72, 12, 78]\n",
      "Episode = [41, 72, 12, 78, 33, 59, 2, 27, 82, 6]\n",
      "Reward = 47.607182005174\n",
      "Input state = [83, 91, 34, 5]\n",
      "Episode = [83, 91, 34, 5, 33, 59, 2, 82, 27, 29]\n",
      "Reward = 48.78303022059196\n",
      "Input state = [96, 67, 53, 60]\n",
      "Episode = [96, 67, 53, 60, 33, 2, 59, 29, 6, 27]\n",
      "Reward = 48.61133558683697\n",
      "Input state = [91, 1, 49, 84]\n",
      "Episode = [91, 1, 49, 84, 33, 2, 59, 27, 29, 6]\n",
      "Reward = 47.49057369831867\n",
      "Input state = [10, 51, 17, 45]\n",
      "Episode = [10, 51, 17, 45, 33, 2, 59, 29, 6, 48]\n",
      "Reward = 44.998315556184636\n",
      "Input state = [28, 49, 40, 91]\n",
      "Episode = [28, 49, 40, 91, 33, 2, 59, 82, 29, 6]\n",
      "Reward = 43.84464960460685\n",
      "Input state = [1, 58, 29, 19]\n",
      "Episode = [1, 58, 29, 19, 33, 2, 59, 27, 82, 6]\n",
      "Reward = 47.94764022637614\n",
      "Input state = [47, 51, 69, 14]\n",
      "Episode = [47, 51, 69, 14, 33, 2, 59, 29, 27, 48]\n",
      "Reward = 47.613162552339865\n",
      "Input state = [91, 66, 76, 53]\n",
      "Episode = [91, 66, 76, 53, 33, 59, 2, 6, 29, 27]\n",
      "Reward = 42.277772655819476\n",
      "Input state = [45, 70, 59, 27]\n",
      "Episode = [45, 70, 59, 27, 2, 33, 29, 6, 66, 48]\n",
      "Reward = 39.9177538606957\n",
      "Input state = [72, 65, 55, 21]\n",
      "Episode = [72, 65, 55, 21, 33, 2, 59, 27, 29, 6]\n",
      "Reward = 50.10947991375615\n",
      "Input state = [68, 48, 33, 44]\n",
      "Episode = [68, 48, 33, 44, 2, 59, 6, 29, 27, 82]\n",
      "Reward = 43.113334323993996\n",
      "Input state = [7, 62, 87, 72]\n",
      "Episode = [7, 62, 87, 72, 33, 2, 59, 82, 27, 29]\n",
      "Reward = 47.63565412758869\n",
      "Input state = [70, 15, 28, 43]\n",
      "Episode = [70, 15, 28, 43, 33, 59, 2, 80, 27, 29]\n",
      "Reward = 48.9593695248024\n",
      "Input state = [94, 36, 86, 14]\n",
      "Episode = [94, 36, 86, 14, 33, 2, 59, 27, 29, 6]\n",
      "Reward = 47.60259864605199\n",
      "Input state = [13, 70, 83, 12]\n",
      "Episode = [13, 70, 83, 12, 33, 2, 59, 27, 82, 29]\n",
      "Reward = 47.1876410647711\n",
      "Input state = [13, 64, 84, 12]\n",
      "Episode = [13, 64, 84, 12, 33, 59, 2, 80, 27, 29]\n",
      "Reward = 47.31044854759879\n",
      "Input state = [84, 5, 35, 71]\n",
      "Episode = [84, 5, 35, 71, 33, 59, 2, 27, 82, 6]\n",
      "Reward = 49.124716893640574\n",
      "Input state = [39, 33, 18, 12]\n",
      "Episode = [39, 33, 18, 12, 59, 2, 80, 27, 82, 29]\n",
      "Reward = 45.074781139829895\n",
      "Input state = [84, 8, 94, 91]\n",
      "Episode = [84, 8, 94, 91, 33, 2, 59, 29, 6, 48]\n",
      "Reward = 44.87007665076456\n",
      "Input state = [30, 89, 43, 33]\n",
      "Episode = [30, 89, 43, 33, 59, 2, 80, 27, 29, 6]\n",
      "Reward = 38.96094530084188\n",
      "Input state = [81, 90, 60, 11]\n",
      "Episode = [81, 90, 60, 11, 33, 59, 2, 6, 80, 27]\n",
      "Reward = 49.52040976091172\n",
      "Input state = [52, 49, 81, 88]\n",
      "Episode = [52, 49, 81, 88, 33, 59, 2, 80, 27, 82]\n",
      "Reward = 46.061638710135576\n",
      "Input state = [54, 18, 53, 2]\n",
      "Episode = [54, 18, 53, 2, 33, 59, 29, 6, 27, 48]\n",
      "Reward = 41.5771495600291\n",
      "Input state = [15, 95, 24, 18]\n",
      "Episode = [15, 95, 24, 18, 33, 59, 2, 82, 27, 48]\n",
      "Reward = 45.948202660200636\n",
      "Input state = [91, 51, 20, 86]\n",
      "Episode = [91, 51, 20, 86, 33, 59, 2, 82, 27, 29]\n",
      "Reward = 46.44521502958902\n",
      "Input state = [50, 1, 80, 61]\n",
      "Episode = [50, 1, 80, 61, 33, 2, 59, 27, 29, 6]\n",
      "Reward = 50.29433751218131\n",
      "Input state = [85, 58, 4, 82]\n",
      "Episode = [85, 58, 4, 82, 33, 2, 59, 29, 6, 66]\n",
      "Reward = 41.166228356493555\n",
      "Input state = [65, 44, 99, 19]\n",
      "Episode = [65, 44, 99, 19, 33, 59, 2, 82, 27, 29]\n",
      "Reward = 48.14038255151574\n",
      "Input state = [99, 70, 27, 56]\n",
      "Episode = [99, 70, 27, 56, 33, 2, 59, 82, 29, 6]\n",
      "Reward = 44.08175178265683\n",
      "Input state = [66, 80, 71, 23]\n",
      "Episode = [66, 80, 71, 23, 33, 2, 59, 6, 29, 27]\n",
      "Reward = 46.74775312598819\n",
      "Input state = [13, 86, 79, 9]\n",
      "Episode = [13, 86, 79, 9, 33, 59, 2, 27, 82, 29]\n",
      "Reward = 50.656573618128355\n",
      "Input state = [14, 49, 72, 17]\n",
      "Episode = [14, 49, 72, 17, 33, 59, 2, 82, 27, 29]\n",
      "Reward = 47.2905284055982\n",
      "Input state = [49, 68, 65, 20]\n",
      "Episode = [49, 68, 65, 20, 33, 59, 2, 82, 27, 29]\n",
      "Reward = 49.343674833130436\n",
      "Input state = [7, 11, 91, 15]\n",
      "Episode = [7, 11, 91, 15, 33, 59, 2, 82, 27, 48]\n",
      "Reward = 47.260755737544955\n",
      "Input state = [29, 67, 48, 63]\n",
      "Episode = [29, 67, 48, 63, 2, 33, 59, 82, 27, 30]\n",
      "Reward = 43.135671683113216\n",
      "Input state = [99, 18, 47, 44]\n",
      "Episode = [99, 18, 47, 44, 33, 59, 2, 82, 27, 29]\n",
      "Reward = 46.45355182898938\n",
      "Input state = [89, 48, 35, 1]\n",
      "Episode = [89, 48, 35, 1, 2, 33, 59, 27, 29, 6]\n",
      "Reward = 47.64407006106602\n",
      "Input state = [22, 14, 67, 75]\n",
      "Episode = [22, 14, 67, 75, 33, 2, 59, 29, 27, 82]\n",
      "Reward = 44.02760395292961\n",
      "Input state = [39, 99, 94, 68]\n",
      "Episode = [39, 99, 94, 68, 33, 59, 2, 80, 27, 82]\n",
      "Reward = 49.742412168793614\n",
      "Input state = [63, 82, 25, 10]\n",
      "Episode = [63, 82, 25, 10, 33, 2, 59, 29, 27, 48]\n",
      "Reward = 44.54803270636404\n",
      "Input state = [69, 37, 17, 63]\n",
      "Episode = [69, 37, 17, 63, 33, 59, 2, 80, 27, 82]\n",
      "Reward = 49.10950969728413\n",
      "Input state = [53, 15, 11, 9]\n",
      "Episode = [53, 15, 11, 9, 33, 59, 2, 80, 27, 82]\n",
      "Reward = 49.837053241494715\n",
      "Input state = [79, 31, 36, 31]\n",
      "Episode = [79, 31, 36, 31, 33, 59, 2, 27, 29, 6]\n",
      "Reward = 49.658838056279926\n",
      "Input state = [46, 66, 7, 97]\n",
      "Episode = [46, 66, 7, 97, 33, 59, 2, 82, 27, 29]\n",
      "Reward = 44.44871055744393\n",
      "Input state = [70, 59, 4, 0]\n",
      "Episode = [70, 59, 4, 0, 33, 2, 82, 27, 29, 6]\n",
      "Reward = 45.2545420171382\n",
      "Input state = [29, 60, 47, 25]\n",
      "Episode = [29, 60, 47, 25, 33, 59, 2, 82, 27, 48]\n",
      "Reward = 46.966498856353894\n",
      "Input state = [64, 83, 53, 61]\n",
      "Episode = [64, 83, 53, 61, 33, 59, 2, 27, 80, 6]\n",
      "Reward = 46.63122974897545\n",
      "Input state = [30, 46, 56, 59]\n",
      "Episode = [30, 46, 56, 59, 33, 2, 80, 27, 82, 29]\n",
      "Reward = 39.76976658532693\n",
      "Input state = [82, 64, 65, 78]\n",
      "Episode = [82, 64, 65, 78, 33, 59, 2, 27, 29, 6]\n",
      "Reward = 45.26285273491689\n",
      "Input state = [29, 75, 96, 25]\n",
      "Episode = [29, 75, 96, 25, 33, 59, 2, 80, 27, 46]\n",
      "Reward = 43.95876602670153\n",
      "Input state = [34, 63, 39, 46]\n",
      "Episode = [34, 63, 39, 46, 33, 2, 59, 29, 6, 80]\n",
      "Reward = 47.89956194057736\n",
      "Input state = [80, 43, 16, 13]\n",
      "Episode = [80, 43, 16, 13, 33, 2, 59, 82, 27, 29]\n",
      "Reward = 47.097736795139916\n",
      "Input state = [77, 97, 27, 87]\n",
      "Episode = [77, 97, 27, 87, 33, 2, 59, 29, 6, 82]\n",
      "Reward = 40.27858941937606\n",
      "Input state = [33, 43, 8, 52]\n",
      "Episode = [33, 43, 8, 52, 59, 2, 80, 27, 29, 6]\n",
      "Reward = 42.509446822484065\n",
      "Input state = [88, 28, 38, 51]\n",
      "Episode = [88, 28, 38, 51, 33, 59, 2, 48, 27, 29]\n",
      "Reward = 47.26847212861711\n",
      "Input state = [92, 72, 53, 39]\n",
      "Episode = [92, 72, 53, 39, 33, 2, 59, 29, 27, 82]\n",
      "Reward = 46.93970340138439\n",
      "Input state = [51, 34, 66, 80]\n",
      "Episode = [51, 34, 66, 80, 33, 2, 59, 82, 27, 29]\n",
      "Reward = 45.06637018923891\n",
      "Input state = [71, 27, 87, 42]\n",
      "Episode = [71, 27, 87, 42, 33, 59, 2, 82, 29, 6]\n",
      "Reward = 43.58651490060764\n",
      "Input state = [96, 9, 77, 93]\n",
      "Episode = [96, 9, 77, 93, 33, 2, 59, 82, 27, 29]\n",
      "Reward = 45.93099281956035\n",
      "Input state = [35, 35, 93, 36]\n",
      "Episode = [35, 35, 93, 36, 33, 2, 59, 82, 27, 29]\n",
      "Reward = 45.80385000031153\n",
      "Input state = [60, 69, 53, 48]\n",
      "Episode = [60, 69, 53, 48, 33, 2, 59, 27, 29, 6]\n",
      "Reward = 47.87866411531312\n",
      "Input state = [96, 98, 65, 94]\n",
      "Episode = [96, 98, 65, 94, 33, 59, 2, 80, 27, 29]\n",
      "Reward = 49.69449087253194\n",
      "Input state = [86, 47, 60, 40]\n",
      "Episode = [86, 47, 60, 40, 33, 2, 59, 82, 27, 29]\n",
      "Reward = 47.60172399332159\n",
      "Input state = [12, 47, 38, 24]\n",
      "Episode = [12, 47, 38, 24, 33, 59, 2, 82, 27, 29]\n",
      "Reward = 46.2857879314663\n",
      "Input state = [21, 26, 71, 2]\n",
      "Episode = [21, 26, 71, 2, 33, 59, 82, 27, 29, 6]\n",
      "Reward = 42.2441528696814\n",
      "Input state = [22, 91, 92, 43]\n",
      "Episode = [22, 91, 92, 43, 33, 2, 59, 29, 27, 48]\n",
      "Reward = 43.56388020314242\n",
      "Input state = [67, 61, 12, 32]\n",
      "Episode = [67, 61, 12, 32, 33, 2, 59, 29, 27, 82]\n",
      "Reward = 49.20730324677934\n",
      "Input state = [65, 35, 84, 27]\n",
      "Episode = [65, 35, 84, 27, 33, 2, 59, 29, 6, 66]\n",
      "Reward = 43.16966711696646\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.init()\n",
    "nitems = len(ru)\n",
    "t=0\n",
    "dist = D.flatten()\n",
    "print(\"Greedy Algo\")\n",
    "idx=0\n",
    "p=0\n",
    "reward=[]\n",
    "for j in range(0,100):\n",
    "    t=0\n",
    "    if(j<0):\n",
    "        idx=0\n",
    "        state=[b[j]]\n",
    "        print(\"Input state =\", state)\n",
    "    else:\n",
    "        state=[b[4*j], b[4*j+1], b[4*j+2], b[4*j+3]]\n",
    "        idx=3\n",
    "        print(\"Input state =\", state)\n",
    "    for k in range(idx,9):\n",
    "        a=[]\n",
    "        for j in range(0, 100):\n",
    "            a.append(ru[j][0])\n",
    "            # k=len(state)\n",
    "            # if j in state:\n",
    "            #     a[j]+=-10\n",
    "            # else:\n",
    "            #     a[j]+=dist[(state[k-1])*nitems + j]\n",
    "            for i in range(0, len(state)):\n",
    "                if j in state:\n",
    "                    a[j]+=-20\n",
    "                else:\n",
    "                    a[j]+= (1/((len(state)-i)+1)) * dist[(state[i])*nitems + j]\n",
    "        max_v=max(a)\n",
    "        index = a.index(max_v)\n",
    "        t+=max_v\n",
    "        state.append(index)\n",
    "    print(\"Episode =\", state)\n",
    "    print(\"Reward =\",t)\n",
    "    reward.append(t)\n",
    "xx=[i for i in range(0,100)]\n",
    "\n",
    "data = [[x, y] for (x, y) in zip(xx, reward)]\n",
    "data = [[x, y] for (x, y) in zip(xx, reward)]\n",
    "table = wandb.Table(data=data, columns = [\"episode comp\", \"total reward\"])\n",
    "wandb.log({\"custom_plot\" : wandb.plot.line(table, \"episode comp\",\"total reward\",\n",
    "           title=\"Greedy algo\")})\n",
    "reward_dict={\"episode_reward\":reward}\n",
    "reward_df=pd.DataFrame(reward_dict)\n",
    "reward_table=wandb.Table(data=reward_df, columns=[\"rewards\"])\n",
    "wandb.log({'reward_per_session':wandb.plot.line(reward_table, xx, reward)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100)\n",
      "100\n",
      "Testing for 10 episodes ...\n",
      "[13  2 25 25 42 48 77 66 77 54]\n",
      "Episode 1: reward: 11.282, steps: 9\n",
      "[36  2 33 25 42 48 77 66 77 54]\n",
      "Episode 2: reward: 24.243, steps: 9\n",
      "[60  2 25 42 42 48 11 85 20 85]\n",
      "Episode 3: reward: 9.358, steps: 9\n",
      "[38  2 33 25 42 48 77 66 77 54]\n",
      "Episode 4: reward: 24.023, steps: 9\n",
      "[65  2 25 42 42 48 11 85 20 97]\n",
      "Episode 5: reward: 24.867, steps: 9\n",
      "[71  2 25 42 42 48 11 85 20 97]\n",
      "Episode 6: reward: 25.034, steps: 9\n",
      "[76  2 25 42 42 48 11 85 20 97]\n",
      "Episode 7: reward: 23.875, steps: 9\n",
      "[ 6  2 25 25 42 48 77 66 77 54]\n",
      "Episode 8: reward: 9.841, steps: 9\n",
      "[71  2 25 42 42 48 11 85 20 97]\n",
      "Episode 9: reward: 25.034, steps: 9\n",
      "[20  2 25 25 42 48 77 66 77 54]\n",
      "Episode 10: reward: 10.901, steps: 9\n",
      "18.845854214263476\n"
     ]
    }
   ],
   "source": [
    "print(model.output_shape)\n",
    "print(actions)\n",
    "results = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(results.history['episode_reward'])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rl.callbacks\n",
    "# class EpisodeLogger(rl.callbacks.Callback):\n",
    "#     def __init__(self):\n",
    "#         self.observations = {}\n",
    "#         self.rewards = {}\n",
    "#         self.actions = {}\n",
    "\n",
    "#     def on_episode_begin(self, episode, logs):\n",
    "#         self.observations[episode] = []\n",
    "#         self.rewards[episode] = []\n",
    "#         self.actions[episode] = []\n",
    "        \n",
    "#     def on_step_end(self, step, logs):\n",
    "#         episode = logs['episode']\n",
    "#         self.observations[episode].append(logs['observation'])\n",
    "#         print(logs['observation'])\n",
    "#         print(logs['reward'])\n",
    "#         print(logs['action'])\n",
    "#         self.rewards[episode].append(logs['reward'])\n",
    "#         self.actions[episode].append(logs['action'])\n",
    "\n",
    "# cb_ep = EpisodeLogger()\n",
    "# dqn.test(env, nb_episodes=10, visualize=False, callbacks=[cb_ep])\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for obs in cb_ep.rewards.values():\n",
    "#     plt.plot([o for o in obs])\n",
    "# plt.xlabel(\"step\")\n",
    "# plt.ylabel(\"reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cb_ep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rewards\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obs \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcb_ep\u001b[49m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m      3\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(obs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cb_ep' is not defined"
     ]
    }
   ],
   "source": [
    "rewards=[]\n",
    "for obs in cb_ep.rewards.values():\n",
    "    rewards.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_reward=[]\n",
    "for i in rewards:\n",
    "    p=0\n",
    "    u=[]\n",
    "    for j in i:\n",
    "        j*=-1\n",
    "        p+=j\n",
    "        u.append(p)\n",
    "    cum_reward.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (12,) and (9,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m purchases \u001b[38;5;241m=\u001b[39m cum_reward[i]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(purchases)):\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpurchases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:2728\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2726\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2727\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2728\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2729\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2730\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/matplotlib/axes/_axes.py:1662\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1662\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1663\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1664\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/git/rl_recsys/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (12,) and (9,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "x=list(range(12))\n",
    "for i in range(7):\n",
    "    purchases = cum_reward[i]\n",
    "    for j in range(len(purchases)):\n",
    "        plt.plot(x,purchases)\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(\"reward\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1aae1df98fcdaba42211eb04fb1c10f1061d9efaa8b282c66397dab6b26e66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
